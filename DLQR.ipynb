{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0a09ea",
   "metadata": {},
   "source": [
    "# Dynamic LLM Query Router\n",
    "\n",
    "**Project:** Dynamic LLM Query Router (DLQR) for Multi-Model Orchestration\n",
    "**Author:** Pranshu Goyal  \n",
    "**Date:** 2025-08-16  \n",
    "**Environment:** Python ≥ 3.10  \n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "This notebook presents a consolidated, production-oriented implementation of a **Dynamic LLM Query Router (DLQR)**, designed for multimodal healthcare queries and decision support.  \n",
    "The system routes user queries dynamically across three specialized modules:  \n",
    "\n",
    "- **Prediction** — supervised classification using biomarkers and patient metadata.  \n",
    "- **Case-Based Reasoning (CBR)** — retrieval and ranking of similar historical cases.  \n",
    "- **Retrieval-Augmented Generation (RAG)** — large language model–driven question answering and summarisation.  \n",
    "\n",
    "The router employs advanced mechanisms for **temperature calibration, circuit-breaking, hedging, and bandit-based exploration**, ensuring robust performance under uncertainty. Integrated utilities support preprocessing, dataset management, supervised training, and artifact persistence, enabling full end-to-end experimentation.\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Instructions\n",
    "1. **Environment Setup:** Create and activate a clean Python virtual environment.  \n",
    "2. **Notebook Runtime:** Open in JupyterLab or VS Code with the correct kernel.  \n",
    "3. **Configuration:** Ensure a `.env` file is present in the working directory (see provided template).  \n",
    "4. **Execution Order:** Run the notebook sequentially from top to bottom. All modules degrade gracefully if artifacts are missing, emitting warnings instead of failing.  \n",
    "\n",
    "> **Note:** On first execution, some components (e.g., Ollama LLM backends, Sentence-Transformer embeddings) may automatically download pretrained weights. If required artifacts for Prediction/CBR/RAG are absent, the corresponding tools will provide clear diagnostic messages while the router itself continues to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b152e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1) Environment Setup \n",
    "# ==============================\n",
    "\n",
    "# %pip install -U -qq \\\n",
    "#   python-dotenv==1.0.1 \\\n",
    "#   rich==13.9.4 \\\n",
    "#   loguru==0.7.2 \\\n",
    "#   tqdm==4.66.5 \\\n",
    "#   numpy==1.26.4 \\\n",
    "#   pandas==2.2.3 \\\n",
    "#   scipy==1.11.4 \\\n",
    "#   scikit-learn==1.5.2 \\\n",
    "#   matplotlib==3.9.2 \\\n",
    "#   seaborn==0.13.2 \\\n",
    "#   joblib==1.4.2 \\\n",
    "#   torch==2.4.1 \\\n",
    "#   sentence-transformers==3.0.1 \\\n",
    "#   xgboost==2.1.1 \\\n",
    "#   \"accelerate>=0.26.0\" \\\n",
    "#   \"transformers>=4.41,<5\" \\\n",
    "#   \"pydantic>=2,<3\" \\\n",
    "#   \"llama-index>=0.11,<0.12\" \\\n",
    "#   \"llama-index-llms-ollama>=0.3,<0.4\" \\\n",
    "#   \"llama-index-embeddings-huggingface>=0.3,<0.4\"\n",
    "\n",
    "# print(\"Pip Dependencies Installed Successfully.\")\n",
    "\n",
    "# %conda install -y -q -c conda-forge faiss-cpu=1.7.4\n",
    "\n",
    "# print(\"Conda Dependencies Installed Successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174ec4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranshugoyal/anaconda3/envs/AI-Thesis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK. Python: 3.10.14\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 2) Imports \n",
    "# ==============================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# --- Warnings ---\n",
    "import warnings\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)\n",
    "\n",
    "# --- Standard library ---\n",
    "import html\n",
    "import hashlib\n",
    "import inspect\n",
    "import re\n",
    "import asyncio\n",
    "import concurrent.futures as _fut\n",
    "from collections import Counter, defaultdict, deque\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from enum import Enum\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import threading as _thread\n",
    "import time\n",
    "import traceback\n",
    "import uuid\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Sequence, Iterable, Union\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# --- Third-party ---\n",
    "from dotenv import load_dotenv\n",
    "import faiss\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from scipy.stats import multivariate_normal\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n",
    "import xgboost  \n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make tokenizers deterministic on CPU-only notebooks\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"Imports OK. Python:\", sys.version.split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f55931",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration & Environment Setup\n",
    "\n",
    "This code sets up **environment variables, project directories, random seeds, and model/router configurations**.  \n",
    "It prepares the system for reproducible ML experiments and defines key parameters for the **Dynamic LLM Query Router** pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e131d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /Users/pranshugoyal/Downloads/Query Router 17-August-2025 3/data\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 🌍 Environment Setup\n",
    "# -------------------------------\n",
    "\n",
    "# Load environment variables from a .env file (if present)\n",
    "# This allows project configuration without hardcoding values\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 📂 Project Directories\n",
    "# -------------------------------\n",
    "\n",
    "# Root directory of the project (defaults to current dir \".\")\n",
    "ROOT_DIR: Path = Path(os.getenv(\"ROOT_DIR\", \".\")).resolve()\n",
    "\n",
    "# Data directory where input/output artifacts are stored\n",
    "# If \"ROUTER_DATA_DIR\" is not set, defaults to ./data\n",
    "DATA_DIR: Path = Path(os.getenv(\"ROUTER_DATA_DIR\", \"data\")).resolve()\n",
    "\n",
    "# Create the data directory if it doesn’t exist\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 🎲 Reproducibility (Random Seeds)\n",
    "# -------------------------------\n",
    "\n",
    "# Global random seed for reproducibility\n",
    "RANDOM_SEED: int = int(os.getenv(\"RANDOM_SEED\", \"42\"))\n",
    "\n",
    "# Function to enforce deterministic behavior across Python, NumPy, and Torch\n",
    "def set_seed(seed: int = RANDOM_SEED) -> None:\n",
    "    random.seed(seed)          # Python built-in random module\n",
    "    np.random.seed(seed)       # NumPy random generator\n",
    "    torch.manual_seed(seed)    # Torch RNG (CPU & GPU if applicable)\n",
    "\n",
    "# Set seed immediately after definition\n",
    "set_seed()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 🔧 Router / Model Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Embedding model used for vectorization of queries (default: MiniLM)\n",
    "EMBED_MODEL: str = os.getenv(\"EMBED_MODEL\", \"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Threshold for uncertainty-based decisions in router\n",
    "UNCERTAINTY_T: float = float(os.getenv(\"UNCERTAINTY_T\", \"0.60\"))\n",
    "\n",
    "# Number of hash bits for locality-sensitive hashing (2^6 = 64 buckets)\n",
    "HASH_BITS: int = int(os.getenv(\"BUCKET_HASH_BITS\", \"6\"))\n",
    "\n",
    "# Whether to use learned (adaptive) buckets instead of static\n",
    "LEARNED_BUCKETS: bool = bool(int(os.getenv(\"LEARNED_BUCKETS\", \"1\")))\n",
    "\n",
    "# Number of clusters used in unsupervised router grouping\n",
    "N_CLUSTERS: int = int(os.getenv(\"N_CLUSTERS\", \"5\"))\n",
    "\n",
    "# -------------------------------\n",
    "# 🤖 LLM Backbone Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Whether to use LLM-based intent classification in routing\n",
    "USE_LLM_INTENT: bool = bool(int(os.getenv(\"USE_LLM_INTENT\", \"1\")))\n",
    "\n",
    "# Backbone language model for LLM classifier\n",
    "LLM_BACKBONE: str = os.getenv(\"LLM_BACKBONE\", \"distilbert-base-uncased\")\n",
    "\n",
    "# Weight assigned to LLM classifier in ensemble decisions\n",
    "LLM_WEIGHT: float = float(os.getenv(\"LLM_WEIGHT\", \"0.70\"))\n",
    "\n",
    "# -------------------------------\n",
    "# 🔄 Execution & Hedging Policies\n",
    "# -------------------------------\n",
    "\n",
    "# Use multi-model executor (placeholder wiring point, default off)\n",
    "USE_MM_EXECUTOR: bool = bool(int(os.getenv(\"USE_MM_EXECUTOR\", \"0\")))\n",
    "\n",
    "# Whether to apply hedging (sending query to multiple targets if uncertain)\n",
    "USE_HEDGING: bool = bool(int(os.getenv(\"USE_HEDGING\", \"1\")))\n",
    "\n",
    "# Secondary uncertainty threshold for hedging\n",
    "HEDGE_UNCERTAINTY_T: float = float(os.getenv(\"HEDGE_UNCERTAINTY_T\", \"0.25\"))\n",
    "\n",
    "# Maximum number of alternative targets allowed in hedging\n",
    "HEDGE_MAX_TARGETS: int = int(os.getenv(\"HEDGE_MAX_TARGETS\", \"2\"))\n",
    "\n",
    "# Default timeout (ms) when waiting for a target model’s response\n",
    "DEFAULT_TARGET_TIMEOUT_MS: int = int(os.getenv(\"TARGET_TIMEOUT_MS\", \"60000\"))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 📁 Derived Paths (Artifacts & Checkpoints)\n",
    "# -------------------------------\n",
    "\n",
    "# These are auto-derived from ROOT_DIR and DATA_DIR, stored as strings\n",
    "# to remain compatible with libraries expecting raw paths.\n",
    "\n",
    "LOG_Path: str = str(DATA_DIR / \"router_events.jsonl\")        # Router event logs\n",
    "POLICY_Path: str = str(DATA_DIR / \"policy.pt\")               # Learned routing policy\n",
    "CALIB_Path: str = str(DATA_DIR / \"temperature.json\")         # Temperature calibration\n",
    "BANDIT_STATE_Path: str = str(DATA_DIR / \"lints_state.json\")  # Bandit state checkpoint\n",
    "LLM_DIR: str = str(DATA_DIR / \"llm_intent\")                  # Directory for LLM intent model\n",
    "CSV_Path = str(DATA_DIR / \"router_training_dataset.csv\")     # Training dataset for router\n",
    "SPLIT_Path = str(DATA_DIR / \"router_splits.json\")            # Dataset split metadata\n",
    "MODEL_DIR = str(ROOT_DIR / \"trained_router_llm_nometa_v1\")   # Trained router model storage\n",
    "REPORTS_DIR = str(ROOT_DIR / \"reports\")                      # Experiment reports\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 🖥️ Device Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Force computation on CPU (changeable if CUDA available)\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# Sanity check: print key runtime paths & device info\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779919a",
   "metadata": {},
   "source": [
    "---\n",
    "## Router Dataset Preprocessing Module\n",
    "\n",
    "This module provides **data cleaning, normalization, validation, deduplication, and reporting** for the **Dynamic LLM Query Router** training dataset.  \n",
    "It ensures consistent schema (`query`, `context`, `label`) while enforcing label rules and removing noisy/invalid samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d83e04-371d-460f-83d8-7f478e930fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader ready.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 📦 Preprocess Configuration\n",
    "# -------------------------------\n",
    "@dataclass(frozen=True)\n",
    "class PreprocessConfig:\n",
    "    # Column names for text, context, and labels\n",
    "    text_col: str = \"query\"\n",
    "    context_col: str = \"context\"\n",
    "    label_col: str = \"label\"\n",
    "\n",
    "    # --- Cleaning toggles ---\n",
    "    lowercase: bool = True          # Convert text to lowercase\n",
    "    strip_whitespace: bool = True   # Trim leading/trailing spaces\n",
    "    normalize_space: bool = True    # Replace multiple spaces with single space\n",
    "    unescape_html: bool = True      # Convert HTML entities (&amp; → &)\n",
    "    remove_urls: bool = True        # Strip URLs from text\n",
    "    remove_emails: bool = True      # Strip email addresses\n",
    "    normalize_quotes: bool = True   # Convert fancy quotes to ASCII equivalents\n",
    "\n",
    "    # --- Label normalization / validation ---\n",
    "    force_lower_labels: bool = True                         # Enforce lowercase labels\n",
    "    allowed_labels: Tuple[str, ...] = (\"prediction\", \"cbr\", \"qa\", \"summarize\")  \n",
    "    # Restrict to these canonical labels\n",
    "    label_map: Optional[Dict[str, str]] = None              # Map synonyms (e.g. \"summarise\" → \"summarize\")\n",
    "\n",
    "    # --- Length filters ---\n",
    "    min_text_chars: int = 3         # Drop too-short queries\n",
    "    max_text_chars: int = 5000      # Cap overly long queries\n",
    "    max_context_chars: int = 8000   # Cap context size\n",
    "\n",
    "    # --- Deduplication ---\n",
    "    dedupe_by: Tuple[str, ...] = (\"query\", \"context\", \"label\")\n",
    "\n",
    "    # --- Context handling ---\n",
    "    treat_empty_context_as_none: bool = True\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 🔍 Regex & Helpers for Cleaning\n",
    "# -------------------------------\n",
    "_url_re = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)  # Match URLs\n",
    "_email_re = re.compile(r\"\\b\\S+@\\S+\\.\\S+\\b\")                    # Match emails\n",
    "_whitespace_re = re.compile(r\"\\s+\")                            # Match multi-space\n",
    "_quotes_map = str.maketrans({\"“\": '\"', \"”\": '\"', \"’\": \"'\", \"‘\": \"'\", \n",
    "                             \"„\": '\"', \"‛\": \"'\", \"‹\": \"<\", \"›\": \">\"})\n",
    "\n",
    "def _clean_text(s: Any, cfg: PreprocessConfig, is_context: bool = False) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Clean text or context according to config toggles.\n",
    "    Handles NaNs, HTML entities, quotes, URLs, emails, whitespace, casing,\n",
    "    and length restrictions.\n",
    "    \"\"\"\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return None\n",
    "    s = str(s)\n",
    "    if cfg.unescape_html:\n",
    "        s = html.unescape(s)\n",
    "    if cfg.normalize_quotes:\n",
    "        s = s.translate(_quotes_map)\n",
    "    if cfg.remove_urls:\n",
    "        s = _url_re.sub(\" \", s)\n",
    "    if cfg.remove_emails:\n",
    "        s = _email_re.sub(\" \", s)\n",
    "    if cfg.strip_whitespace:\n",
    "        s = s.strip()\n",
    "    if cfg.normalize_space:\n",
    "        s = _whitespace_re.sub(\" \", s)\n",
    "    if cfg.lowercase:\n",
    "        s = s.lower()\n",
    "\n",
    "    # --- Enforce length limits ---\n",
    "    if not is_context:\n",
    "        if len(s) < cfg.min_text_chars:\n",
    "            return None\n",
    "        if len(s) > cfg.max_text_chars:\n",
    "            s = s[: cfg.max_text_chars]\n",
    "    else:\n",
    "        if len(s) > cfg.max_context_chars:\n",
    "            s = s[: cfg.max_context_chars]\n",
    "\n",
    "    # Handle empty contexts\n",
    "    if cfg.treat_empty_context_as_none and is_context and (s == \"\" or s.isspace()):\n",
    "        return None\n",
    "    return s\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 🧹 Main Preprocessing Function\n",
    "# -------------------------------\n",
    "def preprocess_router_dataframe(df_raw: pd.DataFrame, cfg: PreprocessConfig) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"Return a cleaned dataframe + audit report. Includes label normalization and validation.\"\"\"\n",
    "\n",
    "    report: Dict[str, Any] = {}\n",
    "\n",
    "    # --- Validate required columns ---\n",
    "    needed = [cfg.text_col, cfg.context_col, cfg.label_col]\n",
    "    missing = [c for c in needed if c not in df_raw.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns in CSV: {missing}\")\n",
    "\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # --- LABEL NORMALIZATION ---\n",
    "    labels = df[cfg.label_col].astype(str)\n",
    "    if cfg.force_lower_labels:\n",
    "        labels = labels.str.strip().str.lower()\n",
    "    if cfg.label_map:  # Remap synonyms if provided\n",
    "        labels = labels.map(lambda x: cfg.label_map.get(x, x))\n",
    "\n",
    "    # Validate against allowed labels\n",
    "    allowed = set(cfg.allowed_labels)\n",
    "    mask_valid = labels.isin(allowed)\n",
    "    dropped_invalid = int((~mask_valid).sum())\n",
    "    if dropped_invalid > 0:\n",
    "        df = df[mask_valid].copy()\n",
    "    df[cfg.label_col] = labels.loc[df.index].values\n",
    "    report[\"dropped_invalid_labels\"] = dropped_invalid\n",
    "\n",
    "    # --- TEXT / CONTEXT CLEAN ---\n",
    "    df[\"__text__\"] = df[cfg.text_col].map(lambda x: _clean_text(x, cfg, is_context=False))\n",
    "    df[\"__ctx__\"] = df[cfg.context_col].map(lambda x: _clean_text(x, cfg, is_context=True))\n",
    "\n",
    "    # Drop empty or too-short text\n",
    "    n0 = len(df)\n",
    "    df = df[df[\"__text__\"].notna() & df[cfg.label_col].notna()]\n",
    "    report[\"dropped_empty_or_short\"] = int(n0 - len(df))\n",
    "\n",
    "    # --- DEDUPLICATION ---\n",
    "    if cfg.dedupe_by:\n",
    "        key_cols = []\n",
    "        for c in cfg.dedupe_by:\n",
    "            if c == cfg.text_col:\n",
    "                key_cols.append(\"__text__\")\n",
    "            elif c == cfg.context_col:\n",
    "                key_cols.append(\"__ctx__\")\n",
    "            else:\n",
    "                key_cols.append(c)\n",
    "        n_before = len(df)\n",
    "        df = df.drop_duplicates(subset=key_cols, keep=\"first\")\n",
    "        report[\"dropped_duplicates\"] = int(n_before - len(df))\n",
    "\n",
    "    # --- FINALIZE SCHEMA ---\n",
    "    df = df.assign(query=df[\"__text__\"], context=df[\"__ctx__\"]).drop(columns=[\"__text__\", \"__ctx__\"])\n",
    "    cols = [\"query\", \"context\", cfg.label_col] + [c for c in df.columns if c not in {\"query\", \"context\", cfg.label_col}]\n",
    "    df = df[cols].reset_index(drop=True)\n",
    "\n",
    "    # --- REPORT ---\n",
    "    report[\"class_distribution\"] = df[cfg.label_col].value_counts().to_dict()\n",
    "    report[\"final_rows\"] = int(len(df))\n",
    "    report[\"preprocess_config\"] = asdict(cfg)\n",
    "    return df, report\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 📊 Reporting Utilities\n",
    "# -------------------------------\n",
    "def _maybe_display_df(df: pd.DataFrame, title: Optional[str] = None) -> None:\n",
    "    \"\"\"Pretty-print a DataFrame in Jupyter (hide index), fallback to console if unavailable.\"\"\"\n",
    "    try:\n",
    "        from IPython.display import display, HTML  # type: ignore\n",
    "        if title:\n",
    "            display(HTML(f\"<h4 style='margin:8px 0'>{title}</h4>\"))\n",
    "        display(df.style.hide(axis=\"index\"))  # Cleaner look\n",
    "    except Exception:\n",
    "        if title:\n",
    "            print(f\"\\n{title}\")\n",
    "        print(df.to_string(index=False))\n",
    "\n",
    "\n",
    "def _dict_to_kv_df(d: Dict[str, Any], key_col: str, val_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Convert a dict into a tidy 2-column DataFrame for display.\"\"\"\n",
    "    rows = []\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, (list, tuple, set)):\n",
    "            v = \", \".join(map(str, v))\n",
    "        rows.append({key_col: k, val_col: v})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def _show_preprocess_report(report: Dict[str, Any]) -> None:\n",
    "    \"\"\"Render audit report: metrics, class distribution, and config settings.\"\"\"\n",
    "    rpt = dict(report)  # shallow copy\n",
    "    class_dist = rpt.pop(\"class_distribution\", None)\n",
    "    cfg = rpt.pop(\"preprocess_config\", None)\n",
    "\n",
    "    # 1. Metrics summary\n",
    "    metrics_df = _dict_to_kv_df(rpt, \"Metric\", \"Value\")\n",
    "    if not metrics_df.empty:\n",
    "        metrics_df[\"Metric\"] = metrics_df[\"Metric\"].str.replace(\"_\", \" \").str.title()\n",
    "        _maybe_display_df(metrics_df, \"🧹 Preprocess Report\")\n",
    "\n",
    "    # 2. Class distribution\n",
    "    if isinstance(class_dist, dict) and class_dist:\n",
    "        dist_df = pd.DataFrame(\n",
    "            [{\"Label\": k, \"Count\": int(v)} for k, v in class_dist.items()]\n",
    "        ).sort_values(\"Count\", ascending=False)\n",
    "        _maybe_display_df(dist_df, \"📊 Class Distribution\")\n",
    "\n",
    "    # 3. Config details\n",
    "    if isinstance(cfg, dict) and cfg:\n",
    "        cfg_df = _dict_to_kv_df(cfg, \"Setting\", \"Value\").sort_values(\"Setting\")\n",
    "        _maybe_display_df(cfg_df, \"⚙️ Preprocess Config\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 🚀 Public Loader API\n",
    "# -------------------------------\n",
    "DEFAULT_PREPROC = PreprocessConfig(text_col=\"query\", context_col=\"context\", label_col=\"label\")\n",
    "\n",
    "def load_router_dataset_preprocessed(\n",
    "    csv_Path: str, preprocess_cfg: Optional[PreprocessConfig] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Load, preprocess, and summarize a router dataset from CSV.\n",
    "    Returns cleaned DataFrame + config used.\n",
    "    \"\"\"\n",
    "    raw_df = pd.read_csv(csv_Path)\n",
    "    cfg = preprocess_cfg or DEFAULT_PREPROC\n",
    "\n",
    "    df_clean, report = preprocess_router_dataframe(raw_df, cfg)\n",
    "\n",
    "    # Render audit report and sample rows\n",
    "    _show_preprocess_report(report)\n",
    "    try:\n",
    "        _maybe_display_df(df_clean.head(5), \"🔎 Sample Of Cleaned Rows\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return df_clean, cfg\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Ready Message\n",
    "# -------------------------------\n",
    "print(\"Data loader ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a43440",
   "metadata": {},
   "source": [
    "---\n",
    "## Router Data Preprocessing (Usage Example)\n",
    "\n",
    "This snippet demonstrates how to:\n",
    "1. Define a **preprocessing configuration** for the router dataset.  \n",
    "2. Load and preprocess the CSV dataset using that configuration.  \n",
    "3. Return both the cleaned dataframe (`df`) and the effective configuration (`cfg`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e8fab-b825-413d-9477-20769bf4d335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4 style='margin:8px 0'>🧹 Preprocess Report</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_2323d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_2323d_level0_col0\" class=\"col_heading level0 col0\" >Metric</th>\n",
       "      <th id=\"T_2323d_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_2323d_row0_col0\" class=\"data row0 col0\" >Dropped Invalid Labels</td>\n",
       "      <td id=\"T_2323d_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2323d_row1_col0\" class=\"data row1 col0\" >Dropped Empty Or Short</td>\n",
       "      <td id=\"T_2323d_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2323d_row2_col0\" class=\"data row2 col0\" >Dropped Duplicates</td>\n",
       "      <td id=\"T_2323d_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2323d_row3_col0\" class=\"data row3 col0\" >Final Rows</td>\n",
       "      <td id=\"T_2323d_row3_col1\" class=\"data row3 col1\" >10240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16b123520>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4 style='margin:8px 0'>📊 Class Distribution</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e6512\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_e6512_level0_col0\" class=\"col_heading level0 col0\" >Label</th>\n",
       "      <th id=\"T_e6512_level0_col1\" class=\"col_heading level0 col1\" >Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_e6512_row0_col0\" class=\"data row0 col0\" >prediction</td>\n",
       "      <td id=\"T_e6512_row0_col1\" class=\"data row0 col1\" >2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e6512_row1_col0\" class=\"data row1 col0\" >cbr</td>\n",
       "      <td id=\"T_e6512_row1_col1\" class=\"data row1 col1\" >2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e6512_row2_col0\" class=\"data row2 col0\" >qa</td>\n",
       "      <td id=\"T_e6512_row2_col1\" class=\"data row2 col1\" >2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e6512_row3_col0\" class=\"data row3 col0\" >summarize</td>\n",
       "      <td id=\"T_e6512_row3_col1\" class=\"data row3 col1\" >2560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16b123520>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4 style='margin:8px 0'>⚙️ Preprocess Config</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_63d74\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_63d74_level0_col0\" class=\"col_heading level0 col0\" >Setting</th>\n",
       "      <th id=\"T_63d74_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row0_col0\" class=\"data row0 col0\" >allowed_labels</td>\n",
       "      <td id=\"T_63d74_row0_col1\" class=\"data row0 col1\" >prediction, cbr, qa, summarize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row1_col0\" class=\"data row1 col0\" >context_col</td>\n",
       "      <td id=\"T_63d74_row1_col1\" class=\"data row1 col1\" >context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row2_col0\" class=\"data row2 col0\" >dedupe_by</td>\n",
       "      <td id=\"T_63d74_row2_col1\" class=\"data row2 col1\" >query, context, label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row3_col0\" class=\"data row3 col0\" >force_lower_labels</td>\n",
       "      <td id=\"T_63d74_row3_col1\" class=\"data row3 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row4_col0\" class=\"data row4 col0\" >label_col</td>\n",
       "      <td id=\"T_63d74_row4_col1\" class=\"data row4 col1\" >label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row5_col0\" class=\"data row5 col0\" >label_map</td>\n",
       "      <td id=\"T_63d74_row5_col1\" class=\"data row5 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row6_col0\" class=\"data row6 col0\" >lowercase</td>\n",
       "      <td id=\"T_63d74_row6_col1\" class=\"data row6 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row7_col0\" class=\"data row7 col0\" >max_context_chars</td>\n",
       "      <td id=\"T_63d74_row7_col1\" class=\"data row7 col1\" >8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row8_col0\" class=\"data row8 col0\" >max_text_chars</td>\n",
       "      <td id=\"T_63d74_row8_col1\" class=\"data row8 col1\" >5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row9_col0\" class=\"data row9 col0\" >min_text_chars</td>\n",
       "      <td id=\"T_63d74_row9_col1\" class=\"data row9 col1\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row10_col0\" class=\"data row10 col0\" >normalize_quotes</td>\n",
       "      <td id=\"T_63d74_row10_col1\" class=\"data row10 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row11_col0\" class=\"data row11 col0\" >normalize_space</td>\n",
       "      <td id=\"T_63d74_row11_col1\" class=\"data row11 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row12_col0\" class=\"data row12 col0\" >remove_emails</td>\n",
       "      <td id=\"T_63d74_row12_col1\" class=\"data row12 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row13_col0\" class=\"data row13 col0\" >remove_urls</td>\n",
       "      <td id=\"T_63d74_row13_col1\" class=\"data row13 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row14_col0\" class=\"data row14 col0\" >strip_whitespace</td>\n",
       "      <td id=\"T_63d74_row14_col1\" class=\"data row14 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row15_col0\" class=\"data row15 col0\" >text_col</td>\n",
       "      <td id=\"T_63d74_row15_col1\" class=\"data row15 col1\" >query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row16_col0\" class=\"data row16 col0\" >treat_empty_context_as_none</td>\n",
       "      <td id=\"T_63d74_row16_col1\" class=\"data row16 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_63d74_row17_col0\" class=\"data row17 col0\" >unescape_html</td>\n",
       "      <td id=\"T_63d74_row17_col1\" class=\"data row17 col1\" >True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16b123520>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4 style='margin:8px 0'>🔎 Sample Of Cleaned Rows</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_076ab\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_076ab_level0_col0\" class=\"col_heading level0 col0\" >query</th>\n",
       "      <th id=\"T_076ab_level0_col1\" class=\"col_heading level0 col1\" >context</th>\n",
       "      <th id=\"T_076ab_level0_col2\" class=\"col_heading level0 col2\" >label</th>\n",
       "      <th id=\"T_076ab_level0_col3\" class=\"col_heading level0 col3\" >id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_076ab_row0_col0\" class=\"data row0 col0\" >estimate oral cancer risk from biomarkers: mmp9 353, il-8 37.</td>\n",
       "      <td id=\"T_076ab_row0_col1\" class=\"data row0 col1\" >None</td>\n",
       "      <td id=\"T_076ab_row0_col2\" class=\"data row0 col2\" >prediction</td>\n",
       "      <td id=\"T_076ab_row0_col3\" class=\"data row0 col3\" >6a5184809296dc8e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_076ab_row1_col0\" class=\"data row1 col0\" >retrieve top-3 similar oscc cases (cbr). focus: mmp-9. [caseid:26c7e73d]</td>\n",
       "      <td id=\"T_076ab_row1_col1\" class=\"data row1 col1\" >None</td>\n",
       "      <td id=\"T_076ab_row1_col2\" class=\"data row1 col2\" >cbr</td>\n",
       "      <td id=\"T_076ab_row1_col3\" class=\"data row1 col3\" >602e40f2add97875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_076ab_row2_col0\" class=\"data row2 col0\" >q: what do these findings imply for oscc risk/staging? focus: mmp-9. [caseid:26c7e73d]</td>\n",
       "      <td id=\"T_076ab_row2_col1\" class=\"data row2 col1\" >None</td>\n",
       "      <td id=\"T_076ab_row2_col2\" class=\"data row2 col2\" >qa</td>\n",
       "      <td id=\"T_076ab_row2_col3\" class=\"data row2 col3\" >ef26d3217e50f82e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_076ab_row3_col0\" class=\"data row3 col0\" >summarize the oscc clinical context and biomarker profile into key points. focus: mmp-9. [caseid:26c7e73d]</td>\n",
       "      <td id=\"T_076ab_row3_col1\" class=\"data row3 col1\" >None</td>\n",
       "      <td id=\"T_076ab_row3_col2\" class=\"data row3 col2\" >summarize</td>\n",
       "      <td id=\"T_076ab_row3_col3\" class=\"data row3 col3\" >722c9bb1de320775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_076ab_row4_col0\" class=\"data row4 col0\" >estimate oral cancer risk from biomarkers: mmp9 477, il-8 61.</td>\n",
       "      <td id=\"T_076ab_row4_col1\" class=\"data row4 col1\" >discharge note: continue ace inhibitor, repeat labs in 2 weeks, nephrology referral if egfr < 45.</td>\n",
       "      <td id=\"T_076ab_row4_col2\" class=\"data row4 col2\" >prediction</td>\n",
       "      <td id=\"T_076ab_row4_col3\" class=\"data row4 col3\" >7ab0360bccc31abc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16b121ea0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Preprocessing Configuration\n",
    "# -------------------------------\n",
    "# Define a preprocessing config object with key toggles.\n",
    "# These settings determine how raw dataset rows (query, context, label) are cleaned.\n",
    "PREPROC = PreprocessConfig(\n",
    "    text_col=\"query\",            # Column in CSV containing user queries\n",
    "    context_col=\"context\",       # Column containing optional context (background text)\n",
    "    label_col=\"label\",           # Column containing task label (prediction/cbr/qa/summarize)\n",
    "\n",
    "    # --- Cleaning rules ---\n",
    "    lowercase=True,              # Convert all text to lowercase\n",
    "    remove_urls=True,            # Strip URLs from text\n",
    "    remove_emails=True,          # Strip email addresses\n",
    "    normalize_space=True,        # Replace multiple spaces/tabs with single space\n",
    "\n",
    "    # --- Deduplication ---\n",
    "    dedupe_by=(\"query\", \"context\", \"label\"),  # Ensure uniqueness by combination of these columns\n",
    "\n",
    "    # --- Label handling ---\n",
    "    label_map=None               # Optional mapping for label synonyms (e.g. {\"summarise\": \"summarize\"})\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load & Preprocess Dataset\n",
    "# -------------------------------\n",
    "# Apply the defined config to load + preprocess router training data.\n",
    "# - Cleans text/context\n",
    "# - Normalizes and validates labels\n",
    "# - Deduplicates rows\n",
    "# - Produces an audit report (printed as tables if in Jupyter)\n",
    "#\n",
    "# Returns:\n",
    "#   df  → Cleaned Pandas DataFrame (ready for model training/evaluation)\n",
    "#   cfg → Final preprocessing config actually applied\n",
    "df, cfg = load_router_dataset_preprocessed(CSV_Path, preprocess_cfg=PREPROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d4c1ce",
   "metadata": {},
   "source": [
    "## Dataset Splitting Module\n",
    "\n",
    "This module handles **canonical train/validation/test splits** for the router dataset.  \n",
    "Key features:\n",
    "- Uses **SHA-256 hashing** to detect changes in CSV or preprocessing config.  \n",
    "- Ensures **stratified splits** by label distribution.  \n",
    "- Caches splits in a JSON file for reproducibility.  \n",
    "- Regenerates splits if the dataset or preprocessing config changes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d9c5b-ab82-4327-9959-83d3650dd3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Splitter ready.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 🔐 SHA-256 Hashing Utilities\n",
    "# -------------------------------\n",
    "def _sha256_file(Path: str, chunk_size: int = 1 << 20) -> str:\n",
    "    \"\"\"\n",
    "    Compute SHA-256 hash of a file.\n",
    "    Used to detect changes in dataset CSV.\n",
    "    \"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    with open(Path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def _sha256_text(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Compute SHA-256 hash of a text string.\n",
    "    Used to fingerprint preprocessing configs.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# ⚙️ Split Configuration\n",
    "# -------------------------------\n",
    "@dataclass(frozen=True)\n",
    "class RouterSplitConfig:\n",
    "    \"\"\"\n",
    "    Configuration for dataset splitting.\n",
    "    - csv_Path: Path to cleaned/preprocessed CSV\n",
    "    - split_Path: Path to JSON file storing canonical splits\n",
    "    - label_col: Column used for stratified splitting\n",
    "    - val_size: Fraction of data reserved for validation\n",
    "    - test_size: Fraction of data reserved for test set\n",
    "    - seed: Random seed for reproducibility\n",
    "    - preprocess: Optional preprocessing config to apply before splitting\n",
    "    \"\"\"\n",
    "    csv_Path: str\n",
    "    split_Path: str  # e.g., DATA_DIR/'router_splits.json'\n",
    "    label_col: str = \"label\"\n",
    "    val_size: float = 0.20\n",
    "    test_size: float = 0.15\n",
    "    seed: Optional[int] = None\n",
    "    preprocess: Optional[PreprocessConfig] = None\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 📊 Stratified Split Generator\n",
    "# -------------------------------\n",
    "def _make_stratified_splits(y: Sequence, val_size: float, test_size: float, seed: int) -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Create stratified train/val/test splits.\n",
    "    Preserves class distribution across splits.\n",
    "    \"\"\"\n",
    "    idx = np.arange(len(y))\n",
    "\n",
    "    # First split: Test set\n",
    "    if test_size and test_size > 0:\n",
    "        idx_tmp, idx_test = train_test_split(idx, test_size=test_size, random_state=seed, stratify=y)\n",
    "        y_tmp = [y[i] for i in idx_tmp]\n",
    "    else:\n",
    "        idx_tmp, idx_test = idx, np.array([], dtype=int)\n",
    "        y_tmp = y\n",
    "\n",
    "    # Second split: Validation set (relative to train size)\n",
    "    if val_size and val_size > 0:\n",
    "        rel_val = val_size if not test_size else val_size / (1.0 - test_size)\n",
    "        idx_train, idx_val = train_test_split(idx_tmp, test_size=rel_val, random_state=seed, stratify=y_tmp)\n",
    "    else:\n",
    "        idx_train, idx_val = idx_tmp, np.array([], dtype=int)\n",
    "\n",
    "    return {\n",
    "        \"train\": [int(i) for i in idx_train],\n",
    "        \"val\": [int(i) for i in idx_val],\n",
    "        \"test\": [int(i) for i in idx_test],\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 📂 Get or Create Splits\n",
    "# -------------------------------\n",
    "def get_or_create_splits(cfg: RouterSplitConfig, *, force: bool = False) -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Build (or load) a canonical split manifest on the **preprocessed** dataframe.\n",
    "    Regenerates splits if:\n",
    "      - CSV file changes (detected via SHA-256 hash)\n",
    "      - Preprocessing config changes (detected via SHA-256 hash)\n",
    "      - `force=True` is specified\n",
    "    Returns dictionary with 'train', 'val', 'test' indices.\n",
    "    \"\"\"\n",
    "    csv_Path = Path(cfg.csv_Path)\n",
    "    split_Path = Path(cfg.split_Path)\n",
    "    split_Path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Hash CSV to detect modifications\n",
    "    csv_sha = _sha256_file(str(csv_Path))\n",
    "    seed = cfg.seed if cfg.seed is not None else RANDOM_SEED\n",
    "\n",
    "    # Apply preprocessing before splitting (ensures stratification on clean labels)\n",
    "    raw_df = pd.read_csv(csv_Path)\n",
    "    pre_cfg = cfg.preprocess or PreprocessConfig(text_col=\"query\", context_col=\"context\", label_col=cfg.label_col)\n",
    "    df_clean, prep_report = preprocess_router_dataframe(raw_df, pre_cfg)\n",
    "    prep_fingerprint = _sha256_text(json.dumps(prep_report[\"preprocess_config\"], sort_keys=True))\n",
    "\n",
    "    # Metadata stored alongside split indices\n",
    "    desired_meta = {\n",
    "        \"csv_Path\": str(csv_Path.resolve()),\n",
    "        \"csv_sha256\": csv_sha,\n",
    "        \"label_col\": cfg.label_col,\n",
    "        \"val_size\": cfg.val_size,\n",
    "        \"test_size\": cfg.test_size,\n",
    "        \"seed\": int(seed),\n",
    "        \"format_version\": 2,\n",
    "        \"preprocess_sha256\": prep_fingerprint,\n",
    "    }\n",
    "\n",
    "    # --- Load existing split if valid ---\n",
    "    if split_Path.exists() and not force:\n",
    "        try:\n",
    "            saved = json.loads(split_Path.read_text())\n",
    "            meta_ok = all(saved.get(\"meta\", {}).get(k) == v for k, v in desired_meta.items())\n",
    "            if meta_ok and {\"train\", \"val\", \"test\"} <= set(saved.get(\"indices\", {}).keys()):\n",
    "                return saved[\"indices\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # --- Otherwise regenerate stratified splits ---\n",
    "    labels_for_strat = df_clean[cfg.label_col].tolist()\n",
    "    indices = _make_stratified_splits(labels_for_strat, cfg.val_size, cfg.test_size, seed)\n",
    "\n",
    "    # Save new split manifest\n",
    "    out = {\"meta\": desired_meta, \"indices\": indices}\n",
    "    split_Path.write_text(json.dumps(out, indent=2))\n",
    "    print(f\"🗂️  Saved canonical splits to: {split_Path}\")\n",
    "    return indices\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Ready Message\n",
    "# -------------------------------\n",
    "print(\"Data Splitter ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d68f50",
   "metadata": {},
   "source": [
    "## Build or Load Canonical Dataset Splits\n",
    "\n",
    "This snippet shows how to configure and run the **canonical train/validation/test split process** on the **preprocessed dataset**.  \n",
    "\n",
    "Steps:\n",
    "1. Define a `RouterSplitConfig` with CSV path, split manifest path, and preprocessing config.  \n",
    "2. Call `get_or_create_splits()` to either **load cached splits** (if valid) or **regenerate stratified splits** (if dataset or config changed).  \n",
    "3. Receive a dictionary with index lists for `\"train\"`, `\"val\"`, and `\"test\"`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326dc66-d198-4581-8a8c-acbaacd8804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# ⚙️ Split Configuration\n",
    "# -------------------------------\n",
    "split_cfg = RouterSplitConfig(\n",
    "    csv_Path=CSV_Path,       # Path to preprocessed training dataset CSV\n",
    "    split_Path=SPLIT_Path,   # Path where canonical splits JSON will be stored\n",
    "    seed=RANDOM_SEED,        # Random seed for reproducibility\n",
    "    val_size=0.20,           # 20% of data reserved for validation\n",
    "    test_size=0.15,          # 15% of data reserved for testing\n",
    "    preprocess=PREPROC,      # Preprocessing config (ensures clean labels before splitting)\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 🚀 Create or Load Splits\n",
    "# -------------------------------\n",
    "# - If a valid split JSON exists (and matches CSV + preprocess config fingerprint), reuse it.\n",
    "# - Otherwise, regenerate stratified splits (maintaining label balance).\n",
    "# - Returns dictionary of index arrays: {\"train\": [...], \"val\": [...], \"test\": [...]}\n",
    "splits = get_or_create_splits(split_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288db1f7-99f1-45ec-90dc-2f4d173231aa",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocessing Utilities\n",
    "\n",
    "This module provides:\n",
    "1. **`LogTransformer`** – a custom scikit-learn–compatible transformer for applying log-scaling.  \n",
    "2. **`_install_create_ratios_shim`** – a runtime shim to make sure the function `create_ratios` is always available, regardless of whether pickled pipelines expect it from `__main__` or from `preprocessing_utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae4c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing shims ready.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 📉 Log Transformer\n",
    "# -------------------------------\n",
    "class LogTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Log1p transformer compatible with scikit-learn pipelines.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting needed, identity return\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Apply log(1 + x) element-wise to avoid log(0) issues\n",
    "        return np.log1p(X)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        # Pass through feature names unchanged (for sklearn compatibility)\n",
    "        return input_features\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 🛠️ Install Ratios Shim\n",
    "# -------------------------------\n",
    "def _install_create_ratios_shim(results_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Install a runtime-compatible `create_ratios` function where pickles expect it.\n",
    "\n",
    "    Motivation:\n",
    "    - Saved pipelines may refer to `__main__.create_ratios` or `preprocessing_utils.create_ratios`.\n",
    "    - This shim ensures that unpickling works regardless of which reference is stored.\n",
    "\n",
    "    How it works:\n",
    "    - Looks up feature set definitions (e.g., ratio_definitions) from a saved pickle.\n",
    "    - Defines a local `create_ratios` function that constructs ratio features.\n",
    "    - Injects this function into both:\n",
    "        * `preprocessing_utils` module\n",
    "        * `__main__` module\n",
    "    \"\"\"\n",
    "\n",
    "    import types\n",
    "\n",
    "    # Ensure preprocessing_utils module exists (create if not loaded yet)\n",
    "    mod = sys.modules.setdefault('preprocessing_utils', types.ModuleType('preprocessing_utils'))\n",
    "\n",
    "    ratio_specs: Dict[str, Any] = {}\n",
    "    try:\n",
    "        # Try to load feature set definitions from pickle\n",
    "        fs_Path = os.path.join(results_dir, \"feature_sets.pkl\")\n",
    "        if os.path.exists(fs_Path):\n",
    "            fs = joblib.load(fs_Path)\n",
    "            if isinstance(fs, dict) and 'ratio_definitions' in fs:\n",
    "                ratio_specs = dict(fs['ratio_definitions'])\n",
    "    except Exception:\n",
    "        # Fail silently, fallback to empty ratio specs\n",
    "        pass\n",
    "\n",
    "    # --- Actual ratio creator ---\n",
    "    def create_ratios(df: pd.DataFrame, ratio_defs: dict | None = None, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add ratio-based features to a dataframe.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe.\n",
    "            ratio_defs (dict): Mapping of {new_column: (numerator_col, denominator_col)}.\n",
    "                               If None, falls back to loaded ratio_specs.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Extended dataframe with ratio features.\n",
    "        \"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            return df\n",
    "\n",
    "        out = df.copy()\n",
    "        defs = ratio_defs if ratio_defs is not None else ratio_specs\n",
    "\n",
    "        if not isinstance(defs, dict):\n",
    "            return out\n",
    "\n",
    "        for new_col, pair in defs.items():\n",
    "            # Expect ratio definition as a tuple/list of length 2\n",
    "            if not (isinstance(pair, (list, tuple)) and len(pair) == 2):\n",
    "                continue\n",
    "            num, den = pair\n",
    "\n",
    "            # Add ratio if both numerator and denominator exist\n",
    "            if num in out.columns and den in out.columns:\n",
    "                out[new_col] = out[num] / (out[den] + 1e-6)  # avoid div/0\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Inject function into both module references\n",
    "    setattr(mod, 'create_ratios', create_ratios)\n",
    "    setattr(sys.modules['__main__'], 'create_ratios', create_ratios)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Ready Message\n",
    "# -------------------------------\n",
    "print(\"Preprocessing shims ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f0afd",
   "metadata": {},
   "source": [
    "---\n",
    "## Dynamic LLM Query Router — Core Components\n",
    "\n",
    "This file defines the **core orchestration logic** of the router system.  \n",
    "It integrates:  \n",
    "- **Action space** (prediction, CBR, QA, summarize)  \n",
    "- **Schemas (Pydantic models)** for requests/decisions  \n",
    "- **Encoder & Policy network** for intent classification  \n",
    "- **Calibration** (temperature scaling, expected calibration error)  \n",
    "- **Contextual bandit (LinTS)** for uncertainty-aware routing  \n",
    "- **LLM intent classifier** (Hugging Face backbone)  \n",
    "- **Circuit breakers & targets** for service resilience  \n",
    "- **Router** class that unifies everything  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Action space (intent labels)\n",
    "# -----------------------------\n",
    "# The core set of router \"actions\" (a.k.a. intents) the system can route to.\n",
    "# Using Enum ensures type safety and consistent string values.\n",
    "class Action(str, Enum):\n",
    "    PREDICTION = \"prediction\"\n",
    "    CBR = \"cbr\"\n",
    "    QA = \"qa\"\n",
    "    SUMMARIZE = \"summarize\"\n",
    "\n",
    "# Flat list of string labels (e.g., [\"prediction\",\"cbr\",\"qa\",\"summarize\"])\n",
    "ACTIONS = [a.value for a in Action]\n",
    "# Number of actions\n",
    "A = len(ACTIONS)\n",
    "# Maps label string -> integer id (e.g., {\"prediction\":0,...})\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ACTIONS)}\n",
    "# Inverse map integer id -> label string (e.g., {0:\"prediction\",...})\n",
    "ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b5c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# API Schemas (Pydantic)\n",
    "# -----------------------------\n",
    "# Pydantic models define structured inputs/outputs for API calls.\n",
    "# RouteRequest: input payload for routing\n",
    "class RouteRequest(BaseModel):\n",
    "    query: str\n",
    "    context_text: Optional[str] = Field(None)\n",
    "    explicit_action: Optional[Action] = None\n",
    "\n",
    "# RouteDecision: the router's decision payload\n",
    "class RouteDecision(BaseModel):\n",
    "    chosen: Action\n",
    "    probs: Dict[str, float]\n",
    "    uncertainty: float\n",
    "    utility: Dict[str, float]\n",
    "    stage: str\n",
    "    trace_id: str\n",
    "    target_id: Optional[str] = None\n",
    "    why: Optional[Dict[str, Any]] = None\n",
    "\n",
    "# Feedback: user/system feedback to update learning signals\n",
    "class Feedback(BaseModel):\n",
    "    trace_id: str\n",
    "    chosen: Action\n",
    "    thumbs_up: Optional[bool] = None\n",
    "    task_success: Optional[bool] = None\n",
    "    quality: Optional[float] = Field(None, ge=0.0, le=1.0)\n",
    "    faithfulness: Optional[float] = Field(None, ge=0.0, le=1.0)\n",
    "\n",
    "# SupervisedBatch: batch container for supervised training of the policy\n",
    "class SupervisedBatch(BaseModel):\n",
    "    texts: List[str]\n",
    "    context_texts: Optional[List[Optional[str]]] = None\n",
    "    labels: List[Action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29406bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Utils\n",
    "# -----------------------------\n",
    "# Append a JSON record to the router event log (newline-delimited JSON).\n",
    "def log_event(kind: str, payload: Dict[str, Any]) -> None:\n",
    "    rec = {\"ts\": time.time(), \"kind\": kind, **payload}\n",
    "    with open(LOG_Path, \"a\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "# A lightweight guardlist for obvious prompt-injection patterns.\n",
    "INJECTION_TRIGGERS = [\n",
    "    \"ignore previous\", \"disregard previous\", \"bypass safety\", \"exfiltrate\",\n",
    "    \"system prompt\", \"developer instructions\", \"confidential\", \"leak data\",\n",
    "]\n",
    "# Heuristic detector for suspicious input content (query + context).\n",
    "def is_suspect_prompt(q: str, c: Optional[str]) -> bool:\n",
    "    text = (q or \"\") + \" \" + (c or \"\")\n",
    "    text_low = text.lower()\n",
    "    return any(t in text_low for t in INJECTION_TRIGGERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deae4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Encoder (Sentence-Transformers)\n",
    "# -----------------------------\n",
    "# Wraps sentence-transformers encoding and produces a feature vector:\n",
    "# - If context is present: concat [q, c, q*c, |q-c|]\n",
    "# - Else: just q\n",
    "class Encoder:\n",
    "    def __init__(self, name: str = EMBED_MODEL):\n",
    "        self.model = SentenceTransformer(name)\n",
    "        self.dim = self.model.get_sentence_embedding_dimension()\n",
    "    def encode_pair(self, query: str, ctx: Optional[str]) -> np.ndarray:\n",
    "        if ctx:\n",
    "            vecs = self.model.encode([query, ctx], convert_to_numpy=True, normalize_embeddings=True)\n",
    "            q, c = vecs[0], vecs[1]\n",
    "            feats = np.concatenate([q, c, q * c, np.abs(q - c)])\n",
    "        else:\n",
    "            q = self.model.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "            feats = q\n",
    "        return feats.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8473e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Policy Net (intent only)\n",
    "# -----------------------------\n",
    "# Simple MLP classifier that predicts intent logits given features.\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int = 512, n_actions: int = A):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(0.1),\n",
    "        )\n",
    "        self.intent = nn.Linear(hidden, n_actions)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h = self.backbone(x)\n",
    "        logits = self.intent(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f6f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Temperature Calibration + ECE\n",
    "# -----------------------------\n",
    "# Two calibrators:\n",
    "# - MixCalibrator operates on probabilities\n",
    "# - TemperatureCalibrator operates on logits\n",
    "# Both learn a single temperature parameter tau to rescale confidence.\n",
    "class MixCalibrator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_tau = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, probs: torch.Tensor) -> torch.Tensor:\n",
    "        eps = 1e-8\n",
    "        logits = torch.log(torch.clamp(probs, eps, 1 - eps))\n",
    "        tau = torch.exp(self.log_tau)\n",
    "        logits = logits / tau\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "    def save(self, Path: str):\n",
    "        with open(Path, \"w\") as f:\n",
    "            json.dump({\"log_tau\": float(self.log_tau.detach().cpu().item())}, f)\n",
    "    def load(self, Path: str):\n",
    "        if os.path.exists(Path):\n",
    "            d = json.load(open(Path, \"r\"))\n",
    "            self.log_tau.data = torch.tensor([d.get(\"log_tau\", 0.0)], dtype=torch.float32)\n",
    "\n",
    "class TemperatureCalibrator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_tau = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        tau = torch.exp(self.log_tau)\n",
    "        return logits / tau\n",
    "    def save(self, Path: str):\n",
    "        with open(Path, \"w\") as f:\n",
    "            json.dump({\"log_tau\": float(self.log_tau.detach().cpu().item())}, f)\n",
    "    def load(self, Path: str):\n",
    "        if os.path.exists(Path):\n",
    "            d = json.load(open(Path, \"r\"))\n",
    "            self.log_tau.data = torch.tensor([d.get(\"log_tau\", 0.0)], dtype=torch.float32)\n",
    "\n",
    "# Expected Calibration Error (ECE) for measuring calibration quality.\n",
    "def expected_calibration_error(probs: np.ndarray, labels: np.ndarray, n_bins: int = 15) -> float:\n",
    "    confidences = probs.max(axis=1)\n",
    "    predictions = probs.argmax(axis=1)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        start, end = bins[i], bins[i + 1]\n",
    "        mask = (confidences > start) & (confidences <= end)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        acc = np.mean(predictions[mask] == labels[mask])\n",
    "        conf = np.mean(confidences[mask])\n",
    "        ece += (np.sum(mask) / len(labels)) * abs(acc - conf)\n",
    "    return float(ece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Contextual Bandit: LinTS\n",
    "# -----------------------------\n",
    "# A lightweight Linear Thompson Sampling contextual bandit. Maintains\n",
    "# per-action (A,b) state per bucket to explore/exploit target choices.\n",
    "@dataclass\n",
    "class LinTSArm:\n",
    "    A: np.ndarray\n",
    "    b: np.ndarray\n",
    "\n",
    "class LinTS:\n",
    "    def __init__(self, d: int, Path: str = BANDIT_STATE_Path, reg: float = 1.0):\n",
    "        self.d = d\n",
    "        self.Path = Path\n",
    "        self.reg = reg\n",
    "        self.state: Dict[str, Dict[str, LinTSArm]] = {}\n",
    "        self._load()\n",
    "        if not self.state:\n",
    "            self._init_bucket(\"general\")\n",
    "    def _init_bucket(self, bucket: str):\n",
    "        self.state[bucket] = {a: LinTSArm(A=np.eye(self.d) * self.reg, b=np.zeros(self.d)) for a in ACTIONS}\n",
    "    def _load(self):\n",
    "        if os.path.exists(self.Path):\n",
    "            try:\n",
    "                raw = json.load(open(self.Path, \"r\"))\n",
    "                self.state = {\n",
    "                    bucket: {a: LinTSArm(A=np.array(v[\"A\"]), b=np.array(v[\"b\"])) for a, v in arms.items()}\n",
    "                    for bucket, arms in raw.items()\n",
    "                }\n",
    "            except Exception:\n",
    "                self.state = {}\n",
    "    def _save(self):\n",
    "        serial = {bucket: {a: {\"A\": arm.A.tolist(), \"b\": arm.b.tolist()} for a, arm in arms.items()} for bucket, arms in self.state.items()}\n",
    "        json.dump(serial, open(self.Path, \"w\"))\n",
    "    def ensure(self, bucket: str):\n",
    "        if bucket not in self.state:\n",
    "            self._init_bucket(bucket)\n",
    "            self._save()\n",
    "    # Sample a parameter vector theta ~ N(A^-1 b, A^-1) for a given arm\n",
    "    def sample_theta(self, arm: LinTSArm) -> np.ndarray:\n",
    "        A_inv = np.linalg.inv(arm.A)\n",
    "        mu = A_inv @ arm.b\n",
    "        return np.random.multivariate_normal(mu, A_inv)\n",
    "    # Select the action with highest sampled score theta·x\n",
    "    def select(self, bucket: str, ctx: np.ndarray) -> str:\n",
    "        self.ensure(bucket)\n",
    "        scores: Dict[str, float] = {}\n",
    "        for a, arm in self.state[bucket].items():\n",
    "            theta = self.sample_theta(arm)\n",
    "            scores[a] = float(theta @ ctx)\n",
    "        return max(scores, key=scores.get)\n",
    "    # Update the (A,b) for the chosen action given context x and reward r\n",
    "    def update(self, bucket: str, action: str, ctx: np.ndarray, reward: float):\n",
    "        self.ensure(bucket)\n",
    "        arm = self.state[bucket][action]\n",
    "        x = ctx.reshape(-1)\n",
    "        arm.A += np.outer(x, x)\n",
    "        arm.b += reward * x\n",
    "        self._save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb10a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# LLM Intent Classifier\n",
    "# -----------------------------\n",
    "# Dataset wrapper combining query and optional context into a single text.\n",
    "class IntentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, texts: List[str], ctxs: List[Optional[str]], labels: Optional[List[int]] = None, max_length: int = 256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.ctxs = ctxs\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "    def _combine(self, q: str, c: Optional[str]) -> str:\n",
    "        parts = [q]\n",
    "        if c:\n",
    "            parts.append(\"[CTX] \" + c)\n",
    "        return \" \\n\".join(parts)\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        text = self._combine(self.texts[i], self.ctxs[i])\n",
    "        enc = self.tokenizer(text, truncation=True, max_length=self.max_length)\n",
    "        item = {k: torch.tensor(v) for k, v in enc.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[i], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# HF Trainer args vary across versions; this helper filters kwargs to compatible ones.\n",
    "def make_training_args(**kwargs):\n",
    "    # sig = signature(TrainingArguments.__init__)\n",
    "    sig = inspect.signature(TrainingArguments.__init__)\n",
    "    params = sig.parameters.keys()\n",
    "    valid: Dict[str, Any] = {}\n",
    "    if \"evaluation_strategy\" in kwargs and \"evaluation_strategy\" not in params:\n",
    "        if \"eval_strategy\" in params:\n",
    "            valid[\"eval_strategy\"] = kwargs[\"evaluation_strategy\"]\n",
    "        else:\n",
    "            if \"do_eval\" in params:\n",
    "                valid[\"do_eval\"] = True\n",
    "    for k in (\"save_strategy\", \"logging_strategy\"):\n",
    "        if k in kwargs and k not in params:\n",
    "            pass\n",
    "        elif k in kwargs:\n",
    "            valid[k] = kwargs[k]\n",
    "    for k, v in kwargs.items():\n",
    "        if k in (\"evaluation_strategy\", \"save_strategy\", \"logging_strategy\"):\n",
    "            continue\n",
    "        if k in params:\n",
    "            valid[k] = v\n",
    "    if \"save_steps\" in params and \"save_steps\" not in valid:\n",
    "        valid[\"save_steps\"] = 500\n",
    "    if \"logging_steps\" in params and \"logging_steps\" not in valid and \"logging_strategy\" not in valid:\n",
    "        valid[\"logging_steps\"] = kwargs.get(\"logging_steps\", 50)\n",
    "    return TrainingArguments(**valid)\n",
    "    \n",
    "# --- Validate HF model dirs (unified helper + wrappers) ---\n",
    "# Validates a Hugging Face model/checkpoint directory in either strict or lenient mode.\n",
    "def _is_valid_hf_checkpoint_dir(path: Union[str, os.PathLike], *, strict: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    strict=True  -> require config.json with a dict containing 'model_type'\n",
    "    strict=False -> accept any common HF artifact file or a 'checkpoint-*' subdir\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.is_dir():\n",
    "        return False\n",
    "\n",
    "    if strict:\n",
    "        cfg = p / \"config.json\"\n",
    "        if not cfg.exists():\n",
    "            return False\n",
    "        try:\n",
    "            import json as _json\n",
    "            data = _json.loads(cfg.read_text())\n",
    "            return isinstance(data, dict) and \"model_type\" in data\n",
    "        except Exception:\n",
    "            return False\n",
    "    else:\n",
    "        # any common HF artifact or at least one checkpoint-* subdir\n",
    "        candidates = {\n",
    "            \"config.json\",\n",
    "            \"pytorch_model.bin\",\n",
    "            \"model.safetensors\",\n",
    "            \"adapter_config.json\",\n",
    "            \"training_args.bin\",\n",
    "        }\n",
    "        if any((p / f).is_file() for f in candidates):\n",
    "            return True\n",
    "        return any(\n",
    "            child.is_dir() and child.name.startswith(\"checkpoint-\")\n",
    "            for child in p.iterdir()\n",
    "        )\n",
    "\n",
    "# LLMIntent encapsulates tokenizer/model initialization, training, and probability prediction.\n",
    "class LLMIntent:\n",
    "    def __init__(self, backbone: str = LLM_BACKBONE):\n",
    "        self.backbone = backbone\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.id2label = ID2LABEL\n",
    "        self.label2id = LABEL2ID\n",
    "\n",
    "    # Initialize from a saved checkpoint if valid; otherwise from the backbone\n",
    "    def init_or_load(self, model_dir: Optional[str] = None):\n",
    "        # Only treat model_dir as a saved checkpoint if it looks valid; else fall back to backbone\n",
    "        if model_dir and _is_valid_hf_checkpoint_dir(model_dir, strict=True):\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.backbone)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                self.backbone,\n",
    "                num_labels=len(self.id2label),\n",
    "                id2label=self.id2label,\n",
    "                label2id=self.label2id,\n",
    "            )\n",
    "        self.model.to(DEVICE)\n",
    "    \n",
    "    # Fine-tune the intent classifier using HF Trainer, then save and reload best model.\n",
    "    def train(\n",
    "        self,\n",
    "        texts, ctxs, labels,\n",
    "        val_texts, val_ctxs, val_labels,\n",
    "        output_dir: str = LLM_DIR,\n",
    "        epochs: int = 2,\n",
    "        batch_size: int = 16,\n",
    "        lr: float = 5e-5,\n",
    "    ):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.init_or_load(None)\n",
    "        train_ds = IntentDataset(self.tokenizer, texts, ctxs, labels)\n",
    "        val_ds   = IntentDataset(self.tokenizer, val_texts, val_ctxs, val_labels)\n",
    "        args = make_training_args(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=epochs,\n",
    "            learning_rate=lr,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=50,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            save_total_limit=2,\n",
    "            seed=RANDOM_SEED,\n",
    "            report_to=[],\n",
    "            dataloader_pin_memory=False,\n",
    "        )\n",
    "        data_collator = DataCollatorWithPadding(self.tokenizer)\n",
    "        def compute_metrics(eval_pred):\n",
    "            from sklearn.metrics import accuracy_score\n",
    "            logits, labels_np = eval_pred\n",
    "            y_pred = logits.argmax(axis=1)\n",
    "            return {\"accuracy\": accuracy_score(labels_np, y_pred)}\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        trainer.train()\n",
    "        try:\n",
    "            trainer.evaluate()\n",
    "        except Exception:\n",
    "            pass\n",
    "        trainer.save_model(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        self.init_or_load(output_dir)\n",
    "        return {\"saved_to\": output_dir}\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(\n",
    "        self, texts: List[str], ctxs: List[Optional[str]], batch_size: int = 16, max_length: int = 256\n",
    "    ) -> np.ndarray:\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            self.init_or_load(LLM_DIR if os.path.exists(LLM_DIR) else None)\n",
    "        # Reuse existing dataset to avoid duplication; identical tokenization behavior.\n",
    "        ds = IntentDataset(\n",
    "            tokenizer=self.tokenizer, texts=texts, ctxs=ctxs, labels=None, max_length=max_length\n",
    "        )\n",
    "        collate = DataCollatorWithPadding(self.tokenizer)\n",
    "        loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "        self.model.eval()\n",
    "        out_probs = []\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            logits = self.model(**batch).logits\n",
    "            probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "            out_probs.append(probs)\n",
    "        return np.vstack(out_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa721b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Circuit Breakers & Targets\n",
    "# -----------------------------\n",
    "# CircuitBreakerConfig controls thresholds/timeouts for the circuit breaker pattern.\n",
    "@dataclass\n",
    "class CircuitBreakerConfig:\n",
    "    failure_threshold: int = 5\n",
    "    recovery_time_ms: int = 30_000\n",
    "    half_open_max_calls: int = 3\n",
    "\n",
    "# Internal mutable state for the circuit breaker.\n",
    "class CircuitBreakerState:\n",
    "    def __init__(self, cfg: CircuitBreakerConfig):\n",
    "        self.cfg = cfg\n",
    "        self.failures = 0\n",
    "        self.state = \"CLOSED\"\n",
    "        self.opened_at_ms = 0\n",
    "        self.half_open_calls = 0\n",
    "\n",
    "# Basic circuit breaker: CLOSED -> OPEN on repeated failures, HALF_OPEN to test recovery.\n",
    "class CircuitBreaker:\n",
    "    def __init__(self, cfg: Optional[CircuitBreakerConfig] = None):\n",
    "        self.cfg = cfg or CircuitBreakerConfig()\n",
    "        self._state = CircuitBreakerState(self.cfg)\n",
    "        self._lock = _thread.Lock()\n",
    "    def allow(self) -> bool:\n",
    "        with self._lock:\n",
    "            now_ms = int(time.time() * 1000)\n",
    "            if self._state.state == \"OPEN\":\n",
    "                if now_ms - self._state.opened_at_ms >= self.cfg.recovery_time_ms:\n",
    "                    self._state.state = \"HALF_OPEN\"\n",
    "                    self._state.half_open_calls = 0\n",
    "                else:\n",
    "                    return False\n",
    "            if self._state.state == \"HALF_OPEN\":\n",
    "                if self._state.half_open_calls >= self.cfg.half_open_max_calls:\n",
    "                    return False\n",
    "                self._state.half_open_calls += 1\n",
    "            return True\n",
    "    def on_success(self):\n",
    "        with self._lock:\n",
    "            self._state.failures = 0\n",
    "            self._state.state = \"CLOSED\"\n",
    "            self._state.half_open_calls = 0\n",
    "    def on_failure(self):\n",
    "        with self._lock:\n",
    "            self._state.failures += 1\n",
    "            if self._state.failures >= self.cfg.failure_threshold:\n",
    "                self._state.state = \"OPEN\"\n",
    "                self._state.opened_at_ms = int(time.time() * 1000)\n",
    "    def snapshot(self) -> Dict[str, Any]:\n",
    "        with self._lock:\n",
    "            return {\n",
    "                \"state\": self._state.state,\n",
    "                \"failures\": self._state.failures,\n",
    "                \"opened_at_ms\": self._state.opened_at_ms,\n",
    "                \"half_open_calls\": self._state.half_open_calls,\n",
    "            }\n",
    "\n",
    "# TargetSpec: describes an invocable model/service endpoint.\n",
    "@dataclass\n",
    "class TargetSpec:\n",
    "    id: str\n",
    "    action: str\n",
    "    provider: str\n",
    "    model: str\n",
    "    base_latency_ms: float = 500.0\n",
    "    cost_per_ktok_usd: float = 0.0\n",
    "    timeout_ms: int = DEFAULT_TARGET_TIMEOUT_MS\n",
    "    health_cb: Callable[[], bool] = lambda: True\n",
    "    breaker: CircuitBreaker = field(default_factory=CircuitBreaker)\n",
    "\n",
    "# Registry of available targets, grouped by action type.\n",
    "class TargetRegistry:\n",
    "    def __init__(self):\n",
    "        self._by_action: Dict[str, List[TargetSpec]] = defaultdict(list)\n",
    "    def register(self, spec: TargetSpec):\n",
    "        self._by_action[spec.action].append(spec)\n",
    "    def healthy_targets(self, action: str) -> List[TargetSpec]:\n",
    "        out: List[TargetSpec] = []\n",
    "        for t in self._by_action.get(action, []):\n",
    "            try:\n",
    "                ok = t.health_cb()\n",
    "            except Exception:\n",
    "                ok = False\n",
    "            if ok and t.breaker.allow():\n",
    "                out.append(t)\n",
    "        return out\n",
    "    def all_targets(self, action: str) -> List[TargetSpec]:\n",
    "        return list(self._by_action.get(action, []))\n",
    "\n",
    "# Global registry instance\n",
    "TARGETS = TargetRegistry()\n",
    "\n",
    "# Default targets registered for each action, mixing local and vendor examples.\n",
    "def _register_default_targets() -> None:\n",
    "    # QA\n",
    "    TARGETS.register(TargetSpec(id=\"qa.local.mistral\", action=Action.QA.value, provider=\"local\", model=\"mistral-7b\", base_latency_ms=900))\n",
    "    TARGETS.register(TargetSpec(id=\"qa.vendor.fast\", action=Action.QA.value, provider=\"vendorA\", model=\"qa-small\", base_latency_ms=350, cost_per_ktok_usd=0.0008))\n",
    "    # Summarize\n",
    "    TARGETS.register(TargetSpec(id=\"summarize.local.mistral\", action=Action.SUMMARIZE.value, provider=\"local\", model=\"mistral-7b\", base_latency_ms=900))\n",
    "    TARGETS.register(TargetSpec(id=\"summarize.vendor.fast\", action=Action.SUMMARIZE.value, provider=\"vendorA\", model=\"summarize-small\", base_latency_ms=350, cost_per_ktok_usd=0.0008))\n",
    "    # CBR\n",
    "    TARGETS.register(TargetSpec(id=\"cbr.local.cosine\", action=Action.CBR.value, provider=\"local\", model=\"cbr-cosine\", base_latency_ms=150))\n",
    "    # prediction\n",
    "    TARGETS.register(TargetSpec(id=\"prediction.local.xgb\", action=Action.PREDICTION.value, provider=\"local\", model=\"xgboost\", base_latency_ms=80))\n",
    "    TARGETS.register(TargetSpec(id=\"prediction.local.mlp\", action=Action.PREDICTION.value, provider=\"local\", model=\"mlp\", base_latency_ms=120))\n",
    "\n",
    "_register_default_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ee50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core router components defined.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Router\n",
    "# -----------------------------\n",
    "# Cache for last route decisions keyed by (query,ctx), with TTL.\n",
    "_ROUTE_CACHE: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "# The central orchestrator: encodes inputs, applies ML+LLM, calibrates, optionally\n",
    "# defers to bandit under uncertainty, selects targets (with hedging), logs decisions.\n",
    "class Router:\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder(EMBED_MODEL)\n",
    "        self.proj_dim = self.encoder.dim * 4 if self.encoder.dim <= 384 else self.encoder.dim\n",
    "        self.projector = nn.Linear(self.proj_dim, self.proj_dim, bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.projector.weight.data.copy_(torch.eye(self.proj_dim))\n",
    "        self.policy = PolicyNet(self.proj_dim).to(DEVICE)\n",
    "        self.mix_calib = MixCalibrator().to(DEVICE)\n",
    "        self.calib = TemperatureCalibrator().to(DEVICE)\n",
    "        self.bandit = LinTS(d=self.proj_dim)\n",
    "        self.kmeans: Optional[KMeans] = None\n",
    "        rng = np.random.RandomState(RANDOM_SEED)\n",
    "        self._rp = rng.randn(self.proj_dim, HASH_BITS).astype(np.float32)\n",
    "        self._decision_cache: Dict[str, Tuple[float, Any]] = {}\n",
    "        self._cache_ttl_sec = 60.0\n",
    "        self.llm_intent = LLMIntent(LLM_BACKBONE) if USE_LLM_INTENT else None\n",
    "        self._load()\n",
    "    # Load trained policy + calibrators (if present)\n",
    "    def _load(self) -> None:\n",
    "        if os.path.exists(POLICY_Path):\n",
    "            state = torch.load(POLICY_Path, map_location=DEVICE)\n",
    "            self.projector.load_state_dict(state[\"projector\"])  # type: ignore\n",
    "            self.policy.load_state_dict(state[\"policy\"])        # type: ignore\n",
    "        self.calib.load(CALIB_Path)\n",
    "        self.mix_calib.load(os.path.join(str(DATA_DIR), \"mix_temperature.json\"))\n",
    "    # Persist current policy/calibration parameters to disk\n",
    "    def _save(self) -> None:\n",
    "        torch.save({\"projector\": self.projector.state_dict(), \"policy\": self.policy.state_dict()}, POLICY_Path)\n",
    "        self.calib.save(CALIB_Path)\n",
    "        self.mix_calib.save(os.path.join(str(DATA_DIR), \"mix_temperature.json\"))\n",
    "    # Encode and pad/truncate to projector dimension\n",
    "    def _base_features(self, query: str, ctx: Optional[str]) -> np.ndarray:\n",
    "        vec = self.encoder.encode_pair(query, ctx)\n",
    "        if vec.shape[0] < self.proj_dim:\n",
    "            vec = np.concatenate([vec, np.zeros(self.proj_dim - vec.shape[0], dtype=np.float32)])\n",
    "        return vec[: self.proj_dim]\n",
    "    # Hook for adding extra features; currently returns base features.\n",
    "    def _features(self, query: str, ctx: Optional[str]) -> np.ndarray:\n",
    "        return self._base_features(query, ctx)\n",
    "    # Produce a SimHash-like bucket code from random projections (fast, unsupervised).\n",
    "    def _bucket_simhash(self, x_vec: np.ndarray) -> str:\n",
    "        proj = x_vec @ self._rp\n",
    "        bits = (proj >= 0).astype(np.uint8)\n",
    "        code = 0\n",
    "        for b in bits:\n",
    "            code = (code << 1) | int(b)\n",
    "        return f\"sim_{code:0{max(1, (HASH_BITS + 3)//4)}x}\"\n",
    "    # Choose a bucket: learned clusters (if trained) or SimHash code\n",
    "    def _bucket(self, query: str, ctx: Optional[str]) -> str:\n",
    "        x = self._base_features(query, ctx).astype(np.float32, copy=False)\n",
    "        if LEARNED_BUCKETS and (self.kmeans is not None):\n",
    "            cid = int(self.kmeans.predict([x])[0])\n",
    "            return f\"cluster_{cid}\"\n",
    "        return self._bucket_simhash(x)\n",
    "    # Optional diagnostic signals based on query-context embedding similarity.\n",
    "    def _pair_signals(self, query: str, ctx: Optional[str]) -> Dict[str, Any]:\n",
    "        out: Dict[str, Any] = {}\n",
    "        if ctx:\n",
    "            vecs = self.encoder.model.encode([query, ctx], convert_to_numpy=True, normalize_embeddings=True)\n",
    "            out[\"ctx_cosine\"] = float(np.dot(vecs[0], vecs[1]))\n",
    "        return out\n",
    "    # Construct a human-readable \"why\" explanation blob.\n",
    "    def _why(self, query: str, ctx: Optional[str], probs: Dict[str, float], chosen: str, stage: str, bucket: str, uncertainty: float, target_id: Optional[str]) -> Dict[str, Any]:\n",
    "        order = sorted(ACTIONS, key=lambda a: probs[a], reverse=True)\n",
    "        top = order[0]\n",
    "        second = order[1] if len(order) > 1 else None\n",
    "        why = {\n",
    "            \"decision_basis\": stage,\n",
    "            \"bucket\": bucket,\n",
    "            \"uncertainty\": uncertainty,\n",
    "            \"target_id\": target_id,\n",
    "            \"chosen\": {\"action\": top, \"prob\": probs[top]},\n",
    "            \"alternative\": {\n",
    "                \"action\": second,\n",
    "                \"prob\": probs[second] if second else None,\n",
    "                \"delta_prob\": (probs[top] - probs[second]) if second else None,\n",
    "            },\n",
    "            \"signals\": self._pair_signals(query, ctx),\n",
    "            \"thresholds\": {\"UNCERTAINTY_T\": UNCERTAINTY_T, \"HEDGE_UNCERTAINTY_T\": HEDGE_UNCERTAINTY_T},\n",
    "        }\n",
    "        return why\n",
    "    # Backfill explanation for a previous route decision by trace_id.\n",
    "    def explain_last(self, trace_id: str, exec_log: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        rec = _ROUTE_CACHE.get(trace_id)\n",
    "        if not rec:\n",
    "            return {}\n",
    "        probs = exec_log.get(\"probs\", {})\n",
    "        return self._why(\n",
    "            query=rec[\"query\"], ctx=rec[\"ctx\"],\n",
    "            probs=probs, chosen=exec_log.get(\"chosen\"), stage=exec_log.get(\"stage\"),\n",
    "            bucket=rec[\"bucket\"], uncertainty=exec_log.get(\"uncertainty\", 0.0), target_id=exec_log.get(\"target_id\"),\n",
    "        )\n",
    "    # Rank targets for an action, prioritizing lower latency then lower cost.\n",
    "    def select_targets(self, action: str, probs_vec: np.ndarray, k: int = 1) -> List[TargetSpec]:\n",
    "        candidates = TARGETS.healthy_targets(action) or TARGETS.all_targets(action)\n",
    "        if not candidates:\n",
    "            return []\n",
    "        ranked = sorted(candidates, key=lambda t: (t.base_latency_ms, t.cost_per_ktok_usd))\n",
    "        return ranked[:k]\n",
    "    # Main routing method:\n",
    "    # - Checks cache\n",
    "    # - Injection guard\n",
    "    # - Policy(+LLM) prediction + calibration\n",
    "    # - Uncertainty-based bandit fallback\n",
    "    # - Target selection (with hedging) and logging\n",
    "    @torch.no_grad()\n",
    "    def route(self, query: str, ctx: Optional[str], explicit: Optional[Action]) -> Tuple[str, Dict[str, float], float, Dict[str, float], str, str, Optional[str]]:\n",
    "        key = json.dumps({\"q\": query, \"c\": ctx}, sort_keys=True)\n",
    "        now = time.time()\n",
    "        if key in self._decision_cache:\n",
    "            ts, res = self._decision_cache[key]\n",
    "            if now - ts <= self._cache_ttl_sec:\n",
    "                chosen, probs_dict, uncertainty, utility, _old_trace, stage, target_id = res\n",
    "                trace = str(uuid.uuid4())\n",
    "                bucket = self._bucket(query, ctx)\n",
    "                x_np = self._features(query, ctx)\n",
    "                x_proj = self.projector(torch.from_numpy(x_np).to(DEVICE)).detach().cpu().numpy()\n",
    "                _ROUTE_CACHE[trace] = {\"query\": query, \"ctx\": ctx, \"chosen\": chosen, \"bucket\": bucket, \"x_proj\": x_proj}\n",
    "                log_event(\"route\", {\"trace_id\": trace, \"query\": query, \"context_present\": bool(ctx), \"probs\": probs_dict, \"uncertainty\": uncertainty, \"chosen\": chosen, \"stage\": stage + \"+cache\", \"bucket\": bucket, \"target_id\": target_id})\n",
    "                return chosen, probs_dict, uncertainty, utility, trace, stage + \"+cache\", target_id\n",
    "        if is_suspect_prompt(query, ctx):\n",
    "            chosen = Action.QA.value\n",
    "            probs_dict = {a: (1.0 if a == chosen else 0.0) for a in ACTIONS}\n",
    "            uncertainty = 0.0\n",
    "            utility = {a: probs_dict[a] for a in ACTIONS}\n",
    "            trace = str(uuid.uuid4())\n",
    "            stage = \"guard_injection\"\n",
    "            # target_id = (self.select_targets(chosen, np.array([1.0])) or [None])[0].id if self.select_targets(chosen, np.array([1.0])) else None\n",
    "            top_targets = self.select_targets(chosen, np.array([1.0]))\n",
    "            target_id = top_targets[0].id if top_targets else None\n",
    "            bucket = self._bucket(query, ctx)\n",
    "            x_np = self._features(query, ctx)\n",
    "            x_proj = self.projector(torch.from_numpy(x_np).to(DEVICE)).detach().cpu().numpy()\n",
    "            _ROUTE_CACHE[trace] = {\"query\": query, \"ctx\": ctx, \"chosen\": chosen, \"bucket\": bucket, \"x_proj\": x_proj}\n",
    "            log_event(\"route\", {\"trace_id\": trace, \"query\": query, \"context_present\": bool(ctx), \"probs\": probs_dict, \"uncertainty\": uncertainty, \"chosen\": chosen, \"stage\": stage, \"bucket\": bucket, \"target_id\": target_id})\n",
    "            res = (chosen, probs_dict, uncertainty, utility, trace, stage, target_id)\n",
    "            self._decision_cache[key] = (time.time(), res)\n",
    "            return res\n",
    "        self.policy.eval(); self.calib.eval(); self.mix_calib.eval()\n",
    "        trace = str(uuid.uuid4())\n",
    "        x_np = self._features(query, ctx)\n",
    "        x = torch.from_numpy(x_np).to(DEVICE)\n",
    "        x_proj = self.projector(x)\n",
    "        logits = self.policy(x_proj)\n",
    "        logits = self.calib(logits)\n",
    "        policy_probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "        if self.llm_intent is not None:\n",
    "            llm_probs = self.llm_intent.predict_proba([query], [ctx])[0]\n",
    "            probs = (LLM_WEIGHT * llm_probs) + ((1.0 - LLM_WEIGHT) * policy_probs)\n",
    "        else:\n",
    "            probs = policy_probs\n",
    "        probs = self.mix_calib(torch.from_numpy(probs).unsqueeze(0).float()).squeeze(0).cpu().numpy()\n",
    "        probs_dict = {a: float(probs[i]) for i, a in enumerate(ACTIONS)}\n",
    "        max_p = float(probs.max())\n",
    "        uncertainty = 1.0 - max_p\n",
    "        utility = {a: float(probs[i]) for i, a in enumerate(ACTIONS)}\n",
    "        if explicit is not None:\n",
    "            chosen = explicit.value\n",
    "            stage = \"forced\"\n",
    "        else:\n",
    "            if max_p < UNCERTAINTY_T:\n",
    "                bucket = self._bucket(query, ctx)\n",
    "                chosen = self.bandit.select(bucket, x_proj.detach().cpu().numpy())\n",
    "                stage = \"ml+bandit\"\n",
    "            else:\n",
    "                chosen = ACTIONS[int(np.argmax(probs))]\n",
    "                stage = \"ml_prob\"\n",
    "        top_targets = self.select_targets(chosen, probs, k=max(1, (HEDGE_MAX_TARGETS if USE_HEDGING and uncertainty >= HEDGE_UNCERTAINTY_T else 1)))\n",
    "        target_id = top_targets[0].id if top_targets else None\n",
    "        bucket = self._bucket(query, ctx)\n",
    "        _ROUTE_CACHE[trace] = {\"query\": query, \"ctx\": ctx, \"chosen\": chosen, \"bucket\": bucket, \"x_proj\": x_proj.detach().cpu().numpy(), \"target_id\": target_id}\n",
    "        log_event(\"route\", {\"trace_id\": trace, \"query\": query, \"context_present\": bool(ctx), \"probs\": probs_dict, \"uncertainty\": uncertainty, \"chosen\": chosen, \"stage\": stage, \"bucket\": bucket, \"target_id\": target_id})\n",
    "        res = (chosen, probs_dict, uncertainty, utility, trace, stage, target_id)\n",
    "        self._decision_cache[key] = (time.time(), res)\n",
    "        return res\n",
    "    # Supervised training loop for the policy + projector + calibrator\n",
    "    def train_supervised(self, batch: SupervisedBatch, lr: float = 1e-3, epochs: int = 3):\n",
    "        self.policy.train(); self.projector.train(); self.calib.train()\n",
    "        opt = torch.optim.Adam(list(self.policy.parameters()) + list(self.projector.parameters()) + list(self.calib.parameters()), lr=lr, weight_decay=1e-4)\n",
    "        texts = batch.texts\n",
    "        ctxs  = batch.context_texts or [None] * len(texts)\n",
    "        labels = [ACTIONS.index(l.value if isinstance(l, Action) else str(l)) for l in batch.labels]\n",
    "        y = torch.tensor(labels, dtype=torch.long)\n",
    "        X_np = [self._features(q, c) for q, c in zip(texts, ctxs)]\n",
    "        X = torch.from_numpy(np.stack(X_np)).float()\n",
    "        counts = Counter([ACTIONS[i] for i in labels])\n",
    "        class_counts = np.array([counts.get(a, 0) for a in ACTIONS], dtype=np.float32)\n",
    "        weights_np = (class_counts.sum() / (len(ACTIONS) * np.maximum(class_counts, 1.0)))\n",
    "        weights = torch.tensor(weights_np, dtype=torch.float32).to(DEVICE)\n",
    "        for _ in range(epochs):\n",
    "            opt.zero_grad()\n",
    "            Xp = self.projector(X.to(DEVICE))\n",
    "            logits = self.policy(Xp)\n",
    "            logits = self.calib(logits)\n",
    "            loss_cls  = F.cross_entropy(logits, y.to(DEVICE), weight=weights)\n",
    "            loss = loss_cls\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(list(self.policy.parameters()) + list(self.projector.parameters()), max_norm=1.0)\n",
    "            opt.step()\n",
    "        self._save()\n",
    "        return {\"loss\": float(loss.detach().cpu().item())}\n",
    "    # Update the bandit with a scalar reward for a given trace\n",
    "    def bandit_update(self, trace_id: str, reward: float):\n",
    "        rec = _ROUTE_CACHE.get(trace_id)\n",
    "        if not rec:\n",
    "            return {\"status\": \"miss\"}\n",
    "        self.bandit.update(rec[\"bucket\"], rec[\"chosen\"], rec[\"x_proj\"], reward)\n",
    "        return {\"status\": \"ok\"}\n",
    "    # Fit KMeans buckets on features for learned bucketization\n",
    "    def fit_buckets(self, texts: List[str], ctxs: List[Optional[str]]):\n",
    "        if not LEARNED_BUCKETS:\n",
    "            return {\"status\": \"skipped\"}\n",
    "        X = [self._base_features(q, c) for q, c in zip(texts, ctxs)]\n",
    "        X = np.stack(X).astype(np.float64, copy=False)\n",
    "        self.kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=RANDOM_SEED, n_init=\"auto\")\n",
    "        self.kmeans.fit(X)\n",
    "        return {\"status\": \"ok\", \"clusters\": N_CLUSTERS}\n",
    "\n",
    "print(\"Core router components defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd328b",
   "metadata": {},
   "source": [
    "---\n",
    "## Tool Discovery Utility\n",
    "\n",
    "This script automatically **detects and registers available tools** in the project workspace.  \n",
    "Each tool is organized in its own folder, containing at least one Jupyter Notebook (`.ipynb`).  \n",
    "\n",
    "Currently supported tools:\n",
    "- **RAG** → Retrieval-Augmented Generation (`QA+SUMMARIZATION RAG Modal Final`)\n",
    "- **PREDICTION** → Prediction Models (`Prediction modal Final`)\n",
    "- **CBR** → Case-Based Reasoning (`Case Based Reasoning Final`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c99ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovering tools...\n",
      "Discovered tools: ['RAG', 'PREDICTION', 'CBR']\n"
     ]
    }
   ],
   "source": [
    "def discover_tools(base_path: str = \".\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Scan the directory structure for tool folders and identify their main notebook.\n",
    "\n",
    "    Purpose:\n",
    "    --------\n",
    "    - Each \"tool\" (RAG, Prediction, CBR) is implemented in its own folder.\n",
    "    - Each folder contains a representative `.ipynb` notebook file.\n",
    "    - This function automatically finds these notebooks to register tools dynamically.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str\n",
    "        The root path where tool folders are located (default: current directory \".\").\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, str]\n",
    "        Mapping of tool name (e.g., \"RAG\", \"PREDICTION\", \"CBR\") → full path to its notebook file.\n",
    "        If a folder does not exist or no `.ipynb` file is found, that tool is skipped.\n",
    "    \"\"\"\n",
    "\n",
    "    # Predefined mapping of tool identifiers → expected folder names\n",
    "    # Keys are short tool names; values are the actual directory names\n",
    "    tool_folders = {\n",
    "        \"RAG\": \"QA+SUMMARIZATION RAG Modal Final\",   # Retrieval-Augmented Generation\n",
    "        \"PREDICTION\": \"Prediction modal Final\",      # ML prediction models\n",
    "        \"CBR\": \"Case Based Reasoning Final\",         # Case-based reasoning engine\n",
    "    }\n",
    "\n",
    "    # Dictionary to store discovered tool notebooks\n",
    "    discovered_tools: Dict[str, str] = {}\n",
    "\n",
    "    # Iterate over known tool folders\n",
    "    for tool_name, folder_name in tool_folders.items():\n",
    "        folder_path = os.path.join(base_path, folder_name)  # Construct absolute/relative path\n",
    "\n",
    "        # Check if the folder exists in the given base path\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Iterate over files in the folder to find the first notebook (.ipynb)\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".ipynb\"):\n",
    "                    # Register the tool name with the notebook's full path\n",
    "                    discovered_tools[tool_name] = os.path.join(folder_path, file)\n",
    "                    break   # Stop after finding the first notebook\n",
    "\n",
    "    # Return the final mapping of tools discovered\n",
    "    return discovered_tools\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 🔎 Runtime discovery logging\n",
    "# ---------------------------\n",
    "print(\"Discovering tools...\")\n",
    "\n",
    "# Execute the discovery function\n",
    "tools = discover_tools()\n",
    "\n",
    "# Print only the tool names (keys of the dictionary)\n",
    "print(f\"Discovered tools: {list(tools.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d0c9f",
   "metadata": {},
   "source": [
    "---\n",
    "## Tool Helpers (Prediction, CBR, RAG)\n",
    "\n",
    "This section defines the **core helper classes** that power the DLQR system:  \n",
    "\n",
    "1. **PredictionHelper**  \n",
    "   - Loads prediction model assets (Random Forest, XGBoost, PyTorch MLP).  \n",
    "   - Handles feature preprocessing, model inference, and output formatting.  \n",
    "   - Provides methods for single-patient inference across multiple backends.  \n",
    "\n",
    "2. **CBRHelper (Case-Based Reasoning)**  \n",
    "   - Consolidates structured (BCM) and unstructured (Text CBR) retrieval.  \n",
    "   - Supports prototype-based probability explanation and FAISS-based retrieval.  \n",
    "   - Provides hybrid retrieve-and-rank functionality for case similarity.  \n",
    "\n",
    "3. **RAGHelper (Retrieval-Augmented Generation)**  \n",
    "   - Manages document ingestion, index persistence, and query answering.  \n",
    "   - Uses Mistral via Ollama for LLM backbone and BGE embeddings.  \n",
    "   - Provides natural language query interface over stored medical docs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# prediction Helper (MLP + inference)\n",
    "# ================================\n",
    "# Ensure the helper function `_install_create_ratios_shim` exists in scope.\n",
    "# If not defined elsewhere, we define a fallback no-op here.\n",
    "try:\n",
    "    _install_create_ratios_shim  # type: ignore[name-defined]\n",
    "except NameError:  # pragma: no cover\n",
    "    def _install_create_ratios_shim(_results_dir: str | Path) -> None:\n",
    "        return\n",
    "\n",
    "\n",
    "class predictionHelper:\n",
    "    \"\"\"Owns prediction model assets and all prediction-related methods.\"\"\"\n",
    "\n",
    "    # Keys reused in outputs; constants ensure identical string values everywhere.\n",
    "    _KEY_PRED = \"Predicted Diagnosis\"\n",
    "    _KEY_CONF = \"Confidence\"\n",
    "    _KEY_DPROB = \"Class Probabilities\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Placeholder for all loaded model assets (RF, XGB, MLP, preprocessors, etc.)\n",
    "        self.assets = None\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        \"\"\"PyTorch MLP model architecture for patient-level feature prediction.\"\"\"\n",
    "        def __init__(self, input_size: int, num_classes: int):\n",
    "            super().__init__()\n",
    "            # Deep feed-forward NN with BatchNorm + Dropout for generalization\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(input_size, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.4),\n",
    "                nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.3),\n",
    "                nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.2),\n",
    "                nn.Linear(64, num_classes),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Forward pass through the network\n",
    "            return self.net(x)\n",
    "\n",
    "    def _prepare_features(\n",
    "        self,\n",
    "        patient_data: dict,\n",
    "        preprocessor,\n",
    "        feature_mask: np.ndarray,\n",
    "        default_categoricals: dict,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Shared: build safe dataframe → transform → feature select.\n",
    "        - Ensures all expected columns exist.\n",
    "        - Fills missing categoricals with defaults.\n",
    "        - Applies preprocessing pipeline (scaler, encoder, etc.).\n",
    "        - Applies feature mask to select final input vector.\n",
    "        \"\"\"\n",
    "        expected_columns = list(getattr(preprocessor, \"feature_names_in_\", []))\n",
    "        if not expected_columns:\n",
    "            # Fallback: try to recover expected columns from saved pickle\n",
    "            try:\n",
    "                fs_path = Path(preprocessor.__class__.__module__).parent / \"feature_sets.pkl\"\n",
    "                fs = joblib.load(str(fs_path))\n",
    "                expected_columns = list(fs[\"all_original\"])\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    \"Could not determine input feature names for the preprocessor. \"\n",
    "                    \"Ensure preprocessor.feature_names_in_ or feature_sets.pkl is available.\"\n",
    "                ) from e\n",
    "\n",
    "        # Build a safe, ordered row with all expected columns\n",
    "        safe_data = {col: np.nan for col in expected_columns}\n",
    "        safe_data.update(patient_data or {})\n",
    "\n",
    "        # Fill in defaults for missing categoricals\n",
    "        for cat, default_val in (default_categoricals or {}).items():\n",
    "            if pd.isnull(safe_data.get(cat)):\n",
    "                safe_data[cat] = default_val\n",
    "\n",
    "        # Convert into DataFrame, enforce column order\n",
    "        patient_df = pd.DataFrame([safe_data], columns=expected_columns)\n",
    "\n",
    "        # Apply preprocessing (scaling, OHE, etc.)\n",
    "        patient_processed = preprocessor.transform(patient_df)\n",
    "\n",
    "        # Apply mask to keep only selected features\n",
    "        return patient_processed[:, feature_mask]\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_output(label_encoder, probabilities: np.ndarray):\n",
    "        \"\"\"\n",
    "        Convert model output probabilities into human-readable dictionary:\n",
    "        - Predicted label\n",
    "        - Confidence score\n",
    "        - Full class probability distribution\n",
    "        \"\"\"\n",
    "        predicted_label_index = int(np.argmax(probabilities))\n",
    "        predicted_label_name = label_encoder.inverse_transform([predicted_label_index])[0]\n",
    "\n",
    "        # Map all class indices to probability %\n",
    "        class_probabilities = {\n",
    "            label_encoder.inverse_transform([i])[0]: f\"{prob:.2%}\"\n",
    "            for i, prob in enumerate(probabilities)\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            predictionHelper._KEY_PRED: predicted_label_name,\n",
    "            predictionHelper._KEY_CONF: f\"{probabilities[predicted_label_index]:.2%}\",\n",
    "            predictionHelper._KEY_DPROB: class_probabilities,\n",
    "        }\n",
    "\n",
    "    def load_assets(self, pred_base) -> dict:\n",
    "        \"\"\"\n",
    "        Load all prediction model artifacts from disk.\n",
    "        Artifacts include:\n",
    "        - RandomForest (sklearn)\n",
    "        - XGBoost (sklearn API)\n",
    "        - PyTorch MLP (state_dict)\n",
    "        - Preprocessor (feature engineering pipeline)\n",
    "        - Feature mask\n",
    "        - Label encoder\n",
    "        \"\"\"\n",
    "        # Resolve directory paths\n",
    "        base = Path(unquote(str(pred_base))).expanduser()\n",
    "        base = Path(os.path.abspath(str(base)))\n",
    "        results_dir = base if (base / \"preprocessor.pkl\").exists() else (base / \"results\")\n",
    "\n",
    "        # Ensure results directory exists\n",
    "        if not results_dir.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"[PREDICTION] results_dir not found: {results_dir}\\n\"\n",
    "                \"Expected either <pred_base>/preprocessor.pkl or <pred_base>/results/* artifacts.\"\n",
    "            )\n",
    "\n",
    "        # Shim for backwards compatibility of ratio features\n",
    "        _install_create_ratios_shim(results_dir)\n",
    "\n",
    "        # Register module import alias for pickled preprocessors\n",
    "        sys.modules.setdefault(\n",
    "            \"preprocessing_utils\",\n",
    "            sys.modules.get(\"__main__\", __import__(\"__main__\")),\n",
    "        )\n",
    "\n",
    "        # Required artifacts and their expected paths\n",
    "        needed = {\n",
    "            \"rf\": results_dir / \"random_forest_model.pkl\",\n",
    "            \"xgb\": results_dir / \"xgboost_model.pkl\",\n",
    "            \"mlp\": results_dir / \"mlp_model.pth\",\n",
    "            \"mask\": results_dir / \"selected_features_mask.pkl\",\n",
    "            \"le\": results_dir / \"label_encoder.pkl\",\n",
    "            \"pp\": results_dir / \"preprocessor.pkl\",\n",
    "        }\n",
    "\n",
    "        # Sanity check: all files must exist\n",
    "        missing = [k for k, p in needed.items() if not p.exists()]\n",
    "        if missing:\n",
    "            hint = \", \".join(missing)\n",
    "            raise FileNotFoundError(\n",
    "                f\"[PREDICTION] Missing required artifacts: {hint}\\n\"\n",
    "                f\"Searched in: {results_dir}\"\n",
    "            )\n",
    "\n",
    "        # Load sklearn models\n",
    "        rf_model = joblib.load(str(needed[\"rf\"]))\n",
    "        xgb_model = joblib.load(str(needed[\"xgb\"]))\n",
    "\n",
    "        # Load mask, encoder, preprocessor\n",
    "        feature_mask = np.array(joblib.load(str(needed[\"mask\"])))\n",
    "        label_encoder = joblib.load(str(needed[\"le\"]))\n",
    "        preprocessor = joblib.load(str(needed[\"pp\"]))\n",
    "\n",
    "        # Compute input size for MLP\n",
    "        num_classes = len(getattr(label_encoder, \"classes_\", []))\n",
    "        input_size = int(np.sum(feature_mask)) if feature_mask.dtype == bool else int(len(feature_mask))\n",
    "\n",
    "        # Initialize PyTorch MLP with saved weights\n",
    "        mlp_model_instance = self.MLP(input_size, num_classes)\n",
    "        state_dict = torch.load(str(needed[\"mlp\"]), map_location=\"cpu\")\n",
    "        mlp_model_instance.load_state_dict(state_dict)\n",
    "        mlp_model_instance.eval()\n",
    "\n",
    "        # Best-effort: attach feature_names_in_ if missing\n",
    "        if not hasattr(preprocessor, \"feature_names_in_\"):\n",
    "            fs_path = results_dir / \"feature_sets.pkl\"\n",
    "            if fs_path.exists():\n",
    "                try:\n",
    "                    fs = joblib.load(str(fs_path))\n",
    "                    if isinstance(fs, dict) and \"all_original\" in fs:\n",
    "                        preprocessor.feature_names_in_ = np.array(list(fs[\"all_original\"]), dtype=object)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Store everything inside `self.assets` for later access\n",
    "        self.assets = {\n",
    "            \"models\": {\n",
    "                \"Random Forest\": rf_model,\n",
    "                \"XGBoost\": xgb_model,\n",
    "                \"MLP\": mlp_model_instance,\n",
    "            },\n",
    "            \"preprocessor\": preprocessor,\n",
    "            \"feature_mask\": feature_mask,\n",
    "            \"label_encoder\": label_encoder,\n",
    "            # Default categorical fill-ins for missing patient info\n",
    "            \"default_categoricals\": {\n",
    "                \"Gender\": \"Male\",\n",
    "                \"Smoking_Status\": \"Current\",\n",
    "                \"Cancer_Stage\": \"Stage II\",\n",
    "            },\n",
    "        }\n",
    "        return self.assets\n",
    "\n",
    "    def predict_single_patient(\n",
    "        self,\n",
    "        patient_data: dict,\n",
    "        model,\n",
    "        preprocessor,\n",
    "        feature_mask: np.ndarray,\n",
    "        label_encoder,\n",
    "        default_categoricals: dict,\n",
    "    ):\n",
    "        \"\"\"Run inference with sklearn models that support `predict_proba`.\"\"\"\n",
    "        patient_selected = self._prepare_features(\n",
    "            patient_data, preprocessor, feature_mask, default_categoricals\n",
    "        )\n",
    "        probabilities = model.predict_proba(patient_selected)[0]\n",
    "        return self._format_output(label_encoder, probabilities)\n",
    "\n",
    "    def predict_single_patient_mlp(\n",
    "        self,\n",
    "        patient_data: dict,\n",
    "        model: nn.Module,\n",
    "        preprocessor,\n",
    "        feature_mask: np.ndarray,\n",
    "        label_encoder,\n",
    "        default_categoricals: dict,\n",
    "    ):\n",
    "        \"\"\"Run inference with PyTorch MLP model (softmax for probabilities).\"\"\"\n",
    "        patient_selected = self._prepare_features(\n",
    "            patient_data, preprocessor, feature_mask, default_categoricals\n",
    "        )\n",
    "        # Ensure proper tensor shape and type\n",
    "        X_tensor = torch.as_tensor(np.asarray(patient_selected), dtype=torch.float32)\n",
    "        if X_tensor.ndim == 1:  # just in case a single feature vector is passed\n",
    "            X_tensor = X_tensor[None, :]\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_tensor)\n",
    "            probabilities = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        return self._format_output(label_encoder, probabilities)\n",
    "\n",
    "    def smart_predict_single_patient(\n",
    "        self,\n",
    "        patient_data: dict,\n",
    "        model,\n",
    "        preprocessor,\n",
    "        feature_mask: np.ndarray,\n",
    "        label_encoder,\n",
    "        default_categoricals: dict,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Smart wrapper:\n",
    "        - If model has `predict_proba` (sklearn) → use that.\n",
    "        - Else assume PyTorch MLP and use softmax inference.\n",
    "        \"\"\"\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            return self.predict_single_patient(\n",
    "                patient_data, model, preprocessor, feature_mask, label_encoder, default_categoricals\n",
    "            )\n",
    "        else:\n",
    "            return self.predict_single_patient_mlp(\n",
    "                patient_data, model, preprocessor, feature_mask, label_encoder, default_categoricals\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2238035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# CBR Helper\n",
    "# ================================\n",
    "class CBRHelper:\n",
    "    \"\"\"\n",
    "    Consolidated Case-Based Reasoning (CBR) helper.\n",
    "    - Preserves the public API of prior BCMModule and TextCBRModule.\n",
    "    - Handles structured similarity (Bayesian Case Matching) + text similarity retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compatibility shims to mimic older module API names\n",
    "    class _BCMCompat:\n",
    "        \"\"\"Shim wrapper so older code calling `bcm.predict_proba()` still works.\"\"\"\n",
    "        def __init__(self, parent):\n",
    "            self._p = parent\n",
    "        def predict_proba(self, query_vector):\n",
    "            return self._p.predict_proba(query_vector)\n",
    "        def explain(self, query_vector, prototype_id, scaler, columns):\n",
    "            return self._p.explain(query_vector, prototype_id, scaler, columns)\n",
    "\n",
    "    class _TextCompat:\n",
    "        \"\"\"Shim wrapper for text-based retrieval API compatibility.\"\"\"\n",
    "        def __init__(self, parent):\n",
    "            self._p = parent\n",
    "        def retrieve(self, query_text, k=20):\n",
    "            return self._p.retrieve(query_text, k=k)\n",
    "\n",
    "    def __init__(self):\n",
    "        # Core structured case-based reasoning assets\n",
    "        self.kmeans = None            # clustering model for structured prototypes\n",
    "        self.prototypes_ = None       # prototype definitions (mean/cov/prior per class)\n",
    "        self.n_prototypes = None      # number of prototypes\n",
    "\n",
    "        # Text retrieval assets\n",
    "        self.text_model = None        # sentence embedding model\n",
    "        self.index = None             # FAISS index\n",
    "        self.case_ids = None          # mapping FAISS ids → case ids\n",
    "\n",
    "        # Data scaling + case library\n",
    "        self.scaler = None\n",
    "        self.library = None\n",
    "        self.structured_cols = None\n",
    "        self.library_scaled = None\n",
    "\n",
    "        # Backwards-compatible handles for `.bcm` and `.text`\n",
    "        self.bcm = CBRHelper._BCMCompat(self)\n",
    "        self.text = CBRHelper._TextCompat(self)\n",
    "\n",
    "    def load_assets(self, cbr_base):\n",
    "        \"\"\"\n",
    "        Load all structured + text retrieval artifacts from disk.\n",
    "        - BCM prototype models\n",
    "        - Text embedding model + FAISS index\n",
    "        - Scaler + case library\n",
    "        \"\"\"\n",
    "        print(\"  - Initializing new CBR pipeline from artifacts...\")\n",
    "        artifact_Path = os.path.join(cbr_base, \"results\")\n",
    "\n",
    "        # --- Load structured BCM module\n",
    "        bcm_Path = os.path.join(artifact_Path, \"bcm_module\")\n",
    "        self.kmeans = joblib.load(os.path.join(bcm_Path, \"kmeans.joblib\"))\n",
    "        self.prototypes_ = joblib.load(os.path.join(bcm_Path, \"prototypes.joblib\"))\n",
    "        self.n_prototypes = len(self.prototypes_)\n",
    "        print(f\"  - BCMModule loaded from {bcm_Path}\")\n",
    "\n",
    "        # --- Load text CBR module\n",
    "        text_cbr_Path = os.path.join(artifact_Path, \"text_cbr_module\")\n",
    "        with open(os.path.join(text_cbr_Path, \"metadata.json\"), 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        self.text_model = SentenceTransformer(metadata['model_name'])\n",
    "        self.index = faiss.read_index(os.path.join(text_cbr_Path, \"faiss.index\"))\n",
    "        self.case_ids = metadata['case_ids']\n",
    "        print(f\"  - TextCBRModule loaded from {text_cbr_Path}\")\n",
    "\n",
    "        # --- Load scaler + case library\n",
    "        self.scaler = joblib.load(os.path.join(artifact_Path, \"scaler.joblib\"))\n",
    "        train_df = pd.read_csv(os.path.join(artifact_Path, \"case_library_train.csv\"))\n",
    "\n",
    "        with open(os.path.join(artifact_Path, \"numeric_structured_cols.json\"), \"r\") as f:\n",
    "            numeric_cols = json.load(f)\n",
    "\n",
    "        self.library = train_df.set_index('case_id')\n",
    "        self.structured_cols = numeric_cols\n",
    "\n",
    "        # Create scaled version of case library for structured matching\n",
    "        self.library_scaled = self.library.copy()\n",
    "        self.library_scaled[self.structured_cols] = self.scaler.transform(self.library[self.structured_cols])\n",
    "\n",
    "        print(\"  - CBR unified engine created successfully.\")\n",
    "        return {'engine': self}\n",
    "\n",
    "    def predict_proba(self, query_vector):\n",
    "        \"\"\"\n",
    "        Compute posterior probabilities over prototypes for a structured query vector.\n",
    "        Uses Gaussian likelihoods + stored priors.\n",
    "        \"\"\"\n",
    "        likelihoods = [multivariate_normal.pdf(query_vector, mean=p['mean'], cov=p['cov']) for p in self.prototypes_]\n",
    "        priors = np.array([p['prior'] for p in self.prototypes_])\n",
    "        evidence = np.sum(likelihoods * priors)\n",
    "        return (likelihoods * priors) / evidence if evidence > 0 else np.zeros(self.n_prototypes)\n",
    "\n",
    "    def explain(self, query_vector, prototype_id, scaler, columns):\n",
    "        \"\"\"\n",
    "        Explain similarity by comparing query features vs prototype mean.\n",
    "        Returns a string summary with key feature-level differences.\n",
    "        \"\"\"\n",
    "        proto = self.prototypes_[prototype_id]\n",
    "\n",
    "        # Inverse transform to original scale\n",
    "        query_unscaled = scaler.inverse_transform(query_vector.reshape(1, -1))[0]\n",
    "        proto_mean_unscaled = scaler.inverse_transform(proto['mean'].reshape(1, -1))[0]\n",
    "\n",
    "        explanation = f\"Matches Prototype '{proto['name']}'. Key feature comparison:\\n\"\n",
    "        for i, col in enumerate(columns):\n",
    "            explanation += f\"  - {col}: Query={query_unscaled[i]:.2f}, Proto Avg={proto_mean_unscaled[i]:.2f}\\n\"\n",
    "        return explanation\n",
    "\n",
    "    def retrieve(self, query_text, k=20):\n",
    "        \"\"\"\n",
    "        Text-only retrieval:\n",
    "        - Embed query with SentenceTransformer\n",
    "        - Search FAISS index\n",
    "        - Return top-k case IDs + distances\n",
    "        \"\"\"\n",
    "        query_embedding = self.text_model.encode([query_text], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        return [self.case_ids[i] for i in indices[0]], distances[0]\n",
    "\n",
    "    def retrieve_and_rank(self, query_case, k_text=20, k_final=3, w_text=0.4, w_bcm=0.6):\n",
    "        \"\"\"\n",
    "        Combined retrieval:\n",
    "        - Get candidates via text similarity (FAISS)\n",
    "        - Score them with BCM structured similarity\n",
    "        - Compute weighted final score = w_text*text + w_bcm*structured\n",
    "        - Return top k_final ranked cases\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Step 1: text retrieval\n",
    "        candidate_ids, text_scores = self.retrieve(query_case['clinical_narrative'], k=k_text)\n",
    "        if not candidate_ids:\n",
    "            return pd.DataFrame(), 0\n",
    "\n",
    "        # Step 2: load structured data for candidates\n",
    "        candidates_df = self.library.loc[candidate_ids].copy()\n",
    "        candidates_df['text_similarity'] = text_scores\n",
    "\n",
    "        # Step 3: structured scoring (BCM likelihoods)\n",
    "        query_struct_df = pd.DataFrame([query_case])[self.structured_cols]\n",
    "        query_struct_scaled = self.scaler.transform(query_struct_df)[0]\n",
    "\n",
    "        bcm_scores = []\n",
    "        for _, cand_row in candidates_df.iterrows():\n",
    "            cand_struct_scaled = self.library_scaled.loc[cand_row.name][self.structured_cols].values\n",
    "            posteriors = self.predict_proba(cand_struct_scaled)\n",
    "            bcm_scores.append(np.max(posteriors))\n",
    "        candidates_df['bcm_prob'] = bcm_scores\n",
    "\n",
    "        # Step 4: weighted combination\n",
    "        candidates_df['final_score'] = (w_text * candidates_df['text_similarity']) + (w_bcm * candidates_df['bcm_prob'])\n",
    "\n",
    "        # Sort & return top results\n",
    "        ranked_results = candidates_df.sort_values(by='final_score', ascending=False)\n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        return ranked_results.head(k_final), latency\n",
    "\n",
    "    def run(self, case_data):\n",
    "        \"\"\"\n",
    "        End-to-end run method for case-based reasoning:\n",
    "        - Fill missing structured fields with imputed values\n",
    "        - Perform retrieval + ranking\n",
    "        - Return suggested diagnosis + top-3 similar cases\n",
    "        \"\"\"\n",
    "        query_series_data: Dict[str, Any] = {}\n",
    "        all_structured_cols = self.structured_cols\n",
    "\n",
    "        # Step 1: collect structured fields, fill with NaN if missing\n",
    "        for col in all_structured_cols:\n",
    "            query_series_data[col] = case_data.get(col, np.nan)\n",
    "\n",
    "        # Step 2: clinical narrative fallback\n",
    "        query_series_data['clinical_narrative'] = case_data.get('clinical_narrative', \"No clinical narrative provided.\")\n",
    "\n",
    "        # Step 3: impute missing structured features (midpoint of scaler range)\n",
    "        for i, col in enumerate(all_structured_cols):\n",
    "            if pd.isna(query_series_data[col]):\n",
    "                impute_value = self.scaler.data_min_[i] + (self.scaler.data_range_[i] * 0.5)\n",
    "                query_series_data[col] = impute_value\n",
    "\n",
    "        # Step 4: retrieval + ranking\n",
    "        query_series = pd.Series(query_series_data)\n",
    "        retrieved, _ = self.retrieve_and_rank(query_series)\n",
    "\n",
    "        if retrieved.empty:\n",
    "            return {\"message\": \"No similar cases could be retrieved.\"}\n",
    "\n",
    "        # Suggested diagnosis = top retrieved case’s diagnosis\n",
    "        suggestion = retrieved.iloc[0]['diagnosis']\n",
    "\n",
    "        response = {\n",
    "            \"Suggested Diagnosis\": suggestion,\n",
    "            \"Top 3 Similar Cases\": retrieved[['diagnosis', 'age', 'sex', 'tnm_stage', 'final_score']].to_dict('records'),\n",
    "        }\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2d2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool helpers defined.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# RAG Helper\n",
    "# ================================\n",
    "class RAGHealper:\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation (RAG) helper.\n",
    "    - Loads documents or pre-built FAISS index\n",
    "    - Executes natural language queries using LLM + embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.query_engine = None\n",
    "\n",
    "    def load_assets(self, rag_base):\n",
    "        \"\"\"\n",
    "        Load or build RAG index:\n",
    "        - If no index exists, build from raw docs\n",
    "        - Else, load existing storage\n",
    "        \"\"\"\n",
    "        docs_Path = os.path.join(rag_base, \"docs\")\n",
    "        storage_Path = os.path.join(rag_base, \"storage\")\n",
    "\n",
    "        # Configure LLM + embedding model for RAG\n",
    "        Settings.llm = Ollama(model=\"mistral\", request_timeout=120.0)\n",
    "        Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "        # If index storage missing → build from docs\n",
    "        if not os.path.exists(storage_Path) or not os.listdir(storage_Path):\n",
    "            print(f\"    - No existing index found at '{storage_Path}'. Building from documents...\")\n",
    "            if not os.path.exists(docs_Path) or not os.listdir(docs_Path):\n",
    "                raise FileNotFoundError(f\"No documents found in '{docs_Path}' to build the RAG index.\")\n",
    "\n",
    "            # Load docs + build index\n",
    "            documents = SimpleDirectoryReader(docs_Path).load_data()\n",
    "            index = VectorStoreIndex.from_documents(documents)\n",
    "            index.storage_context.persist(persist_dir=storage_Path)\n",
    "            print(f\"    - Index built and persisted to '{storage_Path}'.\")\n",
    "        else:\n",
    "            # Load existing index from disk\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=storage_Path)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "\n",
    "        # Attach query engine\n",
    "        self.query_engine = index.as_query_engine(similarity_top_k=4)\n",
    "        return {'query_engine': self.query_engine}\n",
    "\n",
    "    def run(self, query_text):\n",
    "        \"\"\"\n",
    "        Execute a query against the RAG engine.\n",
    "        - Handles dict input or plain string.\n",
    "        - Returns LLM-generated response.\n",
    "        \"\"\"\n",
    "        if not self.query_engine:\n",
    "            return \"Error: RAG query engine is not available.\"\n",
    "\n",
    "        # Handle dict inputs (e.g., from router)\n",
    "        if isinstance(query_text, dict):\n",
    "            vals = list(query_text.values())\n",
    "            if vals and isinstance(vals[0], str):\n",
    "                query_text = vals[0]\n",
    "            else:\n",
    "                query_text = str(query_text)\n",
    "\n",
    "        response = self.query_engine.query(query_text)\n",
    "        return str(response)\n",
    "\n",
    "\n",
    "print(\"Tool helpers defined.\")           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f188ff",
   "metadata": {},
   "source": [
    "---\n",
    "## ToolExecutor & Notebook-Friendly Router API\n",
    "\n",
    "This module wires together:\n",
    "- The **ToolExecutor** that loads and runs tools (Prediction, CBR, RAG/QA/Summarize).\n",
    "- Convenience **notebook APIs**: `nb_route`, `nb_execute`, `nb_feedback`, `nb_learn_supervised`, `nb_policy_metrics`.\n",
    "- **Hedging & timeouts** for multi-target execution with circuit breakers.\n",
    "- A simple **health** endpoint and a few utilities for mapping actions → tool params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057dd53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all required models and preprocessors...\n",
      "  - prediction models (RF, XGB, MLP) loaded successfully.\n",
      "  - Initializing new CBR pipeline from artifacts...\n",
      "  - BCMModule loaded from ./Case Based Reasoning Final/results/bcm_module\n",
      "  - TextCBRModule loaded from ./Case Based Reasoning Final/results/text_cbr_module\n",
      "  - CBR unified engine created successfully.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./QA+SUMMARIZATION RAG Modal Final/storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./QA+SUMMARIZATION RAG Modal Final/storage/index_store.json.\n",
      "  - RAG query engine created successfully.\n",
      "✅ All models loaded.\n",
      "\n",
      "=== 🚀 Router Initialization (No-Meta, LLM-integrated + targets) ===\n",
      "Router loaded. Use: health(), nb_route(...), nb_execute(...), nb_feedback(...), nb_learn_supervised(...), nb_policy_metrics().\n"
     ]
    }
   ],
   "source": [
    "class ToolExecutor:\n",
    "    def __init__(self, tool_Paths: Dict[str, str]):\n",
    "        # Mapping of tool logical names → discovered notebook paths\n",
    "        self.tool_Paths = tool_Paths\n",
    "        # Loaded model/asset handles per tool (e.g., {'PREDICTION': {...}, 'CBR': {...}, 'RAG': {...}})\n",
    "        self.models: Dict[str, Any] = {}\n",
    "        # Helper classes manage their own asset loading + inference logic\n",
    "        self.pred_helper = predictionHelper()\n",
    "        self.cbr_helper = CBRHelper()\n",
    "        self.rag_helper = RAGHealper()\n",
    "        # Eager-load all tools on construction so runtime calls are fast\n",
    "        self._load_all_models()\n",
    "\n",
    "    def _load_all_models(self):\n",
    "        print(\"Loading all required models and preprocessors...\")\n",
    "        # prediction\n",
    "        try:\n",
    "            # Base directory for prediction artifacts; supports both <base>/results/* and <base>/*\n",
    "            pred_base = os.path.dirname(self.tool_Paths.get(\"PREDICTION\", \".\"))\n",
    "            self.models['PREDICTION'] = self.pred_helper.load_assets(pred_base)\n",
    "            print(\"  - prediction models (RF, XGB, MLP) loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            # Non-fatal: allow other tools to continue loading\n",
    "            print(f\"  - WARNING: Could not load prediction model assets. Error: {e}\")\n",
    "\n",
    "        # CBR\n",
    "        try:\n",
    "            cbr_base = os.path.dirname(self.tool_Paths.get(\"CBR\", \".\"))\n",
    "            self.models['CBR'] = self.cbr_helper.load_assets(cbr_base)\n",
    "        except Exception as e:\n",
    "            print(f\"  - WARNING: Could not load CBR model assets. Error: {e}\")\n",
    "\n",
    "        # RAG\n",
    "        try:\n",
    "            rag_base = os.path.dirname(self.tool_Paths.get(\"RAG\", \".\"))\n",
    "            # RAG serves QA + SUMMARIZE as the same backend/query engine\n",
    "            self.models['RAG'] = self.rag_helper.load_assets(rag_base)\n",
    "            self.models['QA'] = self.models['RAG']\n",
    "            self.models['SUMMARIZE'] = self.models['RAG']\n",
    "            print(\"  - RAG query engine created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - WARNING: Could not initialize RAG pipeline for QA+SUMMARIZATION model. Error: {e}\")\n",
    "            # Provide a null engine so calls fail gracefully\n",
    "            self.models['RAG'] = {'query_engine': None}\n",
    "\n",
    "        print(\"✅ All models loaded.\")\n",
    "\n",
    "    def run_tool(self, tool_name: str, params: Any):\n",
    "        # Map human-friendly names to underlying tool key (QA/SUMMARIZE → RAG)\n",
    "        alias = {'QA': 'RAG', 'SUMMARIZE': 'RAG'}\n",
    "        real = alias.get(tool_name, tool_name)\n",
    "\n",
    "        # Validate tool availability\n",
    "        if real not in self.models:\n",
    "            return f\"Error: Tool '{tool_name}' not found or failed to load.\"\n",
    "\n",
    "        try:\n",
    "            # Dispatch to the appropriate runner\n",
    "            if tool_name == 'PREDICTION':\n",
    "                return self._run_prediction(params)\n",
    "            elif tool_name == 'CBR':\n",
    "                return self._run_cbr(params)\n",
    "            elif tool_name == 'QA':\n",
    "                return self._run_qa(params)\n",
    "            elif tool_name == 'SUMMARIZE':\n",
    "                return self._run_summarize(params)\n",
    "            else:\n",
    "                return f\"Error: Unknown tool '{tool_name}'\"\n",
    "        except Exception as e:\n",
    "            # Catch all to avoid crashing the caller\n",
    "            return f\"An error occurred while running {tool_name}: {e}\"\n",
    "\n",
    "    def _run_prediction(self, patient_data):\n",
    "        # Basic validation of input type for prediction path\n",
    "        if not isinstance(patient_data, dict):\n",
    "            return \"Error: prediction tool requires structured data (e.g., {'Age': 55}).\"\n",
    "        assets = self.models['PREDICTION']\n",
    "        # Choose a default model (XGBoost) — can be changed to RF/MLP if desired\n",
    "        chosen_model = assets['models'].get(\"XGBoost\")\n",
    "        # Delegate to helper which formats output with label + confidences\n",
    "        return self.pred_helper.smart_predict_single_patient(\n",
    "            patient_data, chosen_model, assets['preprocessor'],\n",
    "            assets['feature_mask'], assets['label_encoder'], assets['default_categoricals'],\n",
    "        )\n",
    "\n",
    "    def _run_cbr(self, case_data):\n",
    "        # CBR helper manages both text + structured retrieval & ranking\n",
    "        return self.cbr_helper.run(case_data)\n",
    "\n",
    "    def _run_qa(self, query_text):\n",
    "        # RAG helper handles retrieval-augmented QA\n",
    "        return self.rag_helper.run(query_text)\n",
    "\n",
    "    def _run_summarize(self, query_text):\n",
    "        # Same RAG backend for summarization\n",
    "        return self.rag_helper.run(query_text)\n",
    "\n",
    "\n",
    "# Instantiate a global Router and ToolExecutor for notebook-style usage\n",
    "router = Router()\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "\n",
    "def health() -> Dict[str, Any]:\n",
    "    # Simple health/config echo useful for quick diagnostics\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"embed_model\": EMBED_MODEL,\n",
    "        \"learned_buckets\": LEARNED_BUCKETS,\n",
    "        \"use_llm_intent\": USE_LLM_INTENT,\n",
    "        \"llm_backbone\": LLM_BACKBONE,\n",
    "        \"llm_weight\": LLM_WEIGHT,\n",
    "        \"use_mm_executor\": USE_MM_EXECUTOR,\n",
    "        \"use_hedging\": USE_HEDGING,\n",
    "    }\n",
    "\n",
    "\n",
    "# Thread pool for concurrent/hedged target execution\n",
    "_THREAD_POOL = _fut.ThreadPoolExecutor(max_workers=8)\n",
    "\n",
    "# Action→tool name mapping used by the router to find the right executor path\n",
    "_ACTION_TO_TOOL_NAME = {\n",
    "    Action.PREDICTION.value: \"PREDICTION\",\n",
    "    Action.CBR.value: \"CBR\",\n",
    "    Action.QA.value: \"QA\",\n",
    "    Action.SUMMARIZE.value: \"SUMMARIZE\",\n",
    "}\n",
    "\n",
    "def _map_action_to_tool_name(action: str) -> str:\n",
    "    # Fallback behavior capitalizes unknown action names\n",
    "    return _ACTION_TO_TOOL_NAME.get(action, action.capitalize())\n",
    "\n",
    "def _prepare_tool_params(action: str, query: str, ctx: Optional[str]) -> Any:\n",
    "    # Convert router (query, context) into the expected tool input payload\n",
    "    if action == Action.QA.value:\n",
    "        return query\n",
    "    if action == Action.SUMMARIZE.value:\n",
    "        return query\n",
    "    if action == Action.CBR.value:\n",
    "        # Try to parse JSON as structured case; otherwise, treat query as narrative\n",
    "        try:\n",
    "            return json.loads(query)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"clinical_narrative\": query}\n",
    "    if action == Action.PREDICTION.value:\n",
    "        # Prediction expects a JSON object with structured fields\n",
    "        try:\n",
    "            return json.loads(query)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"error\": \"Invalid input: prediction tool requires a JSON object string.\"}\n",
    "    return query\n",
    "\n",
    "def _call_with_timeout(fn: Callable[[], Dict[str, Any]], timeout_ms: int) -> Dict[str, Any]:\n",
    "    # Run a callable in the thread pool with a timeout; cancel if necessary\n",
    "    fut = _THREAD_POOL.submit(fn)\n",
    "    try:\n",
    "        return fut.result(timeout=timeout_ms / 1000.0)\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            fut.cancel()\n",
    "        except Exception:\n",
    "            pass\n",
    "        raise e\n",
    "\n",
    "def _run_single_target(action: str, query: str, ctx: Optional[str], t: TargetSpec) -> Tuple[str, Dict[str, Any], Optional[str]]:\n",
    "    # Execute one target with circuit breaker accounting\n",
    "    try:\n",
    "        tool_name = _map_action_to_tool_name(action)\n",
    "        params = _prepare_tool_params(action, query, ctx)\n",
    "        out = tool_executor.run_tool(tool_name, params)\n",
    "        t.breaker.on_success()\n",
    "        return t.id, out, None\n",
    "    except Exception as e:\n",
    "        t.breaker.on_failure()\n",
    "        return t.id, {\"error\": \"target_failed\", \"detail\": repr(e)}, repr(e)\n",
    "\n",
    "def _hedged_first_finish(action: str, query: str, ctx: Optional[str], targets: List[TargetSpec]) -> Tuple[str, Dict[str, Any], Optional[str]]:\n",
    "    # Fire multiple targets in parallel and return the first successful result\n",
    "    if not targets:\n",
    "        return \"none\", {\"error\": \"no_targets\"}, \"no_targets\"\n",
    "    futures = [_THREAD_POOL.submit(_call_with_timeout, lambda t=t: _run_single_target(action, query, ctx, t), t.timeout_ms) for t in targets]\n",
    "    done, pending = _fut.wait(futures, return_when=_fut.FIRST_COMPLETED)\n",
    "    result_id, result_payload, err = None, {\"error\": \"execution_failed\"}, None\n",
    "    for d in done:\n",
    "        try:\n",
    "            tid, payload, e = d.result()\n",
    "            result_id, result_payload, err = tid, payload, e\n",
    "            # Consider success if payload is not an error dict\n",
    "            if not (isinstance(payload, dict) and \"error\" in payload):\n",
    "                for p in pending:\n",
    "                    p.cancel()\n",
    "                break\n",
    "        except Exception as e:\n",
    "            err = repr(e)\n",
    "            continue\n",
    "    # Cancel any remaining slow futures\n",
    "    for p in pending:\n",
    "        try:\n",
    "            p.cancel()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return result_id or targets[0].id, result_payload, err\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def nb_route(req: RouteRequest) -> RouteDecision:\n",
    "    # Lightweight routing-only endpoint (no tool execution), useful for inspection\n",
    "    if not req.query or not req.query.strip():\n",
    "        raise ValueError(\"query is required\")\n",
    "    chosen, probs, uncert, util, trace, stage, target_id = router.route(req.query, req.context_text, req.explicit_action)\n",
    "    return RouteDecision(chosen=Action(chosen), probs=probs, uncertainty=uncert, utility=util, stage=stage, trace_id=trace, target_id=target_id)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def nb_execute(req: RouteRequest) -> Dict[str, Any]:\n",
    "    # Full execution path:\n",
    "    # 1) Route to action\n",
    "    # 2) Select targets (with hedging if uncertain)\n",
    "    # 3) Run the tool(s) with timeouts\n",
    "    # 4) Log and return decision + output\n",
    "    if not req.query or not req.query.strip():\n",
    "        raise ValueError(\"query is required\")\n",
    "\n",
    "    (chosen, probs, uncert, util, trace, stage, target_id) = router.route(req.query, req.context_text, req.explicit_action)\n",
    "\n",
    "    # Determine number of targets to hedge across\n",
    "    k = max(1, (HEDGE_MAX_TARGETS if USE_HEDGING and (uncert or 0.0) >= HEDGE_UNCERTAINTY_T else 1))\n",
    "\n",
    "    # Recompute ML policy probs (for target selection ranking) in the current context\n",
    "    x_np = router._features(req.query, req.context_text)\n",
    "    x = torch.from_numpy(x_np).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = router.calib(router.policy(router.projector(x)))\n",
    "        policy_probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    # Optionally blend LLM intent probabilities\n",
    "    if router.llm_intent is not None:\n",
    "        llm_probs = router.llm_intent.predict_proba([req.query], [req.context_text])[0]\n",
    "        probs_vec = (LLM_WEIGHT * llm_probs) + ((1.0 - LLM_WEIGHT) * policy_probs)\n",
    "    else:\n",
    "        probs_vec = policy_probs\n",
    "\n",
    "    # Apply mix calibration on the probability vector\n",
    "    probs_vec = router.mix_calib(torch.from_numpy(probs_vec).unsqueeze(0).float()).squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Choose targets (sorted by latency/cost); maybe >1 if hedging enabled\n",
    "    targets = router.select_targets(chosen, probs_vec, k=k)\n",
    "\n",
    "    # Execute targets with hedging/timeout handling\n",
    "    t0 = time.time(); err = None; attempts = 0; provider = None; model = None\n",
    "\n",
    "    def single_call(t: TargetSpec):\n",
    "        nonlocal provider, model, attempts\n",
    "        attempts += 1\n",
    "        tid, out, e = _run_single_target(chosen, req.query, req.context_text, t)\n",
    "        if isinstance(out, dict) and \"error\" not in out:\n",
    "            provider = t.provider; model = t.model\n",
    "        return tid, out, e\n",
    "\n",
    "    output = None\n",
    "    chosen_target_id = target_id or (targets[0].id if targets else None)\n",
    "    try:\n",
    "        if USE_HEDGING and len(targets) > 1:\n",
    "            # Race the first few targets; take the first successful return\n",
    "            chosen_target_id, output, err = _hedged_first_finish(chosen, req.query, req.context_text, targets[:HEDGE_MAX_TARGETS])\n",
    "        elif targets:\n",
    "            # Single target execution\n",
    "            tid, out, e = single_call(targets[0])\n",
    "            chosen_target_id, output, err = tid, out, e\n",
    "        else:\n",
    "            # No targets configured: run the tool directly via ToolExecutor\n",
    "            tool_name = _map_action_to_tool_name(chosen)\n",
    "            params = _prepare_tool_params(chosen, req.query, req.context_text)\n",
    "            output = tool_executor.run_tool(tool_name, params)\n",
    "    except Exception as e:\n",
    "        # Fallback path: if target execution fails, degrade to QA\n",
    "        err = repr(e)\n",
    "        chosen = Action.QA.value\n",
    "        stage = (stage or \"exec\") + \"+fallback\"\n",
    "        tool_name = _map_action_to_tool_name(chosen)\n",
    "        params = _prepare_tool_params(chosen, req.query, req.context_text)\n",
    "        output = tool_executor.run_tool(tool_name, params)\n",
    "\n",
    "    # Build and log the execution record\n",
    "    elapsed_ms = int((time.time() - t0) * 1000)\n",
    "    exec_log = {\n",
    "        \"trace_id\": trace,\n",
    "        \"chosen\": chosen,\n",
    "        \"stage\": stage,\n",
    "        \"probs\": probs,\n",
    "        \"uncertainty\": uncert,\n",
    "        \"latency_ms\": elapsed_ms,\n",
    "        \"provider\": provider,\n",
    "        \"model\": model,\n",
    "        \"attempts\": attempts or 1,\n",
    "        \"error\": err,\n",
    "        \"target_id\": chosen_target_id,\n",
    "        \"breaker\": (TARGETS.all_targets(chosen)[0].breaker.snapshot() if TARGETS.all_targets(chosen) else None),\n",
    "    }\n",
    "    log_event(\"execute\", exec_log)\n",
    "\n",
    "    # Build decision with explanation for notebook consumption\n",
    "    decision = RouteDecision(\n",
    "        chosen=Action(chosen), probs=probs, uncertainty=uncert or 0.0, utility=util or {},\n",
    "        stage=stage or \"unknown\", trace_id=trace, target_id=chosen_target_id,\n",
    "        why=router.explain_last(trace, exec_log),\n",
    "    )\n",
    "    return {\"decision\": decision.model_dump(), \"output\": output}\n",
    "\n",
    "\n",
    "def nb_feedback(fb: Feedback) -> Dict[str, Any]:\n",
    "    # Convert any available feedback signals into a scalar reward\n",
    "    sig: List[float] = []\n",
    "    if fb.thumbs_up is not None:\n",
    "        sig.append(1.0 if fb.thumbs_up else 0.0)\n",
    "    if fb.task_success is not None:\n",
    "        sig.append(1.0 if fb.task_success else 0.0)\n",
    "    if fb.quality is not None:\n",
    "        sig.append(float(fb.quality))\n",
    "    if fb.faithfulness is not None:\n",
    "        sig.append(float(fb.faithfulness))\n",
    "    reward = float(np.mean(sig)) if sig else 0.0\n",
    "\n",
    "    # Log and update the contextual bandit\n",
    "    log_event(\"feedback\", {**fb.model_dump(), \"reward\": reward})\n",
    "    router.bandit_update(fb.trace_id, reward)\n",
    "    return {\"status\": \"ok\", \"reward\": reward}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def nb_learn_supervised(batch: SupervisedBatch) -> Dict[str, Any]:\n",
    "    # Trigger supervised fine-tuning of the policy network\n",
    "    info = router.train_supervised(batch)\n",
    "    return {\"status\": \"ok\", **info}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def nb_policy_metrics() -> Dict[str, Any]:\n",
    "    # Parse the JSONL log to compute basic telemetry (route counts, explore rate, p95 latency)\n",
    "    if not os.path.exists(LOG_Path):\n",
    "        return {\"status\": \"no_logs\"}\n",
    "\n",
    "    routes = 0; explores = 0; stages = Counter(); actions = Counter(); latencies: List[int] = []\n",
    "    with open(LOG_Path, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if rec.get(\"kind\") == \"route\":\n",
    "                routes += 1; st = rec.get(\"stage\"); stages[st] += 1\n",
    "                if st and \"ml+bandit\" in st: explores += 1\n",
    "                actions[rec.get(\"chosen\")] += 1\n",
    "            elif rec.get(\"kind\") == \"execute\" and rec.get(\"latency_ms\") is not None:\n",
    "                latencies.append(rec[\"latency_ms\"])\n",
    "\n",
    "    p95_latency = float(np.percentile(latencies, 95)) if latencies else None\n",
    "    return {\n",
    "        \"routes\": routes,\n",
    "        \"explore_rate\": (explores / routes) if routes else 0.0,\n",
    "        \"stage_counts\": dict(stages),\n",
    "        \"action_counts\": dict(actions),\n",
    "        \"p95_latency_ms\": p95_latency,\n",
    "    }\n",
    "\n",
    "# Friendly startup banner for notebook users\n",
    "print(\"\\n=== 🚀 Router Initialization (No-Meta, LLM-integrated + targets) ===\")\n",
    "print(\"Router loaded. Use: health(), nb_route(...), nb_execute(...), nb_feedback(...), nb_learn_supervised(...), nb_policy_metrics().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f331432",
   "metadata": {},
   "source": [
    "---\n",
    "## Router Training & Evaluation Utilities\n",
    "\n",
    "This module provides the utilities to **train, calibrate, validate, and persist the Dynamic LLM Query Router**.  \n",
    "It includes routines for **temperature calibration, mixture calibration, evaluation, artifact persistence, and supervised training**.\n",
    "\n",
    "---\n",
    "\n",
    "### Temperature Calibration Functions\n",
    "\n",
    "These functions calibrate the model’s predicted probabilities to improve confidence alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729552e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utilities ready.\n"
     ]
    }
   ],
   "source": [
    "def fit_temperature_on_val(texts: List[str], ctxs: List[Optional[str]], labels: List[int], max_iter: int = 200) -> float:\n",
    "    \"\"\"\n",
    "    Fits a temperature scaling parameter (tau) for the router's policy head\n",
    "    using validation data. This improves calibration of predicted probabilities.\n",
    "\n",
    "    Args:\n",
    "        texts: List of input queries (strings).\n",
    "        ctxs: Optional list of context strings (can be None).\n",
    "        labels: Ground-truth class labels for each query.\n",
    "        max_iter: Maximum iterations for LBFGS optimizer.\n",
    "\n",
    "    Returns:\n",
    "        Calibrated temperature scalar (tau).\n",
    "    \"\"\"\n",
    "    router.policy.eval()        # Put policy model in evaluation mode\n",
    "    router.calib.train()        # Put calibration layer in training mode (we optimize tau)\n",
    "    \n",
    "    # LBFGS optimizer is well-suited for convex optimization of calibration parameters\n",
    "    opt = torch.optim.LBFGS([router.calib.log_tau], lr=0.1, max_iter=max_iter)\n",
    "\n",
    "    # Compute feature embeddings for all (query, context) pairs\n",
    "    X = torch.stack([torch.from_numpy(router._features(q, c)).float() for q, c in zip(texts, ctxs)])\n",
    "    \n",
    "    # Get logits from policy network (no gradient since tau will be calibrated separately)\n",
    "    with torch.no_grad():\n",
    "        logits = router.policy(router.projector(X))\n",
    "    \n",
    "    # Convert labels into a tensor\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    # Closure function required by LBFGS optimizer\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        logits_cal = router.calib(logits)          # Apply calibration scaling\n",
    "        loss = F.cross_entropy(logits_cal, y)      # Standard CE loss between calibrated logits & ground-truth\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Run optimizer to adjust calibration parameter log_tau\n",
    "    opt.step(closure)\n",
    "    \n",
    "    router.calib.eval()    # Switch calibration layer back to eval mode\n",
    "\n",
    "    # Return tau = exp(log_tau) as a scalar float\n",
    "    return float(torch.exp(router.calib.log_tau).item())\n",
    "\n",
    "\n",
    "def fit_mix_temperature_on_val(texts: List[str], ctxs: List[Optional[str]], labels: List[int], max_iter: int = 200) -> float:\n",
    "    \"\"\"\n",
    "    Fits temperature scaling parameter for a mixture distribution of\n",
    "    router policy probabilities and optional LLM intent probabilities.\n",
    "\n",
    "    Args:\n",
    "        texts: List of input queries.\n",
    "        ctxs: List of contexts (or None).\n",
    "        labels: Ground truth labels.\n",
    "        max_iter: Maximum optimization iterations.\n",
    "\n",
    "    Returns:\n",
    "        Calibrated mixture temperature scalar.\n",
    "    \"\"\"\n",
    "    router.mix_calib.train()   # Set mixture calibration layer in training mode\n",
    "    \n",
    "    # Optimize log_tau parameter for mixture calibration\n",
    "    opt = torch.optim.LBFGS([router.mix_calib.log_tau], lr=0.1, max_iter=max_iter)\n",
    "\n",
    "    # Feature extraction for dataset\n",
    "    X = torch.stack([torch.from_numpy(router._features(q, c)).float() for q, c in zip(texts, ctxs)])\n",
    "\n",
    "    # Compute probabilities from router policy (with calibration already applied)\n",
    "    with torch.no_grad():\n",
    "        logits = router.calib(router.policy(router.projector(X)))\n",
    "        policy_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Compute mixture probabilities:\n",
    "    #   - If LLM intent model exists → weighted combination\n",
    "    #   - Else fallback to router policy only\n",
    "    mixed = (LLM_WEIGHT * torch.from_numpy(router.llm_intent.predict_proba(texts, ctxs)).float()\n",
    "             + (1.0 - LLM_WEIGHT) * policy_probs) if router.llm_intent else policy_probs\n",
    "    \n",
    "    # Convert labels to tensor\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    # Closure for LBFGS\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        p = router.mix_calib(mixed)                 # Apply calibration on mixture distribution\n",
    "        loss = F.nll_loss(torch.log(p + 1e-8), y)   # Use negative log-likelihood loss\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Optimize mixture calibration parameter\n",
    "    opt.step(closure)\n",
    "    router.mix_calib.eval()   # Back to eval mode\n",
    "\n",
    "    # Return calibrated mixture tau\n",
    "    return float(torch.exp(router.mix_calib.log_tau).item())\n",
    "\n",
    "\n",
    "def evaluate_split(texts: List[str], ctxs: List[Optional[str]], labels: List[int]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluates the router on a dataset split (train/val/test).\n",
    "    Computes accuracy, expected calibration error (ECE), and confusion matrix counts.\n",
    "\n",
    "    Args:\n",
    "        texts: List of queries.\n",
    "        ctxs: List of contexts (optional).\n",
    "        labels: Ground-truth labels.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics.\n",
    "    \"\"\"\n",
    "    router.policy.eval()\n",
    "    router.calib.eval()\n",
    "\n",
    "    # Compute features\n",
    "    X = torch.stack([torch.from_numpy(router._features(q, c)).float() for q, c in zip(texts, ctxs)])\n",
    "\n",
    "    # Forward pass through calibrated policy\n",
    "    with torch.no_grad():\n",
    "        logits = router.calib(router.policy(router.projector(X)))\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    # Predictions = argmax of probabilities\n",
    "    y_pred = probs.argmax(axis=1)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = float(np.mean(y_pred == np.array(labels)))\n",
    "\n",
    "    # Expected Calibration Error (ECE)\n",
    "    ece = expected_calibration_error(probs, np.array(labels), n_bins=15)\n",
    "\n",
    "    # Build confusion counts dictionary\n",
    "    cm = defaultdict(Counter)\n",
    "    for t, p in zip(labels, y_pred):\n",
    "        cm[ACTIONS[t]][ACTIONS[p]] += 1\n",
    "\n",
    "    return {\"accuracy\": acc, \"ece\": ece, \"confusion_counts\": {k: dict(v) for k, v in cm.items()}}\n",
    "\n",
    "\n",
    "def _artifacts_exist(model_dir: str, require_llm: bool) -> Tuple[bool, Dict[str, bool]]:\n",
    "    \"\"\"\n",
    "    Checks whether all required model artifacts exist in a given directory.\n",
    "\n",
    "    Args:\n",
    "        model_dir: Path to model directory.\n",
    "        require_llm: Whether LLM artifacts are mandatory.\n",
    "\n",
    "    Returns:\n",
    "        Tuple (all_present, need):\n",
    "            all_present: True if all required artifacts exist.\n",
    "            need: Dictionary mapping artifact type → existence boolean.\n",
    "    \"\"\"\n",
    "    # Check for each artifact file\n",
    "    need = {\n",
    "        \"policy\": os.path.isfile(os.path.join(model_dir, \"policy.pt\")),\n",
    "        \"temperature\": os.path.isfile(os.path.join(model_dir, \"temperature.json\")),\n",
    "        \"mix_temperature\": os.path.isfile(os.path.join(model_dir, \"mix_temperature.json\")),\n",
    "        \"bandit_state\": os.path.isfile(os.path.join(model_dir, \"lints_state.json\")),\n",
    "        \"events_log\": os.path.isfile(os.path.join(model_dir, \"router_events.jsonl\")),\n",
    "    }\n",
    "\n",
    "    # If LLM is required, validate HuggingFace checkpoint\n",
    "    if require_llm:\n",
    "        llm_dir = os.path.join(model_dir, \"llm_intent\")\n",
    "        need[\"llm_intent\"] = _is_valid_hf_checkpoint_dir(llm_dir, strict=False)\n",
    "\n",
    "    # Define required keys based on config\n",
    "    required_keys = [\"policy\", \"temperature\"] + ([\"llm_intent\"] if require_llm else [])\n",
    "    all_present = all(need.get(k, False) for k in required_keys)\n",
    "    return all_present, need\n",
    "\n",
    "\n",
    "def _save_router_artifacts(dest_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves all relevant model artifacts (policy, calibration, logs, LLM if used) to a directory.\n",
    "\n",
    "    Args:\n",
    "        dest_dir: Path where artifacts should be saved.\n",
    "    \"\"\"\n",
    "    Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save policy and temperature calibration\n",
    "    for src, dst in [\n",
    "        (POLICY_Path, os.path.join(dest_dir, \"policy.pt\")),\n",
    "        (CALIB_Path, os.path.join(dest_dir, \"temperature.json\")),\n",
    "    ]:\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "    # Save mixture calibration if available\n",
    "    mix_temp_Path = os.path.join(str(DATA_DIR), \"mix_temperature.json\")\n",
    "    if os.path.exists(mix_temp_Path):\n",
    "        shutil.copy2(mix_temp_Path, os.path.join(dest_dir, \"mix_temperature.json\"))\n",
    "\n",
    "    # Save bandit state if exists\n",
    "    if os.path.exists(BANDIT_STATE_Path):\n",
    "        shutil.copy2(BANDIT_STATE_Path, os.path.join(dest_dir, \"lints_state.json\"))\n",
    "\n",
    "    # Save event logs\n",
    "    if os.path.exists(LOG_Path):\n",
    "        shutil.copy2(LOG_Path, os.path.join(dest_dir, \"router_events.jsonl\"))\n",
    "\n",
    "    # Save LLM artifacts if enabled\n",
    "    if USE_LLM_INTENT and os.path.exists(LLM_DIR):\n",
    "        llm_dest = os.path.join(dest_dir, \"llm_intent\")\n",
    "        if os.path.exists(llm_dest):\n",
    "            shutil.rmtree(llm_dest)    # Remove existing dir\n",
    "        shutil.copytree(LLM_DIR, llm_dest)\n",
    "\n",
    "    print(\"\\n✅ Model artifacts saved to:\", dest_dir)\n",
    "\n",
    "\n",
    "def load_router(load_dir: str):\n",
    "    \"\"\"\n",
    "    Loads model artifacts from a directory back into runtime environment.\n",
    "\n",
    "    Args:\n",
    "        load_dir: Path to load artifacts from.\n",
    "    \"\"\"\n",
    "    # Copy each artifact back into working DATA_DIR\n",
    "    for src_name, dst_Path in [\n",
    "        (\"policy.pt\", POLICY_Path),\n",
    "        (\"temperature.json\", CALIB_Path),\n",
    "        (\"mix_temperature.json\", os.path.join(str(DATA_DIR), \"mix_temperature.json\")),\n",
    "        (\"lints_state.json\", BANDIT_STATE_Path),\n",
    "    ]:\n",
    "        src_Path = os.path.join(load_dir, src_name)\n",
    "        if os.path.exists(src_Path):\n",
    "            shutil.copy2(src_Path, dst_Path)\n",
    "\n",
    "    # Handle LLM artifacts\n",
    "    llm_src = os.path.join(load_dir, \"llm_intent\")\n",
    "    if USE_LLM_INTENT and os.path.exists(llm_src):\n",
    "        if os.path.exists(LLM_DIR):\n",
    "            shutil.rmtree(LLM_DIR)   # Remove existing\n",
    "        shutil.copytree(llm_src, LLM_DIR)\n",
    "\n",
    "    # Reload router internal state\n",
    "    router._load()\n",
    "\n",
    "    # Reload LLM intent model if available\n",
    "    if USE_LLM_INTENT and router.llm_intent:\n",
    "        router.llm_intent.init_or_load(LLM_DIR if os.path.exists(LLM_DIR) else None)\n",
    "\n",
    "    print(f\"✅ Router + LLM loaded from: {load_dir}\")\n",
    "\n",
    "\n",
    "def _train_router_pipeline(\n",
    "    *,\n",
    "    preprocess_cfg: Optional[PreprocessConfig] = None,\n",
    "    splits: Optional[Dict[str, List[int]]] = None,\n",
    "    epochs: int = 6,\n",
    "    random_seed: int = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main supervised training pipeline for the router.\n",
    "\n",
    "    - Loads preprocessed dataset\n",
    "    - Fits bucket structure\n",
    "    - Trains supervised classifier\n",
    "    - Calibrates with temperature scaling\n",
    "    - Evaluates on validation set\n",
    "    - Optionally trains LLM intent model\n",
    "\n",
    "    Args:\n",
    "        preprocess_cfg: Preprocessing configuration.\n",
    "        splits: Canonical dataset splits (train/val/test indices).\n",
    "        epochs: Training epochs.\n",
    "        random_seed: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing training, calibration, evaluation, and LLM info.\n",
    "    \"\"\"\n",
    "    if random_seed is None:\n",
    "        random_seed = RANDOM_SEED\n",
    "\n",
    "    # ---- Load preprocessed dataset\n",
    "    texts = batch_all.texts\n",
    "    ctxs = batch_all.context_texts or [None] * len(texts)\n",
    "    label_ids_all = [\n",
    "        LABEL2ID[l.value if (hasattr(l, \"value\")) else str(l)] for l in batch_all.labels\n",
    "    ]\n",
    "\n",
    "    # ---- Fit buckets (clustering of embeddings for efficient routing)\n",
    "    fit_info = router.fit_buckets(texts, ctxs)\n",
    "    print(\"\\n=== 📦 Bucket Fitting ===\")\n",
    "    print(f\"Status: {fit_info['status']} | Clusters: {fit_info.get('clusters')}\")\n",
    "\n",
    "    # ---- Train/validation split handling\n",
    "    if splits:\n",
    "        train_idx = np.array(splits[\"train\"], dtype=int)\n",
    "        val_idx = np.array(splits[\"val\"], dtype=int)\n",
    "    else:\n",
    "        idx = np.arange(len(df))\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            idx, test_size=0.2, random_state=random_seed, stratify=df[\"label\"]\n",
    "        )\n",
    "\n",
    "    # Helper function to index lists\n",
    "    sub = lambda lst, ids: [lst[i] for i in ids]\n",
    "\n",
    "    # ---- Build training batch\n",
    "    train_batch = SupervisedBatch(\n",
    "        texts=sub(texts, train_idx),\n",
    "        context_texts=sub(ctxs, train_idx),\n",
    "        labels=sub(batch_all.labels, train_idx),\n",
    "    )\n",
    "\n",
    "    # ---- Train supervised policy\n",
    "    train_info = router.train_supervised(train_batch, epochs=epochs)\n",
    "    print(\"\\n=== 🎯 Router Training ===\")\n",
    "    print(f\"Final Loss: {train_info['loss']:.4f}\")\n",
    "\n",
    "    # ---- Prepare validation split\n",
    "    val_texts = sub(texts, val_idx)\n",
    "    val_ctxs = sub(ctxs, val_idx)\n",
    "    val_labels = sub(label_ids_all, val_idx)\n",
    "\n",
    "    # ---- Calibrate temperature (router head)\n",
    "    tau = fit_temperature_on_val(val_texts, val_ctxs, val_labels)\n",
    "    print(\"\\n=== 🔥 Temperature Calibration (router head) ===\")\n",
    "    print(f\"Fitted Temperature (tau): {tau:.6f}\")\n",
    "\n",
    "    # ---- Calibrate mixture temperature (router + LLM)\n",
    "    mix_tau = fit_mix_temperature_on_val(val_texts, val_ctxs, val_labels)\n",
    "    print(\"\\n=== 🔥 Mixture Temperature Calibration ===\")\n",
    "    print(f\"Fitted Mix Temperature (tau_mix): {mix_tau:.6f}\")\n",
    "\n",
    "    # ---- Evaluate metrics\n",
    "    metrics = evaluate_split(val_texts, val_ctxs, val_labels)\n",
    "    print(\"\\n=== 📊 Router Validation Metrics ===\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.2f}\")\n",
    "    print(f\"ECE: {metrics['ece']:.4f}\")\n",
    "\n",
    "    # ---- Train LLM intent model if enabled\n",
    "    llm_info = None\n",
    "    if USE_LLM_INTENT:\n",
    "        llm = router.llm_intent or LLMIntent(LLM_BACKBONE)\n",
    "        llm_info = llm.train(\n",
    "            texts=sub(texts, train_idx),\n",
    "            ctxs=sub(ctxs, train_idx),\n",
    "            labels=sub(label_ids_all, train_idx),\n",
    "            val_texts=val_texts,\n",
    "            val_ctxs=val_ctxs,\n",
    "            val_labels=val_labels,\n",
    "            output_dir=LLM_DIR,\n",
    "            epochs=2,\n",
    "            batch_size=16,\n",
    "            lr=5e-5,\n",
    "        )\n",
    "        print(\"\\n=== 🤖 LLM Intent Training ===\")\n",
    "        print(llm_info)\n",
    "\n",
    "    print(\"\\n=== ✅ Training complete (router + optional LLM) ===\")\n",
    "    return {\n",
    "        \"fit_info\": fit_info,\n",
    "        \"train_info\": train_info,\n",
    "        \"tau\": float(tau),\n",
    "        \"mix_tau\": float(mix_tau),\n",
    "        \"metrics\": metrics,\n",
    "        \"llm_info\": llm_info,\n",
    "    }\n",
    "\n",
    "\n",
    "def load_or_train_router(\n",
    "    model_dir: str,\n",
    "    *,\n",
    "    preprocess_cfg: Optional[PreprocessConfig] = None,\n",
    "    splits: Optional[Dict[str, List[int]]] = None,\n",
    "    force_retrain: bool = False,\n",
    "    epochs: int = 6,\n",
    "    random_seed: Optional[int] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    High-level wrapper: Loads router if artifacts exist, otherwise trains a new one.\n",
    "\n",
    "    Args:\n",
    "        model_dir: Directory where model artifacts are/will be stored.\n",
    "        preprocess_cfg: Preprocessing configuration.\n",
    "        splits: Canonical dataset splits.\n",
    "        force_retrain: If True, ignore existing artifacts and retrain.\n",
    "        epochs: Training epochs.\n",
    "        random_seed: Random seed.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing status, action, model_dir, artifacts, and training results.\n",
    "    \"\"\"\n",
    "    require_llm = bool(USE_LLM_INTENT)\n",
    "\n",
    "    # Check if artifacts already exist\n",
    "    all_present, detail = _artifacts_exist(model_dir, require_llm=require_llm)\n",
    "\n",
    "    if all_present and not force_retrain:\n",
    "        # Load existing model\n",
    "        load_router(model_dir)\n",
    "        return {\n",
    "            \"status\": \"ok\",\n",
    "            \"action\": \"loaded\",\n",
    "            \"model_dir\": model_dir,\n",
    "            \"artifacts\": detail,\n",
    "        }\n",
    "\n",
    "    # ---- Train fresh pipeline\n",
    "    result = _train_router_pipeline(\n",
    "        preprocess_cfg=preprocess_cfg,\n",
    "        splits=splits,\n",
    "        epochs=epochs,\n",
    "        random_seed=random_seed if random_seed is not None else RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    # Save artifacts → reload them\n",
    "    _save_router_artifacts(model_dir)\n",
    "    load_router(model_dir)\n",
    "\n",
    "    # Validate saved artifacts\n",
    "    _, detail_after = _artifacts_exist(model_dir, require_llm=require_llm)\n",
    "\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"action\": \"trained\",\n",
    "        \"model_dir\": model_dir,\n",
    "        \"artifacts\": detail_after,\n",
    "        **result,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Training utilities ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a74e531",
   "metadata": {},
   "source": [
    "---\n",
    "## Router Training: Batch Creation + Train/Load Workflow\n",
    "\n",
    "This section handles two key steps in the **Dynamic LLM Query Router (DLQR)** pipeline:\n",
    "\n",
    "1. **Build a supervised training batch** from a cleaned/preprocessed dataframe.\n",
    "2. **Train or load the router** depending on whether artifacts already exist.\n",
    "\n",
    "\n",
    "### Step 1 — Create a Supervised Batch\n",
    "\n",
    "The `make_supervised_batch_from_df` function takes a `pandas.DataFrame` and constructs a  \n",
    "`SupervisedBatch` object (used downstream by the training loop).  \n",
    "\n",
    "It enforces safety checks to ensure required columns exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_supervised_batch_from_df(df: pd.DataFrame, label_col: str = \"label\"):\n",
    "    \"\"\"Create SupervisedBatch using your existing class with safety checks.\"\"\"\n",
    "    # Validate the input DataFrame has the minimum required columns:\n",
    "    # - \"query\" (text input)\n",
    "    # - label_col (default \"label\"; can be overridden to match your dataset schema)\n",
    "    required = [\"query\", label_col]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        # Fail fast with a clear error message showing what's missing and what exists\n",
    "        raise KeyError(\n",
    "            f\"Missing required columns: {missing}. \"\n",
    "            f\"Available columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Extract the query texts as strings (robust to mixed dtypes)\n",
    "    texts = df[\"query\"].astype(str).tolist()\n",
    "\n",
    "    # Optional context column:\n",
    "    # - If \"context\" exists, coerce to string (keeps alignment with texts)\n",
    "    # - Else, provide a list of Nones of equal length (model understands missing context)\n",
    "    ctxs = df[\"context\"].astype(str).tolist() if \"context\" in df.columns else [None] * len(texts)\n",
    "\n",
    "    # Labels are passed through as-is; upstream preprocessing ensured normalization/validation\n",
    "    labels = df[label_col].tolist()\n",
    "\n",
    "    # Construct the Pydantic SupervisedBatch object consumed by the router trainer\n",
    "    return SupervisedBatch(texts=texts, context_texts=ctxs, labels=labels)\n",
    "\n",
    "\n",
    "# Build supervised batch with the correct label column\n",
    "# Uses cfg.label_col to respect any custom label field (e.g., \"intent\", \"task\", etc.)\n",
    "batch_all = make_supervised_batch_from_df(df, label_col=cfg.label_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cf8ab",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2 — Train or Load the Router\n",
    "\n",
    "The `load_or_train_router` utility handles both **loading existing router artifacts** and **training from scratch** if they don’t exist.\n",
    "\n",
    "- **Load Path**  \n",
    "  If artifacts exist in `MODEL_DIR` (and `force_retrain=False`), the router will simply load them.\n",
    "\n",
    "- **Train Path**  \n",
    "  Otherwise, the function executes the full training pipeline:\n",
    "  - ✅ Supervised training  \n",
    "  - ✅ Bucket fitting  \n",
    "  - ✅ Temperature calibration (policy + mixture)  \n",
    "  - ✅ Optional LLM intent training  \n",
    "  - ✅ Save all artifacts to `MODEL_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73368ca-ab1e-4ff1-b3e0-34ef39d0c39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Router + LLM loaded from: /Users/pranshugoyal/Downloads/Query Router 17-August-2025 3/trained_router_llm_nometa_v1\n",
      "\n",
      "=== 📦 Load/Train Action: loaded (artifacts in /Users/pranshugoyal/Downloads/Query Router 17-August-2025 3/trained_router_llm_nometa_v1) ===\n"
     ]
    }
   ],
   "source": [
    "# --- Train or load router using canonical train/val\n",
    "# High-level orchestration:\n",
    "# - If trained artifacts exist in MODEL_DIR (and force_retrain=False) → load them.\n",
    "# - Otherwise, train the router end-to-end on the provided splits:\n",
    "#     * supervised policy training\n",
    "#     * temperature calibration (policy + mixture)\n",
    "#     * (optional) LLM intent training\n",
    "# Returns a dictionary with metadata (action=\"loaded\"/\"trained\"), metrics, and artifact statuses.\n",
    "train_info = load_or_train_router(\n",
    "    model_dir=MODEL_DIR,\n",
    "    preprocess_cfg=PREPROC,\n",
    "    splits=splits,\n",
    "    force_retrain=False,\n",
    "    epochs=6,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "# Friendly summary line indicating whether we loaded or trained, and where artifacts live.\n",
    "print(f\"\\n=== 📦 Load/Train Action: {train_info['action']} (artifacts in {train_info['model_dir']}) ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3780f845",
   "metadata": {},
   "source": [
    "---\n",
    "## Router Evaluation Utilities\n",
    "\n",
    "This module defines helper functions for **evaluating the Dynamic LLM Query Router (DLQR)**.  \n",
    "It supports:\n",
    "- Predicting labels via different inference modes (`predict_proba`, `predict`, `route`)\n",
    "- Evaluating both router and optional LLM intent models\n",
    "- Computing key metrics: accuracy, calibration error, F1-score, confusion matrices\n",
    "- Saving detailed evaluation reports to disk\n",
    "\n",
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _invert_label_map(label2id: Dict[str, int]) -> Dict[int, str]:\n",
    "    # Reverse a {label→id} dict into {id→label} for report readability\n",
    "    return {v: k for k, v in label2id.items()}\n",
    "\n",
    "def _as_ctx_list(ctxs: Optional[Sequence[Any]], n: int) -> List[Any]:\n",
    "    # Normalize context input to always be a list of length n\n",
    "    if ctxs is None:\n",
    "        return [None] * n\n",
    "    return list(ctxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfb83f0",
   "metadata": {},
   "source": [
    "###  Label Coercion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d19c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coerce_to_id(lbl):\n",
    "    \"\"\"Convert enum/str/int/dict to class ID via LABEL2ID (case-insensitive).\"\"\"\n",
    "    if isinstance(lbl, int):\n",
    "        return lbl\n",
    "    if hasattr(lbl, \"value\"):  # enum-like (e.g., Action.PREDICTION.value)\n",
    "        lbl = lbl.value\n",
    "    if isinstance(lbl, dict):  # unwrap dict results from router outputs\n",
    "        for k in (\"action\", \"label\", \"route\", \"decision\"):\n",
    "            if k in lbl:\n",
    "                lbl = lbl[k]\n",
    "                break\n",
    "    s = str(lbl).strip()\n",
    "    # Try case-insensitive match\n",
    "    if s.lower() in LABEL2ID:\n",
    "        return LABEL2ID[s.lower()]\n",
    "    if s in LABEL2ID:\n",
    "        return LABEL2ID[s]\n",
    "    raise KeyError(f\"Unknown label from router: {lbl!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d817d",
   "metadata": {},
   "source": [
    "## Prediction Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_labels_router(\n",
    "    texts,\n",
    "    ctxs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Try multiple router inference APIs in order of preference:\n",
    "    1. predict_proba (returns probs + labels)\n",
    "    2. predict (returns labels only)\n",
    "    3. route (per-sample execution with fallback arg handling)\n",
    "    \"\"\"\n",
    "    ctxs_list = _as_ctx_list(ctxs, len(texts))\n",
    "\n",
    "    # 1) predict_proba\n",
    "    if hasattr(router, \"predict_proba\"):\n",
    "        probs = np.asarray(router.predict_proba(texts, ctxs_list))\n",
    "        y_pred = np.argmax(probs, axis=1)\n",
    "        return y_pred, probs\n",
    "\n",
    "    # 2) predict\n",
    "    if hasattr(router, \"predict\"):\n",
    "        out = router.predict(texts, ctxs_list)\n",
    "        if not isinstance(out, (list, tuple, np.ndarray)):\n",
    "            out = [out]\n",
    "        y_pred = np.array([_coerce_to_id(o) for o in out], dtype=int)\n",
    "        return y_pred, None\n",
    "\n",
    "    # 3) route (loop over each sample individually)\n",
    "    if hasattr(router, \"route\"):\n",
    "        sig = inspect.signature(router.route)\n",
    "        params = sig.parameters\n",
    "        names = set(params.keys())\n",
    "\n",
    "        kwargs = {}\n",
    "        # Always set explicit=None (never False) to avoid enum issues\n",
    "        if \"explicit\" in names:\n",
    "            kwargs[\"explicit\"] = None\n",
    "        # Switch off extra flags if present\n",
    "        for flag in (\"return_scores\", \"return_prob\", \"return_meta\", \"trace\"):\n",
    "            if flag in names:\n",
    "                kwargs[flag] = False\n",
    "\n",
    "        preds = []\n",
    "        for t, c in zip(texts, ctxs_list):\n",
    "            o = None\n",
    "            try:\n",
    "                # Use ctx if supported\n",
    "                if \"ctx\" in names:\n",
    "                    o = router.route(t, c, **kwargs)\n",
    "                else:\n",
    "                    o = router.route(t, **kwargs)\n",
    "            except TypeError:\n",
    "                # fallback attempt if signature mismatches\n",
    "                o = router.route(t, c) if \"ctx\" in names else router.route(t)\n",
    "\n",
    "            # unwrap tuple/dict predictions\n",
    "            if isinstance(o, tuple) and len(o) > 0:\n",
    "                o = o[0]\n",
    "            if isinstance(o, dict):\n",
    "                for k in (\"action\", \"label\", \"route\", \"decision\"):\n",
    "                    if k in o:\n",
    "                        o = o[k]\n",
    "                        break\n",
    "\n",
    "            preds.append(_coerce_to_id(o))\n",
    "\n",
    "        return np.array(preds, dtype=int), None\n",
    "\n",
    "    raise RuntimeError(\"No usable inference found on `router` (predict_proba/predict/route).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009e5b9",
   "metadata": {},
   "source": [
    "## LLM Prediction Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d46150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_labels_llm(\n",
    "    texts: Sequence[str],\n",
    "    ctxs: Optional[Sequence[Any]] = None,\n",
    ") -> Optional[Tuple[np.ndarray, Optional[np.ndarray]]]:\n",
    "    # Skip if LLM intent is disabled or unavailable\n",
    "    if not USE_LLM_INTENT or not getattr(router, \"llm_intent\", None):\n",
    "        return None\n",
    "\n",
    "    llm = router.llm_intent\n",
    "    ctxs_list = _as_ctx_list(ctxs, len(texts))\n",
    "\n",
    "    # Preferred: predict_proba (with full probability distribution)\n",
    "    if hasattr(llm, \"predict_proba\"):\n",
    "        probs = np.array(llm.predict_proba(texts, ctxs_list))\n",
    "        y_pred = np.argmax(probs, axis=1)\n",
    "        return y_pred, probs\n",
    "\n",
    "    # Fallback: predict (labels only)\n",
    "    if hasattr(llm, \"predict\"):\n",
    "        out = llm.predict(texts, ctxs_list)\n",
    "        try:\n",
    "            y_pred = np.array(out, dtype=int)\n",
    "        except Exception:\n",
    "            # If outputs are strings, map back to IDs\n",
    "            y_pred = np.array([LABEL2ID[str(x)] for x in out])\n",
    "        return y_pred, None\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7085f1",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a91d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_router_model(\n",
    "    *,\n",
    "    preprocess_cfg: Optional[PreprocessConfig] = None,\n",
    "    splits: Optional[Dict[str, List[int]]] = None,\n",
    "    test_size: float = 0.15,  # ignored when splits provided\n",
    "    random_seed: Optional[int] = None,\n",
    "    save_report_dir: Optional[str] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate on canonical TEST split (or fallback stratified test split).\"\"\"\n",
    "    if random_seed is None:\n",
    "        random_seed = RANDOM_SEED\n",
    "\n",
    "    # Prepare full dataset\n",
    "    texts = list(batch_all.texts)\n",
    "    ctxs = batch_all.context_texts or [None] * len(texts)\n",
    "    label_ids_all = [LABEL2ID[l.value if (hasattr(l, \"value\")) else str(l)] for l in batch_all.labels]\n",
    "    id2label = _invert_label_map(LABEL2ID)\n",
    "\n",
    "    # Select TEST split indices\n",
    "    if splits and len(splits.get(\"test\", [])) > 0:\n",
    "        test_idx = np.array(splits[\"test\"], dtype=int)\n",
    "    else:\n",
    "        idx = np.arange(len(df))\n",
    "        _, test_idx = train_test_split(idx, test_size=test_size, random_state=random_seed, stratify=df[\"label\"])\n",
    "\n",
    "    sub = lambda lst, ids: [lst[i] for i in ids]\n",
    "    X_text, X_ctx, y_true = sub(texts, test_idx), sub(ctxs, test_idx), np.array(sub(label_ids_all, test_idx))\n",
    "\n",
    "    # --- Baseline metrics (accuracy, ECE, confusion counts)\n",
    "    base_metrics = evaluate_split(X_text, X_ctx, y_true)\n",
    "\n",
    "    # --- Router-specific metrics\n",
    "    y_pred_router, _ = _predict_labels_router(X_text, X_ctx)\n",
    "    clf_report_router = classification_report(\n",
    "        y_true, y_pred_router, target_names=[id2label[i] for i in sorted(set(y_true))], digits=4, output_dict=True\n",
    "    )\n",
    "    f1_macro_router = f1_score(y_true, y_pred_router, average=\"macro\")\n",
    "    cm_router = confusion_matrix(y_true, y_pred_router).tolist()\n",
    "\n",
    "    # --- Optional LLM head metrics\n",
    "    llm_eval = None\n",
    "    maybe_llm = _predict_labels_llm(X_text, X_ctx)\n",
    "    if maybe_llm is not None:\n",
    "        y_pred_llm, _ = maybe_llm\n",
    "        clf_report_llm = classification_report(\n",
    "            y_true, y_pred_llm, target_names=[id2label[i] for i in sorted(set(y_true))], digits=4, output_dict=True\n",
    "        )\n",
    "        f1_macro_llm = f1_score(y_true, y_pred_llm, average=\"macro\")\n",
    "        cm_llm = confusion_matrix(y_true, y_pred_llm).tolist()\n",
    "        llm_eval = {\n",
    "            \"f1_macro\": float(f1_macro_llm),\n",
    "            \"classification_report\": clf_report_llm,\n",
    "            \"confusion_matrix\": cm_llm,\n",
    "        }\n",
    "\n",
    "    # Aggregate results\n",
    "    result = {\n",
    "        \"status\": \"ok\",\n",
    "        \"n_test\": int(len(X_text)),\n",
    "        \"accuracy\": float(base_metrics.get(\"accuracy\", np.nan)),\n",
    "        \"ece\": float(base_metrics.get(\"ece\", np.nan)),\n",
    "        \"router\": {\n",
    "            \"f1_macro\": float(f1_macro_router),\n",
    "            \"classification_report\": clf_report_router,\n",
    "            \"confusion_matrix\": cm_router,\n",
    "        },\n",
    "        \"llm_intent\": llm_eval,\n",
    "    }\n",
    "\n",
    "    # Save to disk if report directory is provided\n",
    "    if save_report_dir:\n",
    "        out_dir = Path(save_report_dir)\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        with open(out_dir / \"router_eval_report.json\", \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        print(f\"📝 Saved evaluation report to: {out_dir / 'router_eval_report.json'}\")\n",
    "\n",
    "    # Console summary\n",
    "    print(\"\\n=== ✅ Router Evaluation (TEST) ===\")\n",
    "    print(f\"Samples: {result['n_test']} | Accuracy: {result['accuracy']:.4f} | ECE: {result['ece']:.4f}\")\n",
    "    print(f\"Macro F1 (router): {result['router']['f1_macro']:.4f}\")\n",
    "    if result[\"llm_intent\"] is not None:\n",
    "        print(f\"Macro F1 (LLM intent): {result['llm_intent']['f1_macro']:.4f}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c03b7",
   "metadata": {},
   "source": [
    "## Router Evaluation & Confusion Matrix Plot\n",
    "\n",
    "This section evaluates the **Dynamic LLM Query Router (DLQR)** on the **canonical TEST split**  \n",
    "and visualises its performance via a **confusion matrix heatmap**.\n",
    "\n",
    "\n",
    "### Step 1 — Run Evaluation\n",
    "\n",
    "We call `evaluate_router_model` to generate accuracy, calibration, F1 scores,  \n",
    "and confusion matrix statistics. The results are stored in `eval_info` and  \n",
    "optionally saved to `REPORTS_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_info = evaluate_router_model(\n",
    "    preprocess_cfg=PREPROC,     # preprocessing configuration used for text/context cleaning\n",
    "    splits=splits,              # canonical train/val/test splits (ensures reproducibility)\n",
    "    save_report_dir=REPORTS_DIR,# where to persist evaluation JSON report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296d9d2",
   "metadata": {},
   "source": [
    "### Step 2 — Extract Confusion Matrix\n",
    "\n",
    "The confusion matrix is retrieved from the evaluation dictionary and converted into a NumPy array.\n",
    "Labels are sorted according to the LABEL2ID mapping to maintain consistent axis ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990681e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = np.array(eval_info[\"router\"][\"confusion_matrix\"])\n",
    "labels = [k for k,_ in sorted(LABEL2ID.items(), key=lambda kv: kv[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af332fdf",
   "metadata": {},
   "source": [
    "### Step 3 — Plot Confusion Matrix\n",
    "\n",
    "We use Matplotlib to render a heatmap with counts per (True, Predicted) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c92ec-6120-4920-823e-fde6656bf770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Saved evaluation report to: /Users/pranshugoyal/Downloads/Query Router 17-August-2025 3/reports/router_eval_report.json\n",
      "\n",
      "=== ✅ Router Evaluation (TEST) ===\n",
      "Samples: 1536 | Accuracy: 0.9668 | ECE: 0.1879\n",
      "Macro F1 (router): 1.0000\n",
      "Macro F1 (LLM intent): 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "plt.title(\"Router — Confusion Matrix\")\n",
    "plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(labels)), labels)\n",
    "\n",
    "# Make text color adapt to background for readability\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(\n",
    "            j, i, str(cm[i, j]),\n",
    "            ha=\"center\", va=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "        )\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa57600-d8f7-4ecb-a8b5-89c270fd1e62",
   "metadata": {},
   "source": [
    "---\n",
    "## Query Execution Helpers for DLQR\n",
    "\n",
    "This section defines helper utilities for **executing a single query** through the  \n",
    "Dynamic LLM Query Router (DLQR) pipeline.  \n",
    "\n",
    "It provides:\n",
    "- JSON-safe stringification for inputs  \n",
    "- Flexible coercion of `explicit_action` into the standard `Action` enum  \n",
    "- Robust construction of `RouteRequest` objects (with introspection fallback)  \n",
    "- Ensures router models are loaded before execution  \n",
    "- Unified entry point `execute_query()` with multiple fallback strategies  \n",
    "\n",
    "### JSON Stringifier\n",
    "\n",
    "* Ensures all inputs are stringified before passing to the router.\n",
    "* If input is a dict or list, tries json.dumps; otherwise falls back to str().\n",
    "* Guarantees router always receives a safe string (prevents serialization errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _jsonify(x: Any) -> str:\n",
    "    \"\"\"Safe-ish stringify: dict/list -> JSON; other -> str().\"\"\"\n",
    "    if isinstance(x, (dict, list)):\n",
    "        try:\n",
    "            return json.dumps(x, ensure_ascii=False)\n",
    "        except Exception:\n",
    "            return str(x)\n",
    "    return str(x) if x is not None else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6330993",
   "metadata": {},
   "source": [
    "### Explicit Action Coercion\n",
    "\n",
    "* Handles different user inputs for explicit routing action:\n",
    "    * None → no explicit override\n",
    "\t* Action enum-like object → returned directly\n",
    "\t* int → looked up in LABEL2ID inverse map\n",
    "\t* str → lowercased and converted into Action enum\n",
    "* Raises ValueError for unknown or invalid action names/IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a683dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coerce_action(explicit_action: Optional[Union[str, int, Any]]) -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Convert str/int/enum-like to your Action enum instance, or None.\n",
    "    - str: case-insensitive; supports label names (e.g. 'prediction')\n",
    "    - int: maps via inverse of LABEL2ID if available\n",
    "    - enum: passed through (if it has .value we assume it's already Action)\n",
    "    \"\"\"\n",
    "    if explicit_action is None:\n",
    "        return None\n",
    "\n",
    "    # Already enum-like?\n",
    "    if hasattr(explicit_action, \"value\"):\n",
    "        return explicit_action  # assume Action\n",
    "\n",
    "    # Int -> id2label -> Action\n",
    "    if isinstance(explicit_action, int):\n",
    "        try:\n",
    "            id2label = {v: k for k, v in LABEL2ID.items()}\n",
    "            name = id2label[int(explicit_action)]\n",
    "            return Action(name.lower())\n",
    "        except Exception as e:\n",
    "            raise ValueError(\n",
    "                f\"Unknown explicit_action id: {explicit_action}. Valid ids: {sorted(LABEL2ID.values())}\"\n",
    "            ) from e\n",
    "\n",
    "    # String label\n",
    "    if isinstance(explicit_action, str):\n",
    "        name = explicit_action.strip().lower()\n",
    "        try:\n",
    "            return Action(name)\n",
    "        except Exception as e:\n",
    "            valid = \", \".join(sorted(getattr(Action, \"__members__\", {}).keys())) or \"prediction|cbr|qa|summarize\"\n",
    "            raise ValueError(f\"Unknown action: {explicit_action!r}. Use one of: {valid}\") from e\n",
    "\n",
    "    # Unknown type\n",
    "    raise ValueError(\n",
    "        \"explicit_action must be None, str label, int id, or Action enum.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e0228",
   "metadata": {},
   "source": [
    "### RouteRequest Builder\n",
    "\n",
    "* Uses Python inspect.signature to introspect RouteRequest constructor.\n",
    "* Builds payload dynamically so that only supported fields are included.\n",
    "* Falls back to positional instantiation if keyword-based construction fails.\n",
    "* Ensures compatibility across different versions of the router API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f132d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_request(query_str: str, context_str: Optional[str], explicit: Optional[Any], *, trace: bool=False):\n",
    "    \"\"\"\n",
    "    Create a RouteRequest, being tolerant to different constructor signatures.\n",
    "    Known fields: query, context_text, explicit_action, (maybe) trace\n",
    "    \"\"\"\n",
    "    # Introspect RouteRequest to only pass supported kwargs\n",
    "    try:\n",
    "        sig = inspect.signature(RouteRequest)\n",
    "        params = set(sig.parameters.keys())\n",
    "    except Exception:\n",
    "        params = {\"query\", \"context_text\", \"explicit_action\"}  # best-guess\n",
    "\n",
    "    payload = {}\n",
    "    if \"query\" in params:\n",
    "        payload[\"query\"] = query_str\n",
    "    if \"context_text\" in params:\n",
    "        payload[\"context_text\"] = context_str\n",
    "    if \"explicit_action\" in params:\n",
    "        payload[\"explicit_action\"] = explicit\n",
    "    if \"trace\" in params:\n",
    "        payload[\"trace\"] = bool(trace)\n",
    "\n",
    "    try:\n",
    "        return RouteRequest(**payload)\n",
    "    except Exception:\n",
    "        # last resort: try positional minimal form\n",
    "        return RouteRequest(query_str, context_str, explicit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6dd986",
   "metadata": {},
   "source": [
    "### Router Loading Helper\n",
    "\n",
    "* Ensures router model artifacts are loaded once per directory.\n",
    "* Avoids reloading unless force_reload=True or directory changes.\n",
    "* Maintains global variable __ROUTER_LOADED_FROM_DIR for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "__ROUTER_LOADED_FROM_DIR = None\n",
    "def _ensure_loaded(model_dir: str | None = None, force_reload: bool = False):\n",
    "    global __ROUTER_LOADED_FROM_DIR\n",
    "    if model_dir is not None and (force_reload or __ROUTER_LOADED_FROM_DIR != model_dir):\n",
    "        load_router(model_dir)\n",
    "        __ROUTER_LOADED_FROM_DIR = model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f8ad35",
   "metadata": {},
   "source": [
    "### Single Query Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e784d-8a96-4cb9-bd1c-bd287673a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- single query ----------\n",
    "@torch.no_grad()\n",
    "def execute_query(\n",
    "    query: Union[str, dict, list],\n",
    "    context: Union[str, dict, list, None] = None,\n",
    "    *,\n",
    "    model_dir: Optional[str] = None,\n",
    "    force_reload: bool = False,\n",
    "    explicit_action: Optional[Union[str, int, Any]] = None,  # e.g. \"prediction\" or id\n",
    "    return_full: bool = False,\n",
    "    trace: bool = False,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Run a single query through the notebook executor (nb_execute) after ensuring artifacts are loaded.\n",
    "    Falls back to route-only result if nb_execute is unavailable.\n",
    "\n",
    "    Returns:\n",
    "        - default: the `output` field from nb_execute(...)\n",
    "        - if return_full=True: the entire execution envelope (dict)\n",
    "    \"\"\"\n",
    "    _ensure_loaded(model_dir, force_reload)\n",
    "\n",
    "    query_str = _jsonify(query)\n",
    "    context_str = _jsonify(context) if context is not None else None\n",
    "    explicit = _coerce_action(explicit_action)\n",
    "\n",
    "    # Build a tolerant request for nb_execute\n",
    "    req = _build_request(query_str, context_str, explicit, trace=trace)\n",
    "\n",
    "    # Prefer nb_execute if available\n",
    "    if \"nb_execute\" in globals() and callable(globals()[\"nb_execute\"]):\n",
    "        start = time.time()\n",
    "        env = nb_execute(req)  # expected: {\"decision\": {...}, \"output\": ..., ...}\n",
    "        env = env if isinstance(env, dict) else {\"output\": env}\n",
    "        env.setdefault(\"trace_id\", str(uuid.uuid4()))\n",
    "        env.setdefault(\"timing_sec\", time.time() - start)\n",
    "        return env if return_full else env.get(\"output\")\n",
    "\n",
    "    # Fallback path: try nb_route → router.route (no tool execution)\n",
    "    result = {\"trace_id\": str(uuid.uuid4())}\n",
    "\n",
    "    if \"nb_route\" in globals() and callable(globals()[\"nb_route\"]):\n",
    "        try:\n",
    "            routed = nb_route(req)\n",
    "            result[\"decision\"] = routed.get(\"decision\", routed)\n",
    "        except Exception as e:\n",
    "            result[\"decision_error\"] = f\"nb_route failed: {e}\"\n",
    "\n",
    "    if \"decision\" not in result:\n",
    "        # final fallback to raw router.route\n",
    "        try:\n",
    "            # IMPORTANT: your Router.route expects explicit=None or Action (not False)\n",
    "            dec = router.route(query_str, context_str, explicit=None if explicit is None else explicit)\n",
    "            result[\"decision\"] = {\"action\": getattr(dec, \"value\", dec)}\n",
    "        except Exception as e:\n",
    "            result[\"decision_error\"] = f\"router.route failed: {e}\"\n",
    "\n",
    "    # No tool executed in fallback; return the decision envelope\n",
    "    return result if return_full else result.get(\"decision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac5f2a7-57d3-4d79-86e8-ca891b9e6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Pretty Printing & CLI Helpers\n",
    "# ===============================\n",
    "# This module provides utilities to render router execution results nicely in either:\n",
    "# - Rich terminal (if `rich` is available), or\n",
    "# - Plain stdout fallback.\n",
    "# It includes specialized renderers for QA, Summarize, Prediction, CBR, and generic outputs,\n",
    "# plus a `pretty_print_result` entrypoint and an `auto_pretty` convenience wrapper.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import re\n",
    "import textwrap\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "# --------- optional rich UI ---------\n",
    "try:\n",
    "    # Try importing 'rich' for attractive console output. If unavailable, fall back to print().\n",
    "    from rich.console import Console\n",
    "    from rich.table import Table\n",
    "    from rich.panel import Panel\n",
    "    from rich.text import Text\n",
    "    from rich import box\n",
    "\n",
    "    _RICH = True\n",
    "    console = Console()\n",
    "except Exception:\n",
    "    # Rich not installed or failed to import — disable rich UI.\n",
    "    _RICH = False\n",
    "    console = None  # type: ignore\n",
    "\n",
    "\n",
    "# --------- helpers ---------\n",
    "def _is_nan(x) -> bool:\n",
    "    # Robust NaN check (avoids exceptions for non-float inputs).\n",
    "    try:\n",
    "        return isinstance(x, float) and math.isnan(x)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _fmt(val: Any) -> str:\n",
    "    # Generic formatter for table cells:\n",
    "    # - None/NaN as em dash\n",
    "    # - Floats get compact or fixed precision depending on magnitude\n",
    "    # - Others -> str()\n",
    "    if val is None or _is_nan(val):\n",
    "        return \"—\"\n",
    "    if isinstance(val, float):\n",
    "        return f\"{val:.6f}\" if abs(val) < 1e-4 or abs(val) > 9_999 else f\"{val:.6g}\"\n",
    "    return str(val)\n",
    "\n",
    "\n",
    "def _shorten(s: str, width: int = 1000) -> str:\n",
    "    # Truncate long strings for display. Preserves whole-word boundaries with an ellipsis.\n",
    "    s = (s or \"\").strip()\n",
    "    if len(s) <= width:\n",
    "        return s\n",
    "    return textwrap.shorten(s, width=width, placeholder=\" …\")\n",
    "\n",
    "\n",
    "def _decision(meta: Dict[str, Any] | None) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Extract (chosen/action, target_id) from a decision blob.\"\"\"\n",
    "    dec = meta or {}\n",
    "    chosen = dec.get(\"chosen\", dec.get(\"action\"))\n",
    "    target = dec.get(\"target_id\")\n",
    "    return chosen, target\n",
    "\n",
    "\n",
    "def _print_header(title: str, color: str = \"cyan\") -> None:\n",
    "    # Print a section header (Rich panel if available; plain fallback otherwise).\n",
    "    if _RICH:\n",
    "        console.print(Panel(Text(title, style=\"bold white\"), border_style=color, expand=False))\n",
    "    else:\n",
    "        print(f\"\\n=== {title} ===\")\n",
    "\n",
    "\n",
    "def _print_kv_table(pairs: List[Tuple[str, Any]], accent: str = \"cyan\") -> None:\n",
    "    # Render key-value pairs in a compact 2-column table (Rich) or simple \"k: v\" lines.\n",
    "    if _RICH:\n",
    "        tbl = Table(box=box.MINIMAL_DOUBLE_HEAD, show_header=False, pad_edge=False)\n",
    "        tbl.add_column(\"k\", style=f\"bold {accent}\", no_wrap=True)\n",
    "        tbl.add_column(\"v\", style=\"white\")\n",
    "        for k, v in pairs:\n",
    "            tbl.add_row(str(k), _fmt(v))\n",
    "        console.print(tbl)\n",
    "    else:\n",
    "        for k, v in pairs:\n",
    "            print(f\"{k}: {_fmt(v)}\")\n",
    "\n",
    "\n",
    "def _list_of_dicts_table(rows: List[Dict[str, Any]], title: str | None = None) -> None:\n",
    "    \"\"\"Render list-of-dicts as a table. Silent if empty (avoids noise).\"\"\"\n",
    "    if not rows:\n",
    "        return\n",
    "\n",
    "    # Gather all keys across rows in first-seen order to define columns.\n",
    "    keys: List[str] = []\n",
    "    seen = set()\n",
    "    for r in rows:\n",
    "        for k in r.keys():\n",
    "            if k not in seen:\n",
    "                seen.add(k)\n",
    "                keys.append(k)\n",
    "\n",
    "    if _RICH:\n",
    "        # Pretty table with discovered columns.\n",
    "        tbl = Table(*[k.upper() for k in keys], box=box.SIMPLE_HEAVY, title=title)\n",
    "        for r in rows:\n",
    "            tbl.add_row(*[_fmt(r.get(k)) for k in keys])\n",
    "        console.print(tbl)\n",
    "    else:\n",
    "        # Plain-text fallback, enumerating rows for readability.\n",
    "        if title:\n",
    "            print(f\"\\n{title}:\")\n",
    "        for i, r in enumerate(rows, 1):\n",
    "            print(f\"  #{i}\")\n",
    "            for k in keys:\n",
    "                print(f\"    - {k}: {_fmt(r.get(k))}\")\n",
    "\n",
    "\n",
    "def _maybe_split_points(text: str) -> List[str]:\n",
    "    # Attempt to split summaries into bullet points. Handles:\n",
    "    # - Multi-line lists\n",
    "    # - Single line with \"1. ... 2. ... - ... • ...\" patterns\n",
    "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
    "    if len(lines) == 1:\n",
    "        bullets = re.split(r\"\\s*(?:\\d+[\\.\\)]|[-•])\\s+\", lines[0])\n",
    "        bullets = [b for b in bullets if b]\n",
    "        if len(bullets) > 1:\n",
    "            return bullets\n",
    "    return lines\n",
    "\n",
    "\n",
    "def _pick(d: Dict[str, Any], keys: List[str]) -> Any:\n",
    "    # Return the first present key's value from dict `d`, else None.\n",
    "    for k in keys:\n",
    "        if k in d:\n",
    "            return d[k]\n",
    "    return None\n",
    "\n",
    "\n",
    "def _pct(x: Any) -> str:\n",
    "    \"\"\"\n",
    "    Format numeric as percentage. Strings pass through (e.g., '99.9%').\n",
    "    [0,1] → prob; otherwise treated as already-percent value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            return x\n",
    "        fx = float(x)\n",
    "        if 0.0 <= fx <= 1.0:\n",
    "            return f\"{fx * 100:.2f}%\"\n",
    "        return f\"{fx:.2f}%\"\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "\n",
    "# --- helper: parse numeric prob for sorting ---\n",
    "def _to_prob_float(v) -> float | None:\n",
    "    # Normalize probabilities provided as '0.87', 0.87, '87%', or 87.\n",
    "    try:\n",
    "        if isinstance(v, str) and v.strip().endswith(\"%\"):\n",
    "            return float(v.strip().strip(\"%\")) / 100.0\n",
    "        f = float(v)\n",
    "        if 0.0 <= f <= 1.0:\n",
    "            return f\n",
    "        return f / 100.0  # assume 0–100 numeric → convert to 0–1\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --------- specialized renderers ---------\n",
    "def _render_qa(output: Any) -> None:\n",
    "    # Render QA-style text in a panel or plain section.\n",
    "    text = _shorten(str(output), 2000)\n",
    "    if _RICH:\n",
    "        console.print(Panel(text, title=\"Answer\", border_style=\"green\"))\n",
    "    else:\n",
    "        print(\"Answer:\")\n",
    "        print(text)\n",
    "\n",
    "\n",
    "def _render_summary(output: Any) -> None:\n",
    "    # Render summaries as enumerated key points (list or heuristically split text).\n",
    "    points: List[str]\n",
    "    if isinstance(output, list):\n",
    "        points = [str(p).strip() for p in output if str(p).strip()]\n",
    "    else:\n",
    "        points = _maybe_split_points(str(output))\n",
    "\n",
    "    if _RICH:\n",
    "        tbl = Table(\"#\", \"KEY POINT\", box=box.SIMPLE_HEAVY)\n",
    "        for i, p in enumerate(points, 1):\n",
    "            tbl.add_row(str(i), _shorten(p, 300))\n",
    "        console.print(tbl)\n",
    "    else:\n",
    "        print(\"Key Points:\")\n",
    "        for i, p in enumerate(points, 1):\n",
    "            print(f\"  {i}. {p}\")\n",
    "\n",
    "\n",
    "def _render_prediction_like(output: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Prediction-style renderer:\n",
    "      - Predicted Diagnosis (or aliases)\n",
    "      - Confidence (if present; formats to %)\n",
    "      - Class Probabilities as a neat table (sorted desc)\n",
    "    \"\"\"\n",
    "    # Accept multiple possible keys to be resilient to different tool outputs.\n",
    "    diagnosis = _pick(output, [\"Predicted Diagnosis\", \"Suggested Diagnosis\", \"diagnosis\", \"label\", \"prediction\"])\n",
    "    confidence = _pick(output, [\"Confidence\", \"confidence\", \"conf\", \"score\", \"probability\"])\n",
    "    class_probs = _pick(\n",
    "        output,\n",
    "        [\"Class Probabilities\", \"class_probabilities\", \"probs\", \"class_probs\", \"probabilities\"],\n",
    "    )\n",
    "\n",
    "    # Normalize probabilities to rows: (label, pct_str, float_prob_for_sort)\n",
    "    prob_rows: List[Tuple[str, str, Optional[float]]] = []\n",
    "    if isinstance(class_probs, dict):\n",
    "        for k, v in class_probs.items():\n",
    "            prob_rows.append((str(k), _pct(v), _to_prob_float(v)))\n",
    "    elif isinstance(class_probs, list):\n",
    "        # Support list-of-tuples like [(\"OSCC\", 0.9997), ...]\n",
    "        try:\n",
    "            for k, v in class_probs:\n",
    "                prob_rows.append((str(k), _pct(v), _to_prob_float(v)))\n",
    "        except Exception:\n",
    "            prob_rows = []\n",
    "\n",
    "    # Sort: valid numbers first, highest-to-lowest; unknowns last.\n",
    "    prob_rows.sort(key=lambda t: (t[2] is None, -(t[2] or -1.0)))\n",
    "\n",
    "    if _RICH:\n",
    "        # Build a compact grid with a nested table for probabilities.\n",
    "        grid = Table.grid(padding=(0, 1))\n",
    "        grid.add_column(justify=\"right\", style=\"bold\")\n",
    "        grid.add_column()\n",
    "\n",
    "        if diagnosis is not None:\n",
    "            grid.add_row(\"Predicted Diagnosis\", _fmt(diagnosis))\n",
    "        if confidence is not None:\n",
    "            grid.add_row(\"Confidence\", _pct(confidence))\n",
    "\n",
    "        if prob_rows:\n",
    "            probs_tbl = Table(\"Class\", \"Probability\", box=box.SIMPLE_HEAVY, show_header=True)\n",
    "            for label, pct_s, _ in prob_rows:\n",
    "                probs_tbl.add_row(label, pct_s)\n",
    "            grid.add_row(\"Class Probabilities\", probs_tbl)\n",
    "\n",
    "        console.print(Panel(grid, title=\"Prediction\", border_style=\"yellow\"))\n",
    "    else:\n",
    "        # Plain-text fallback\n",
    "        if diagnosis is not None:\n",
    "            print(f\"Predicted Diagnosis: {_fmt(diagnosis)}\")\n",
    "        if confidence is not None:\n",
    "            print(f\"Confidence: {_pct(confidence)}\")\n",
    "        if prob_rows:\n",
    "            print(\"Class Probabilities:\")\n",
    "            for label, pct_s, _ in prob_rows:\n",
    "                print(f\"  - {label}: {pct_s}\")\n",
    "\n",
    "\n",
    "def _render_cbr_like(output: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Minimal CBR renderer (mirrors prediction style for 'Suggested Diagnosis' + similar cases).\n",
    "    \"\"\"\n",
    "    diagnosis = _pick(output, [\"Suggested Diagnosis\", \"Predicted Diagnosis\", \"diagnosis\", \"label\", \"prediction\"])\n",
    "    similar = _pick(\n",
    "        output,\n",
    "        [\"Top 3 Similar Cases\", \"Top-3 Similar Cases\", \"Top Similar Cases\", \"similar_cases\", \"neighbors\"],\n",
    "    )\n",
    "\n",
    "    if _RICH:\n",
    "        console.print(Panel(_fmt(diagnosis), title=\"Suggested Diagnosis\", border_style=\"yellow\"))\n",
    "        if isinstance(similar, list) and similar:\n",
    "            _list_of_dicts_table(similar, title=\"Top 3 Similar Cases\")\n",
    "    else:\n",
    "        print(f\"Suggested Diagnosis: {_fmt(diagnosis)}\")\n",
    "        if isinstance(similar, list) and similar:\n",
    "            _list_of_dicts_table(similar, title=\"Top 3 Similar Cases\")\n",
    "\n",
    "\n",
    "def _render_rag_like(output: Dict[str, Any]) -> None:\n",
    "    # Heuristic renderer for RAG-like outputs with sources.\n",
    "    answer = output.get(\"answer\") or output.get(\"response\") or output.get(\"summary\") or output.get(\"text\")\n",
    "    sources = output.get(\"sources\") or output.get(\"citations\") or output.get(\"documents\") or []\n",
    "    _render_qa(answer)\n",
    "    if isinstance(sources, list) and sources:\n",
    "        norm: List[Dict[str, Any]] = []\n",
    "        for s in sources:\n",
    "            if isinstance(s, dict):\n",
    "                norm.append(\n",
    "                    {\n",
    "                        \"title\": s.get(\"title\") or s.get(\"name\") or \"—\",\n",
    "                        \"url\": s.get(\"url\") or s.get(\"link\") or \"—\",\n",
    "                        \"score\": s.get(\"score\") or s.get(\"similarity\") or \"—\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                norm.append({\"title\": str(s), \"url\": \"—\", \"score\": \"—\"})\n",
    "        _list_of_dicts_table(norm, title=\"Sources\")\n",
    "\n",
    "\n",
    "def _render_generic(output: Any) -> None:\n",
    "    # Generic renderer for dict/list/scalar outputs.\n",
    "    if isinstance(output, dict):\n",
    "        # Separate nested list-of-dicts for tabular rendering.\n",
    "        lists = {k: v for k, v in output.items() if isinstance(v, list) and v and isinstance(v[0], dict)}\n",
    "        scalars = [(k, v) for k, v in output.items() if k not in lists]\n",
    "\n",
    "        if scalars:\n",
    "            _print_kv_table(scalars, accent=\"white\")\n",
    "        for k, rows in lists.items():\n",
    "            _list_of_dicts_table(rows, title=str(k))\n",
    "    elif isinstance(output, list):\n",
    "        if output and isinstance(output[0], dict):\n",
    "            _list_of_dicts_table(output)\n",
    "        else:\n",
    "            if _RICH:\n",
    "                tbl = Table(\"#\", \"ITEM\", box=box.SIMPLE_HEAVY)\n",
    "                for i, item in enumerate(output, 1):\n",
    "                    tbl.add_row(str(i), _fmt(item))\n",
    "                console.print(tbl)\n",
    "            else:\n",
    "                for i, item in enumerate(output, 1):\n",
    "                    print(f\"  {i}. {_fmt(item)}\")\n",
    "    else:\n",
    "        # Scalar or unknown type → treat as QA-style text.\n",
    "        _render_qa(output)\n",
    "\n",
    "\n",
    "# --------- main entrypoint ---------\n",
    "def pretty_print_result(\n",
    "    full: Dict[str, Any],\n",
    "    title: str = \"Execution\",\n",
    "    *,\n",
    "    show_decision: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-print an execute_query(...) result regardless of action/output shape.\n",
    "\n",
    "    Args:\n",
    "        full: The result dict, usually with 'decision' and 'output' keys.\n",
    "        title: Header title (kept as-is). Pass a better string to avoid odd titles.\n",
    "        show_decision: If True, also prints a small 'Decision' card (Chosen/Target).\n",
    "    \"\"\"\n",
    "    # Defensive extraction of action metadata.\n",
    "    dec = full.get(\"decision\", {}) if isinstance(full, dict) else {}\n",
    "    chosen, target = _decision(dec)\n",
    "\n",
    "    # Header\n",
    "    _print_header(title, color=\"cyan\")\n",
    "\n",
    "    # Optional decision card showing the selected action and target id (if any).\n",
    "    if show_decision:\n",
    "        _print_kv_table([(\"Chosen\", chosen), (\"Target\", target)], accent=\"cyan\")\n",
    "\n",
    "    # Extract the tool output payload\n",
    "    output = full.get(\"output\", None)\n",
    "\n",
    "    # Robust action detection (case/format insensitive)\n",
    "    action = (chosen or \"\")\n",
    "    action_upper = action.upper() if isinstance(action, str) else \"\"\n",
    "\n",
    "    try:\n",
    "        # Specialized renderers by action\n",
    "        if \"QA\" in action_upper:\n",
    "            _render_qa(output)\n",
    "        elif \"SUMMARIZE\" in action_upper:\n",
    "            _render_summary(output)\n",
    "        elif \"PREDICT\" in action_upper or \"PREDICTION\" in action_upper:\n",
    "            if isinstance(output, dict):\n",
    "                _render_prediction_like(output)\n",
    "            else:\n",
    "                _render_generic(output)\n",
    "        elif \"CBR\" in action_upper:\n",
    "            if isinstance(output, dict):\n",
    "                _render_cbr_like(output)\n",
    "            else:\n",
    "                _render_generic(output)\n",
    "        # Heuristic: some RAG engines return dicts with 'answer'/'sources'\n",
    "        elif isinstance(output, dict) and any(k in output for k in (\"answer\", \"sources\", \"citations\", \"documents\")):\n",
    "            _render_rag_like(output)\n",
    "        else:\n",
    "            # Fallback renderer\n",
    "            _render_generic(output)\n",
    "    except Exception as e:\n",
    "        # Never let rendering errors crash the app; show raw output.\n",
    "        if _RICH:\n",
    "            console.print(Panel(f\"Render error: {e}\", border_style=\"red\"))\n",
    "            console.print(Panel(_shorten(str(output), 2000), title=\"Raw Output\"))\n",
    "        else:\n",
    "            print(f\"[Render error: {e}]\")\n",
    "            print(\"Raw Output:\")\n",
    "            print(_shorten(str(output), 2000))\n",
    "\n",
    "\n",
    "\n",
    "# --- Post-call auto pretty ---\n",
    "def auto_pretty(full):\n",
    "    \"\"\"Infer a nice title from 'decision' and pretty-print the result.\"\"\"\n",
    "    # Extract 'chosen'/'action' and build a pleasant title per action.\n",
    "    dec = full.get(\"decision\", {}) if isinstance(full, dict) else {}\n",
    "    act = (dec.get(\"chosen\", dec.get(\"action\")) or \"\").upper()\n",
    "    title = {\n",
    "        \"ACTION.QA\": \"QA Execution\",\n",
    "        \"QA\": \"QA Execution\",\n",
    "        \"ACTION.SUMMARIZE\": \"SUMMARIZE Execution\",\n",
    "        \"SUMMARIZE\": \"SUMMARIZE Execution\",\n",
    "        \"ACTION.prediction\": \"Prediction Execution\",\n",
    "        \"prediction\": \"Prediction Execution\",\n",
    "        \"ACTION.CBR\": \"CBR Execution\",\n",
    "        \"CBR\": \"CBR Execution\",\n",
    "    }.get(act, \"Execution\")\n",
    "    pretty_print_result(full, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ab688-544f-40a1-b857-93bb790247cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">╭─────────────────────╮</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">SUMMARIZE Execution</span> <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">╰─────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m╭─────────────────────╮\u001b[0m\n",
       "\u001b[36m│\u001b[0m \u001b[1;37mSUMMARIZE Execution\u001b[0m \u001b[36m│\u001b[0m\n",
       "\u001b[36m╰─────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                                   \n",
       " <span style=\"font-weight: bold\"> # </span> <span style=\"font-weight: bold\"> KEY POINT                                                                                                   </span> \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       "  1   * The dataset contains patient records for individuals presenting with oral cancer symptoms, specifically    \n",
       "      non-healing ulcers or masses in the oral cavity.                                                             \n",
       "  2   * The patients' ages range from 32 to 81 years old, and ethnicities include Black/African, Other, and not    \n",
       "      specified.                                                                                                   \n",
       "  3   * Biomarker analysis reports suggest a high likelihood of malignancy based on elevated levels of specific    \n",
       "      markers in saliva, blood, and urine. These markers include Mirna 21, Cyfra 21 1, Scc Ag, Il 6, and Cea.      \n",
       "  4   * Elevated levels of creatinine and urea in the urine are also commonly observed.                            \n",
       "  5   * The presence of these markers indicates a head and neck primary site for the malignancy.                   \n",
       "  6   * The patients have a history of long-term tobacco use (2 PPD x 30 years), smokeless tobacco use, and        \n",
       "      significant alcohol consumption.                                                                             \n",
       "  7   * The physical examination reveals ulcerated or exophytic masses in various locations within the oral        \n",
       "      cavity. Associated symptoms include odynophagia (painful swallowing).                                        \n",
       "  8   * Palpable lymph nodes are noted in some cases.                                                              \n",
       "  9   * The assessment and plan for each patient involve suspicion of SCC, biopsy of the primary lesion, and       \n",
       "      fine-needle aspiration of cervical lymph nodes where appropriate.                                            \n",
       "                                                                                                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                                                   \n",
       " \u001b[1m \u001b[0m\u001b[1m#\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mKEY POINT                                                                                                  \u001b[0m\u001b[1m \u001b[0m \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       "  1   * The dataset contains patient records for individuals presenting with oral cancer symptoms, specifically    \n",
       "      non-healing ulcers or masses in the oral cavity.                                                             \n",
       "  2   * The patients' ages range from 32 to 81 years old, and ethnicities include Black/African, Other, and not    \n",
       "      specified.                                                                                                   \n",
       "  3   * Biomarker analysis reports suggest a high likelihood of malignancy based on elevated levels of specific    \n",
       "      markers in saliva, blood, and urine. These markers include Mirna 21, Cyfra 21 1, Scc Ag, Il 6, and Cea.      \n",
       "  4   * Elevated levels of creatinine and urea in the urine are also commonly observed.                            \n",
       "  5   * The presence of these markers indicates a head and neck primary site for the malignancy.                   \n",
       "  6   * The patients have a history of long-term tobacco use (2 PPD x 30 years), smokeless tobacco use, and        \n",
       "      significant alcohol consumption.                                                                             \n",
       "  7   * The physical examination reveals ulcerated or exophytic masses in various locations within the oral        \n",
       "      cavity. Associated symptoms include odynophagia (painful swallowing).                                        \n",
       "  8   * Palpable lymph nodes are noted in some cases.                                                              \n",
       "  9   * The assessment and plan for each patient involve suspicion of SCC, biopsy of the primary lesion, and       \n",
       "      fine-needle aspiration of cervical lymph nodes where appropriate.                                            \n",
       "                                                                                                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Example executions + pretty UI\n",
    "# -------------------------------\n",
    "# Each example runs a query via `execute_query(..., return_full=True)` to obtain\n",
    "# a full envelope (decision, output, trace) and then pretty-prints the result.\n",
    "\n",
    "exc_full = execute_query(\n",
    "    \"SUMMARIZE the OSCC clinical context and biomarker profile into key points\",\n",
    "    model_dir=MODEL_DIR,\n",
    "    return_full=True\n",
    ")\n",
    "auto_pretty(exc_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f740db1-c52b-4249-b4ad-bb2f0d6ad361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">╭──────────────╮</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">QA Execution</span> <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">╰──────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m╭──────────────╮\u001b[0m\n",
       "\u001b[36m│\u001b[0m \u001b[1;37mQA Execution\u001b[0m \u001b[36m│\u001b[0m\n",
       "\u001b[36m╰──────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭──────────────────────────────────────────────────── Answer ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> Cisplatin is a chemotherapeutic agent commonly used in the treatment of various cancers, including head and     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> neck cancer. Some common side effects associated with cisplatin use may include nausea, vomiting, loss of       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> appetite, kidney damage (due to increased creatinine levels), hearing loss or tinnitus (ringing in the ears),   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> peripheral neuropathy (numbness, tingling or pain in hands and feet), mouth sores, and anemia. Additionally,    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> cisplatin may cause hair loss and increase the risk of developing secondary cancers later on. It is important   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> to discuss potential side effects with a healthcare provider before starting treatment.                         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m───────────────────────────────────────────────────\u001b[0m\u001b[32m Answer \u001b[0m\u001b[32m────────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m Cisplatin is a chemotherapeutic agent commonly used in the treatment of various cancers, including head and     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m neck cancer. Some common side effects associated with cisplatin use may include nausea, vomiting, loss of       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m appetite, kidney damage (due to increased creatinine levels), hearing loss or tinnitus (ringing in the ears),   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m peripheral neuropathy (numbness, tingling or pain in hands and feet), mouth sores, and anemia. Additionally,    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m cisplatin may cause hair loss and increase the risk of developing secondary cancers later on. It is important   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m to discuss potential side effects with a healthcare provider before starting treatment.                         \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exc_full = execute_query(\n",
    "    \"What are the side effects of cisplatin?\",\n",
    "    model_dir=MODEL_DIR,\n",
    "    return_full=True\n",
    ")\n",
    "auto_pretty(exc_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182d2c4-000f-41ba-8148-fc272aa4638f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">╭───────────╮</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Execution</span> <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">╰───────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m╭───────────╮\u001b[0m\n",
       "\u001b[36m│\u001b[0m \u001b[1;37mExecution\u001b[0m \u001b[36m│\u001b[0m\n",
       "\u001b[36m╰───────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭────────────────────────────────────────────────── Prediction ───────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"font-weight: bold\">Predicted Diagnosis </span>OSCC                                                                                        <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"font-weight: bold\">         Confidence </span>99.97%                                                                                      <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"font-weight: bold\">Class Probabilities </span>                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"font-weight: bold\">                    </span> <span style=\"font-weight: bold\"> Class           </span> <span style=\"font-weight: bold\"> Probability </span>                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"font-weight: bold\">                    </span> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"font-weight: bold\">                    </span>  OSCC              99.97%                                                                  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"font-weight: bold\">                    </span>  Benign Lesion     0.03%                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"font-weight: bold\">                    </span>  Healthy Control   0.00%                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"font-weight: bold\">                    </span>                                                                                            <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────────────────────────\u001b[0m\u001b[33m Prediction \u001b[0m\u001b[33m──────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[1mPredicted Diagnosis\u001b[0m\u001b[1m \u001b[0mOSCC                                                                                        \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[1m         Confidence\u001b[0m\u001b[1m \u001b[0m99.97%                                                                                      \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[1mClass Probabilities\u001b[0m\u001b[1m \u001b[0m                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[1m                    \u001b[0m \u001b[1m \u001b[0m\u001b[1mClass          \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mProbability\u001b[0m\u001b[1m \u001b[0m                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[1m                    \u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[1m                    \u001b[0m  OSCC              99.97%                                                                  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[1m                    \u001b[0m  Benign Lesion     0.03%                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[1m                    \u001b[0m  Healthy Control   0.00%                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[1m                    \u001b[0m                                                                                            \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exc_full = execute_query(\n",
    "    \"predict\",{\n",
    "        \"Age\": 65, \"Gender\": \"Female\", \"Smoking_Status\": \"Former\", \"Cancer_Stage\": \"Stage II\",\n",
    "        \"IL6_Saliva\": 14.5, \"IL8_Saliva\": 21.2, \"TNFa_Saliva\": None, \"LDH_Saliva\": 6.8\n",
    "    },\n",
    "    model_dir=MODEL_DIR,\n",
    "    return_full=True\n",
    ")\n",
    "auto_pretty(exc_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56782f",
   "metadata": {},
   "source": [
    "## 14) Reproducibility & Appendix\n",
    "- Seeds set via `RANDOM_SEED`.  \n",
    "- Deterministic CPU tokenizers.  \n",
    "- Print key package versions for auditability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daba830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "def _ver(pkg):\n",
    "    try:\n",
    "        return importlib.import_module(pkg).__version__\n",
    "    except Exception:\n",
    "        return \"n/a\"\n",
    "print({\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"numpy\": _ver(\"numpy\"),\n",
    "    \"pandas\": _ver(\"pandas\"),\n",
    "    \"sklearn\": _ver(\"sklearn\"),\n",
    "    \"torch\": _ver(\"torch\"),\n",
    "    \"transformers\": _ver(\"transformers\"),\n",
    "    \"sentence_transformers\": _ver(\"sentence_transformers\"),\n",
    "    \"faiss\": _ver(\"faiss\"),\n",
    "    \"llama_index\": _ver(\"llama_index\"),\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI-Thesis)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
