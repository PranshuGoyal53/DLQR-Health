{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aea783e",
   "metadata": {},
   "source": [
    "## Local Retrieval-Augmented Generation (RAG) with LlamaIndex and Ollama  \n",
    "\n",
    "**Author:** Pranshu Goyal  \n",
    "**Date:** August 16, 2025  \n",
    "**Python Version:** ‚â• 3.10  \n",
    "\n",
    "### Abstract  \n",
    "This notebook presents a structured implementation of a **Retrieval-Augmented Generation (RAG)** workflow that combines **LlamaIndex** for document ingestion, preprocessing, embedding, and vector indexing, with **Ollama** as a local large language model (LLM) for downstream tasks such as question answering and summarization.  \n",
    "\n",
    "The workflow has been designed with an emphasis on **reproducibility, resilience, and safe defaults**. Specifically, it provides:  \n",
    "\n",
    "- **Environment Bootstrapping:** Automatic setup of directories (`./docs`, `./storage`, `./logs`) with placeholder files to ensure smooth execution on first run.  \n",
    "- **Deterministic Code Style:** Imports are consistently organized (isort-compatible) and code follows [Black](https://black.readthedocs.io/) formatting conventions.  \n",
    "- **Heuristic Privacy Safeguards:** A lightweight PHI (Protected Health Information) pattern detector is included, though it is heuristic-only and **not HIPAA-compliant**.  \n",
    "- **Graceful Degradation:** The pipeline continues to function if external components are unavailable (e.g., skips question answering/summarization if **Ollama** is not running, or defaults to `UnstructuredReader` if no **LlamaParse** API key is provided).  \n",
    "\n",
    "### Usage Instructions  \n",
    "\n",
    "1. **Execute All Cells:**  \n",
    "   Select *Kernel ‚Üí Restart & Run All* to initialize the notebook. On first execution, the notebook will:  \n",
    "   - Install and validate required dependencies.  \n",
    "   - Create the necessary working directories (`./docs`, `./storage`, `./logs`).  \n",
    "   - Attempt to establish a connection with **Ollama**. If unavailable, the system will safely bypass Ollama-dependent demonstrations.  \n",
    "\n",
    "2. **Optional PDF Parsing via LlamaParse:**  \n",
    "   To enable more accurate PDF ingestion using LlamaParse, create a `.env` file in the project root and specify your API key:  \n",
    "\n",
    "   ```bash\n",
    "   LLAMA_PARSE_KEY=\"your-llamaparse-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ccd8e",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "This section prepares the environment by installing all required dependencies for the **RAG pipeline**.  \n",
    "Both **pip** and **conda** are used to ensure reproducibility and to cover packages better supported by each ecosystem.\n",
    "\n",
    "### 1. üì¶ Pip Dependencies\n",
    "- **Core utilities**:\n",
    "  - `requests` ‚Üí lightweight HTTP client (used to check Ollama availability).  \n",
    "  - `nest-asyncio` ‚Üí patches Jupyter‚Äôs async loop for libraries like LlamaIndex.  \n",
    "  - `python-dotenv` ‚Üí loads environment variables from `.env`.  \n",
    "- **Llama ecosystem**:\n",
    "  - `llama-parse` ‚Üí high-fidelity PDF/complex document parser.  \n",
    "  - `llama-index-core` ‚Üí core LlamaIndex framework.  \n",
    "  - `llama-index-llms-ollama` ‚Üí Ollama LLM integration.  \n",
    "  - `llama-index-embeddings-huggingface` ‚Üí HuggingFace embeddings backend.  \n",
    "  - `llama-index-readers-file` ‚Üí file-type readers for ingestion (DOCX, PPTX, etc.).  \n",
    "- **Data ingestion and preprocessing**:\n",
    "  - `unstructured` ‚Üí parses diverse file formats into text.  \n",
    "  - `pypdf` ‚Üí robust PDF parsing.  \n",
    "- **ML/NLP stack**:\n",
    "  - `torch` ‚Üí PyTorch for deep learning models.  \n",
    "  - `transformers` ‚Üí HuggingFace Transformers for LLMs.  \n",
    "  - `sentence-transformers` ‚Üí pre-trained embedding models.  \n",
    "- **Data handling**:\n",
    "  - `pandas==2.2.3` ‚Üí DataFrame operations (pinned version ensures stability).  \n",
    "\n",
    "‚ö†Ô∏è **Note:** `unstructured[pdf]` is commented out, since it sometimes causes dependency conflicts. Uncomment only if advanced PDF parsing via `unstructured` is required.\n",
    "\n",
    "### 2. üîé Conda Dependencies\n",
    "- `faiss-cpu=1.7.4` ‚Üí Facebook AI Similarity Search (FAISS) for vector indexing and retrieval.  \n",
    "  Installed via conda-forge channel for compatibility and performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c14f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================\n",
    "# # Environment Setup \n",
    "# # ==============================\n",
    "\n",
    "# %pip install -U -qq \\\n",
    "#   requests \\\n",
    "#   nest-asyncio \\\n",
    "#   python-dotenv \\\n",
    "#   llama-parse \\\n",
    "#   llama-index-core \\\n",
    "#   llama-index-llms-ollama \\\n",
    "#   llama-index-embeddings-huggingface \\\n",
    "#   llama-index-readers-file \\\n",
    "#   unstructured \\\n",
    "#   pandas==2.2.3 \\\n",
    "#   pypdf \\\n",
    "#   sentence-transformers \\\n",
    "#   transformers \\\n",
    "#   torch \\\n",
    "#   # \"unstructured[pdf]\"\\\n",
    "\n",
    "# print(\"Pip Dependencies Installed Successfully.\")\n",
    "\n",
    "# %conda install -y -q -c conda-forge faiss-cpu=1.7.4\n",
    "\n",
    "# print(\"Conda Dependencies Installed Successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a96fdf-2004-4a3f-9c92-d04237f0e7c0",
   "metadata": {},
   "source": [
    "---\n",
    "## Overview\n",
    "\n",
    "This cell initializes a lightweight document-processing environment that can optionally work with a local Ollama LLM and LlamaIndex:<br>\n",
    "*\tSilences noisy warnings (including InconsistentVersionWarning from scikit-learn).<br>\n",
    "*\tPrints deterministic runtime info (Python version & platform).<br>\n",
    "*\tChecks whether Ollama is reachable on http://localhost:11434 and lists available models.<br>\n",
    "*\tApplies nest_asyncio so libraries that nest event loops (e.g., LlamaIndex) work smoothly in Jupyter.<br>\n",
    "*\tEnsures minimal project directories exist: docs/, storage/, logs/.<br>\n",
    "*\tDrops a placeholder file in docs/ if it‚Äôs empty, so your later indexing steps never fail due to missing inputs.<br>\n",
    "*\tImports LlamaIndex components and readers, plus LlamaParse for advanced document parsing.<br>\n",
    "\n",
    "This cell is safe to run even if Ollama isn‚Äôt running; Q&A/summarization demos can be skipped when Ollama is down.<br>\n",
    "\n",
    "## Dependencies & What They‚Äôre For\n",
    "\t‚Ä¢\tStandard library: os, sys, time, Path, json, logging, re, functools, typing utilities.\n",
    "\t‚Ä¢\tWarnings control: Filters InconsistentVersionWarning, FutureWarning, and UserWarning to keep output clean.\n",
    "\t‚Ä¢\trequests: Pings Ollama‚Äôs REST endpoint to detect availability.\n",
    "\t‚Ä¢\tnest_asyncio: Patches the Jupyter event loop to avoid ‚Äúalready running loop‚Äù errors.\n",
    "\t‚Ä¢\tpython-dotenv: Loads environment variables if you later call load_dotenv() (import is present here).\n",
    "\t‚Ä¢\tLlamaIndex core: Document, Settings, VectorStoreIndex, storage utilities, directory readers.\n",
    "\t‚Ä¢\tEmbeddings & LLM: HuggingFaceEmbedding, Ollama (the LLM wrapper for local models).\n",
    "\t‚Ä¢\tFile readers: DocxReader, MarkdownReader, PptxReader, UnstructuredReader (broad file-type support).\n",
    "\t‚Ä¢\tllama_parse.LlamaParse: Optional high-fidelity parsing of PDFs/complex docs (API-based)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966431bc-5bb2-4501-9f2d-9a81357e1172",
   "metadata": {},
   "source": [
    "---\n",
    "## Ollama Availability Check\n",
    "\n",
    "is_ollama_up():\n",
    "\t‚Ä¢\tSends a GET to http://localhost:11434/api/tags (2s timeout).\n",
    "\t‚Ä¢\tIf OK, prints status and shows up to 5 model tags (with an ellipsis if more).\n",
    "\t‚Ä¢\tOn failure, prints a friendly message and returns False.\n",
    "\t‚Ä¢\tThe boolean is stored in OLLAMA_UP for later conditional logic.\n",
    "\n",
    "This design lets you write downstream code like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "babb5c94-50cb-4a43-a148-863a57e37a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ nest_asyncio patch applied.\n",
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "Docs directory already populated.\n"
     ]
    }
   ],
   "source": [
    "# Patch Jupyter's event loop (useful for libraries that nest asyncio, e.g. LlamaIndex)\n",
    "nest_asyncio.apply()\n",
    "print(\"‚úÖ nest_asyncio patch applied.\")\n",
    "\n",
    "# Prepare minimal project directories\n",
    "for dir_name in (\"docs\", \"storage\", \"logs\"):\n",
    "    Path(dir_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a placeholder in ./docs so indexing always has at least one document\n",
    "docs_path = Path(\"docs\")\n",
    "if not any(docs_path.iterdir()):\n",
    "    placeholder = docs_path / \"placeholder.txt\"\n",
    "    placeholder.write_text(\n",
    "        \"Add your documents here. This is just a placeholder.\\n\",\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    print(\"Created placeholder document at ./docs/placeholder.txt\")\n",
    "else:\n",
    "    print(\"Docs directory already populated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa07bb53-246e-41fb-be96-1f97696cb182",
   "metadata": {},
   "source": [
    "# Retrieval Transparency & Chunking Stats Helpers\n",
    "\n",
    "This cell defines two **visual-only helper functions** that improve **interpretability and debugging** when working with Retrieval-Augmented Generation (RAG) pipelines.  \n",
    "They **do not affect the actual data flow** ‚Äî they are purely diagnostic and safe to remove if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `_show_retrieval_sources(resp, top=5)`\n",
    "- **Purpose**: Displays the top retrieved context nodes from a query response.  \n",
    "- **Inputs**:  \n",
    "  - `resp`: A LlamaIndex query response (expected to have `.source_nodes`).  \n",
    "  - `top`: Number of top nodes to display (default `5`).  \n",
    "- **Logic**:\n",
    "  - Iterates over retrieved nodes, extracting:  \n",
    "    - `score`: retrieval score (float).  \n",
    "    - `source`: provenance info (file name/path/ID).  \n",
    "    - `chars`: character length of text.  \n",
    "  - Uses `_safe_text()` to extract content length.  \n",
    "  - Displays results in a **Rich table** if available; falls back to plain printing.  \n",
    "- **Why useful**: Gives **transparency** into what documents contributed to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51330bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _show_retrieval_sources(resp, top: int = 5):\n",
    "    \"\"\"\n",
    "    Pretty-print retrieved source nodes for transparency (visual only).\n",
    "    Safe to remove without changing behaviour.\n",
    "    \"\"\"\n",
    "    nodes = getattr(resp, \"source_nodes\", None)\n",
    "    if not nodes:\n",
    "        return\n",
    "    rows = []\n",
    "    for i, sn in enumerate(nodes[:top], 1):\n",
    "        # Node score\n",
    "        score = getattr(sn, \"score\", None)\n",
    "        try:\n",
    "            score = float(score) if score is not None else 0.0\n",
    "        except Exception:\n",
    "            score = 0.0\n",
    "\n",
    "        # Try common metadata keys for filename-like provenance\n",
    "        md = getattr(sn.node, \"metadata\", {}) or {}\n",
    "        src = md.get(\"file_name\") or md.get(\"file_path\") or md.get(\"filename\") or getattr(sn.node, \"id_\", \"‚Äî\")\n",
    "        text = _safe_text(sn.node)\n",
    "        rows.append((i, score, str(src), len(text)))\n",
    "\n",
    "    if _RICH and console:\n",
    "        try:\n",
    "            t = Table(title=\"Retrieved Context (top-k)\", box=box.SIMPLE_HEAVY)\n",
    "            t.add_column(\"Rank\", justify=\"right\")\n",
    "            t.add_column(\"Score\", justify=\"right\")\n",
    "            t.add_column(\"Source\", overflow=\"fold\")\n",
    "            t.add_column(\"Chars\", justify=\"right\")\n",
    "            for r in rows:\n",
    "                t.add_row(str(r[0]), f\"{r[1]:.3f}\", r[2], str(r[3]))\n",
    "            console.print(t)\n",
    "            return\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Fallback plain print\n",
    "    print(\"\\nRetrieved Context (top-k):\")\n",
    "    for r in rows:\n",
    "        print(f\"{r[0]:>2}. score={r[1]:.3f} source={r[2]} chars={r[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc95af-a181-45ce-8642-4778b0102702",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. `_estimate_chunk_stats(documents, chunk_size, chunk_overlap)`\n",
    "- **Purpose**: Estimates how many chunks each document would generate under given chunking parameters.  \n",
    "- **Inputs**:  \n",
    "  - `documents`: list of documents (`Document` objects).  \n",
    "  - `chunk_size`: max characters per chunk.  \n",
    "  - `chunk_overlap`: overlap between chunks.  \n",
    "- **Logic**:\n",
    "  - Calculates stride (`chunk_size - chunk_overlap`).  \n",
    "  - For each doc, estimates number of chunks based on text length and stride.  \n",
    "  - Collects `(name, char_count, ~chunk_count)` for reporting.  \n",
    "  - Displays results in a **Rich table** or as plain text fallback.  \n",
    "- **Why useful**: Helps **tune chunk_size/overlap** before running actual embedding/indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "891babb2-b709-4ae4-92fb-b55978d66edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _estimate_chunk_stats(documents, chunk_size: int, chunk_overlap: int):\n",
    "    \"\"\"\n",
    "    Estimate how many chunks per document given settings (visual only).\n",
    "    Does NOT change how LlamaIndex chunks internally.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    stride = max(1, chunk_size - chunk_overlap)\n",
    "    for i, d in enumerate(documents, 1):\n",
    "        name = (getattr(d, \"metadata\", {}) or {}).get(\"file_name\") or f\"doc_{i}\"\n",
    "        text = _safe_text(d)\n",
    "        est_chunks = 0\n",
    "        if text:\n",
    "            est_chunks = max(1, (len(text) + stride - 1) // stride)\n",
    "        rows.append((name, len(text), est_chunks))\n",
    "\n",
    "    if _RICH and console:\n",
    "        try:\n",
    "            t = Table(title=f\"Estimated Chunking (size={chunk_size}, overlap={chunk_overlap})\",\n",
    "                      box=box.SIMPLE_HEAVY)\n",
    "            t.add_column(\"Source\", overflow=\"fold\")\n",
    "            t.add_column(\"Chars\", justify=\"right\")\n",
    "            t.add_column(\"~Chunks\", justify=\"right\")\n",
    "            for name, chars, ch in rows:\n",
    "                t.add_row(str(name), str(chars), str(ch))\n",
    "            console.print(t)\n",
    "            return\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Fallback plain print\n",
    "    print(f\"\\nEstimated Chunking (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "    for name, chars, ch in rows:\n",
    "        print(f\"- {name}: chars={chars}, ~chunks={ch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e59e8c-251d-4522-91ff-2a08a47a7242",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e8c2374-7e01-4af2-b335-23ee0f8b1a79",
   "metadata": {},
   "source": [
    "## Environment, Logging, Timing & Safety Utilities\n",
    "\n",
    "This cell sets up **essential utilities** that support the rest of the notebook:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Environment Variable Setup\n",
    "- `load_dotenv()` loads environment variables from a `.env` file.\n",
    "- Example: `LLAMA_PARSE_KEY` is expected here for using **LlamaParse** API.\n",
    "- Prints `\"FOUND\"` or `\"NOT FOUND\"` depending on whether the key is set.  \n",
    "  (Currently set as an empty string.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd5abdd9-0810-48d2-ac1c-ec09fd57b8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë LlamaParse Key: NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables (e.g., LLAMA_PARSE_KEY)\n",
    "load_dotenv()\n",
    "LLAMA_PARSE_KEY = \"\"\n",
    "print(\"üîë LlamaParse Key:\", \"FOUND\" if LLAMA_PARSE_KEY else \"NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f7c213-29ee-4bbd-ba04-cf6b7cd5762b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Logger Utility ‚Äî `get_logger(name: str)`\n",
    "- Creates (or reuses) a named logger.\n",
    "- **Avoids duplicate handlers** when rerun in Jupyter.\n",
    "- Configures two handlers:\n",
    "  - **Console handler** ‚Üí logs stream to stdout with timestamp + level.  \n",
    "  - **File handler** ‚Üí saves logs into `./logs/queries.log` (UTF-8 encoded).\n",
    "- Useful for structured, persistent tracking of queries and experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c29f8488-07ad-4f2d-b1c1-e93c8c19e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(name: str) -> logging.Logger:\n",
    "    \"\"\"Create (or return existing) logger with console and file handlers.\n",
    "\n",
    "    Handlers are attached once to avoid duplicate logs across re-runs.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    if not logger.handlers:\n",
    "        # Console handler\n",
    "        ch = logging.StreamHandler(sys.stdout)\n",
    "        ch.setFormatter(\n",
    "            logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "        )\n",
    "        logger.addHandler(ch)\n",
    "        # File handler\n",
    "        fh = logging.FileHandler(\"./logs/queries.log\", encoding=\"utf-8\")\n",
    "        fh.setFormatter(\n",
    "            logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "        )\n",
    "        logger.addHandler(fh)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b632ec3-8370-4be6-8000-c4f105d4376b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Timing Decorator ‚Äî `time_it(func)`\n",
    "- Wraps any function to **print its execution time** in seconds.\n",
    "- Provides wall-clock measurement for profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf85f075",
   "metadata": {
    "id": "configuration-and-safety"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Utility functions (logging, safety) are defined.\n"
     ]
    }
   ],
   "source": [
    "def time_it(func):\n",
    "    \"\"\"Decorator that prints a function's wall-clock execution time.\"\"\"\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"\\n--- ‚è±Ô∏è Function '{func.__name__}' executed in {end - start:.2f} seconds. ---\"\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# Heuristic PHI-like patterns (NOT HIPAA-compliant; informational only)\n",
    "PHI_PATTERNS: Dict[str, re.Pattern[str]] = {\n",
    "    \"SSN\": re.compile(r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\"),\n",
    "    \"Phone\": re.compile(r\"\\b\\(\\d{3}\\)\\s*\\d{3}-\\d{4}\\b|\\b\\d{3}-\\d{3}-\\d{4}\\b\"),\n",
    "    \"Email\": re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"),\n",
    "}\n",
    "\n",
    "\n",
    "def check_for_phi(text: str) -> bool:\n",
    "    \"\"\"Return True if any naive PHI-like pattern matches; False otherwise.\"\"\"\n",
    "    return any(pattern.search(text) for pattern in PHI_PATTERNS.values())\n",
    "\n",
    "\n",
    "notebook_logger = get_logger(\"biomed_notebook\")\n",
    "print(\"‚úÖ Utility functions (logging, safety) are defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0c6ce",
   "metadata": {},
   "source": [
    "---\n",
    "## Core RAG Components ‚Äî Ingestion, Indexing, and Query Engine\n",
    "\n",
    "This cell defines the **core pipeline functions** needed for a Retrieval-Augmented Generation (RAG) system using **LlamaIndex**.  \n",
    "These utilities handle document ingestion, index building, persistence, and query engine creation.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. üìÇ `load_documents(directory: str) -> List[Document]`\n",
    "- **Purpose**: Reads all documents from a given directory into a LlamaIndex-compatible `Document` list.  \n",
    "- **Logic**:\n",
    "  - Defines `file_extractor` mapping of extensions ‚Üí reader:\n",
    "    - `.html` ‚Üí `UnstructuredReader`  \n",
    "    - `.pptx` ‚Üí `PptxReader`  \n",
    "    - `.docx` ‚Üí `DocxReader`  \n",
    "    - `.md` ‚Üí `MarkdownReader`  \n",
    "  - For PDFs:\n",
    "    - If `LLAMA_PARSE_KEY` is set ‚Üí use **LlamaParse** (high-fidelity parsing).  \n",
    "    - Else ‚Üí fall back to `UnstructuredReader`.  \n",
    "  - Wraps readers in `SimpleDirectoryReader` (recursive).  \n",
    "  - Logs progress and returns loaded documents.\n",
    "\n",
    "**Why useful**: Centralized ingestion logic makes the pipeline extensible for new formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea512772-4499-452e-be73-9f5823a7841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory: str) -> List[Document]:\n",
    "    \"\"\"Load documents from `directory`.\n",
    "\n",
    "    - Uses **LlamaParse** for PDFs if `LLAMA_PARSE_KEY` is available.\n",
    "    - Falls back to `UnstructuredReader` for PDFs otherwise.\n",
    "    - Supports HTML, PPTX, DOCX, MD via dedicated readers.\n",
    "    \"\"\"\n",
    "    notebook_logger.info(f\"Loading documents from: {directory}\")\n",
    "\n",
    "    file_extractor = {\n",
    "        \".html\": UnstructuredReader(),\n",
    "        \".pptx\": PptxReader(),\n",
    "        \".docx\": DocxReader(),\n",
    "        \".md\": MarkdownReader(),\n",
    "    }\n",
    "\n",
    "    if LLAMA_PARSE_KEY:\n",
    "        notebook_logger.info(\"LlamaParse key found. Using LlamaParse for PDFs.\")\n",
    "        parser = LlamaParse(api_key=LLAMA_PARSE_KEY, result_type=\"markdown\")\n",
    "        file_extractor[\".pdf\"] = parser\n",
    "    else:\n",
    "        notebook_logger.info(\"LlamaParse key not found. Using UnstructuredReader for PDFs.\")\n",
    "        file_extractor[\".pdf\"] = UnstructuredReader()\n",
    "\n",
    "    reader = SimpleDirectoryReader(directory, file_extractor=file_extractor, recursive=True)\n",
    "    documents = reader.load_data()\n",
    "    notebook_logger.info(f\"Successfully loaded {len(documents)} document(s).\")\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2b077-4845-449e-a4af-ce1b041c2a33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. üì¶ `build_index(documents, embed_model_name, storage_dir, chunk_size, chunk_overlap)`\n",
    "- **Purpose**: Creates and persists a **VectorStoreIndex** with embeddings.  \n",
    "- **Steps**:\n",
    "  - Configures `Settings` for embedding model, chunk size, and overlap.  \n",
    "  - Disables `Settings.llm` since indexing does not need an LLM.  \n",
    "  - Builds index via `VectorStoreIndex.from_documents(documents)`.  \n",
    "  - Persists index to `storage_dir` for reuse later.  \n",
    "\n",
    "**Why useful**: Separates indexing from querying, enabling offline persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7471b63-0c3e-4cec-a3d6-8bb8176fcd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(\n",
    "    documents: List[Document],\n",
    "    embed_model_name: str,\n",
    "    storage_dir: str,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    ") -> None:\n",
    "    \"\"\"Build and persist a VectorStoreIndex using the given embedding model.\"\"\"\n",
    "    notebook_logger.info(f\"Using embedding model: {embed_model_name}\")\n",
    "    Settings.embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "    Settings.chunk_size = chunk_size\n",
    "    Settings.chunk_overlap = chunk_overlap\n",
    "    Settings.llm = None  # Important: no LLM required for indexing\n",
    "\n",
    "    notebook_logger.info(f\"Building index with chunk size {chunk_size} ‚Ä¶\")\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir=storage_dir)\n",
    "    notebook_logger.info(f\"Index persisted to '{storage_dir}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d760ade7-08e9-4cdd-adde-029f28c705f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. ‚úÖ `check_index_exists(storage_dir: str) -> bool`\n",
    "- **Purpose**: Utility check to verify if a persisted index already exists.  \n",
    "- Returns `True` if `storage_dir` exists and is non-empty.  \n",
    "- Prevents unnecessary re-indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43983a3-1b48-4bd8-ad05-19a13b0d837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_index_exists(storage_dir: str) -> bool:\n",
    "    \"\"\"Return True if `storage_dir` exists and is non-empty.\"\"\"\n",
    "    p = Path(storage_dir)\n",
    "    return p.exists() and any(p.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae30654-2b25-484d-a100-c5f1ce2fbe1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. üîé `create_query_engine(storage_dir, llm_model, embed_model_name, k, temperature)`\n",
    "- **Purpose**: Loads a persisted index and initializes a query engine.  \n",
    "- **Steps**:\n",
    "  - Configures `Settings.llm` with **Ollama** (local LLM) + temperature.  \n",
    "  - Configures embedding model again to ensure consistency.  \n",
    "  - Loads `StorageContext` from `storage_dir`.  \n",
    "  - Restores `VectorStoreIndex` from storage.  \n",
    "  - Returns a `query_engine` with `similarity_top_k=k`.  \n",
    "\n",
    "**Why useful**: Separates LLM query logic from indexing.  \n",
    "Enables switching between embedding/LLM backends without rebuilding the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc19f6ed",
   "metadata": {
    "id": "rag-components"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core RAG components (ingestion, indexing, engine) are defined.\n"
     ]
    }
   ],
   "source": [
    "def create_query_engine(\n",
    "    storage_dir: str,\n",
    "    llm_model: str,\n",
    "    embed_model_name: str,\n",
    "    k: int,\n",
    "    temperature: float,\n",
    "):\n",
    "    \"\"\"Create a query engine from a persisted index and the given LLM/embed settings.\"\"\"\n",
    "    notebook_logger.info(\"Creating query engine ‚Ä¶\")\n",
    "    Settings.llm = Ollama(model=llm_model, temperature=temperature, request_timeout=120.0)\n",
    "    Settings.embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    query_engine = index.as_query_engine(similarity_top_k=k)\n",
    "    notebook_logger.info(f\"Query engine ready (LLM='{llm_model}', top-k={k}).\")\n",
    "    return query_engine\n",
    "\n",
    "print(\"‚úÖ Core RAG components (ingestion, indexing, engine) are defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff94d59",
   "metadata": {},
   "source": [
    "---\n",
    "## Engine Cache Helpers ‚Äî QA & Summary Engines\n",
    "\n",
    "This cell defines a simple **engine caching mechanism** to avoid rebuilding query engines multiple times during a notebook session.  \n",
    "Instead of creating a new engine on every call, it reuses previously built instances when available.  \n",
    "\n",
    "### 1. Global Instances\n",
    "- `_qa_engine_instance` ‚Üí holds the singleton instance of the **Q&A query engine**.  \n",
    "- `_summary_engine_instance` ‚Üí holds the singleton instance of the **Summarization query engine**.  \n",
    "- Both are initialized as `None` and created lazily on first use.\n",
    "\n",
    "### 2. `get_or_create_engine(engine_type: str)`\n",
    "- **Purpose**: Returns a cached engine if available; otherwise creates it once and caches it.  \n",
    "- **Parameters**:  \n",
    "  - `engine_type`: must be either `\"qa\"` or `\"summary\"`.  \n",
    "- **Logic**:\n",
    "  - For `\"qa\"`:\n",
    "    - If `_qa_engine_instance` is `None` ‚Üí calls `create_query_engine()` with:  \n",
    "      - storage: `./storage`  \n",
    "      - LLM: `\"mistral\"`  \n",
    "      - Embedding model: `\"BAAI/bge-small-en\"`  \n",
    "      - `k=4` (retrieval depth)  \n",
    "      - `temperature=0.1` (low randomness for factual Q&A).  \n",
    "    - Else ‚Üí reuses the existing instance.  \n",
    "  - For `\"summary\"`:\n",
    "    - If `_summary_engine_instance` is `None` ‚Üí builds with:  \n",
    "      - same storage, LLM, and embedding model  \n",
    "      - `k=8` (broader retrieval)  \n",
    "      - `temperature=0.3` (slightly higher randomness for summaries).  \n",
    "    - Else ‚Üí reuses existing instance.  \n",
    "  - If `engine_type` is anything else ‚Üí raises `ValueError`.  \n",
    "\n",
    "### 3. Why This Matters\n",
    "- **Performance**: Prevents repeatedly rebuilding the same engine (saves time & compute).  \n",
    "- **Consistency**: Ensures that subsequent queries use the **same engine configuration**.  \n",
    "- **Flexibility**: Different settings for `\"qa\"` vs `\"summary\"` support different retrieval and generation strategies.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbe152",
   "metadata": {
    "id": "engine-cache"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Engine cache helpers are defined.\n"
     ]
    }
   ],
   "source": [
    "_qa_engine_instance = None\n",
    "_summary_engine_instance = None\n",
    "\n",
    "def get_or_create_engine(engine_type: str):\n",
    "    \"\"\"Return a cached engine of type 'qa' or 'summary', building it if necessary.\"\"\"\n",
    "    global _qa_engine_instance, _summary_engine_instance\n",
    "\n",
    "    if engine_type == \"qa\":\n",
    "        if _qa_engine_instance is None:\n",
    "            notebook_logger.info(\"üß† No QA engine found. Creating one ‚Ä¶\")\n",
    "            _qa_engine_instance = create_query_engine(\n",
    "                storage_dir=\"./storage\",\n",
    "                llm_model=\"mistral\",\n",
    "                embed_model_name=\"BAAI/bge-small-en\",\n",
    "                k=4,\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            notebook_logger.info(\"‚úÖ QA engine is ready.\")\n",
    "        else:\n",
    "            notebook_logger.info(\"‚ö° Reusing existing QA engine from memory.\")\n",
    "        return _qa_engine_instance\n",
    "\n",
    "    if engine_type == \"summary\":\n",
    "        if _summary_engine_instance is None:\n",
    "            notebook_logger.info(\"üß† No Summary engine found. Creating one ‚Ä¶\")\n",
    "            _summary_engine_instance = create_query_engine(\n",
    "                storage_dir=\"./storage\",\n",
    "                llm_model=\"mistral\",\n",
    "                embed_model_name=\"BAAI/bge-small-en\",\n",
    "                k=8,\n",
    "                temperature=0.3,\n",
    "            )\n",
    "            notebook_logger.info(\"‚úÖ Summary engine is ready.\")\n",
    "        else:\n",
    "            notebook_logger.info(\"‚ö° Reusing existing Summary engine from memory.\")\n",
    "        return _summary_engine_instance\n",
    "\n",
    "    raise ValueError(\"Unknown engine type. Use 'qa' or 'summary'.\")\n",
    "\n",
    "print(\"‚úÖ Engine cache helpers are defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3b853",
   "metadata": {},
   "source": [
    "---\n",
    "## Job Helpers ‚Äî Indexing, Querying, Summarization & Interactive Sessions\n",
    "\n",
    "This cell defines the **high-level workflow functions** that orchestrate the end-to-end Retrieval-Augmented Generation (RAG) pipeline.  \n",
    "Each function is decorated with `@time_it`, so runtime performance is measured and printed.\n",
    "\n",
    "### 1. üìÇ `run_index()`\n",
    "- **Purpose**: Ingests documents from `./docs`, builds a vector index, and persists it to `./storage`.\n",
    "- **Visuals/Diagnostics**:\n",
    "  - Pipeline diagram (`_print_pipeline_diagram`)\n",
    "  - Document inventory by extension (`_print_doc_inventory`)\n",
    "  - Estimated chunking stats (`_estimate_chunk_stats`)\n",
    "  - Spinner/progress bar during index build\n",
    "- **Process**:\n",
    "  1. Loads documents using `load_documents`.\n",
    "  2. Shows file-type distribution and chunking estimates.\n",
    "  3. If documents exist ‚Üí builds index with `build_index`.\n",
    "  4. Persists index into `./storage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a331eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_it\n",
    "def run_index() -> None:\n",
    "    \"\"\"Ingest and index the current contents of ./docs to ./storage.\n",
    "    Visuals:\n",
    "      ‚Ä¢ Pipeline diagram (explains flow)\n",
    "      ‚Ä¢ Document inventory (file-type counts)\n",
    "      ‚Ä¢ Estimated chunking table (size/overlap effect)\n",
    "      ‚Ä¢ Progress bar around index build/persist\n",
    "    \"\"\"\n",
    "    print(\"--- Starting document ingestion and indexing ---\")\n",
    "    _print_pipeline_diagram()  # purely cosmetic diagram\n",
    "\n",
    "    DOCS_DIR = \"./docs\"\n",
    "    STORAGE_DIR = \"./storage\"\n",
    "    EMBED_MODEL = \"BAAI/bge-small-en\"\n",
    "    CHUNK_SIZE = 512\n",
    "    CHUNK_OVERLAP = 50\n",
    "\n",
    "    try:\n",
    "        # Visual spinner for load\n",
    "        with _spinner(\"Loading documents ‚Ä¶\"):\n",
    "            documents = load_documents(DOCS_DIR)\n",
    "\n",
    "        # Inventory + estimated chunking\n",
    "        _print_doc_inventory(DOCS_DIR)\n",
    "        _estimate_chunk_stats(documents, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "        if not documents:\n",
    "            print(\"üõë No documents found in './docs'. Please add files to ./docs and try again.\")\n",
    "            return\n",
    "\n",
    "        # Progress wrapper (visual only)\n",
    "        if _RICH and console:\n",
    "            with Progress(\n",
    "                SpinnerColumn(), TextColumn(\"[progress.description]{task.description}\"),\n",
    "                BarColumn(), TimeElapsedColumn()\n",
    "            ) as progress:\n",
    "                task = progress.add_task(\"Building and persisting index ‚Ä¶\", total=1)\n",
    "                build_index(\n",
    "                    documents=documents,\n",
    "                    embed_model_name=EMBED_MODEL,\n",
    "                    storage_dir=STORAGE_DIR,\n",
    "                    chunk_size=CHUNK_SIZE,\n",
    "                    chunk_overlap=CHUNK_OVERLAP,\n",
    "                )\n",
    "                progress.update(task, advance=1)\n",
    "        else:\n",
    "            print(\"[‚Ä¶] Building and persisting index ‚Ä¶\")\n",
    "            build_index(\n",
    "                documents=documents,\n",
    "                embed_model_name=EMBED_MODEL,\n",
    "                storage_dir=STORAGE_DIR,\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                chunk_overlap=CHUNK_OVERLAP,\n",
    "            )\n",
    "\n",
    "        print(f\"\\n‚úÖ Successfully built index with {len(documents)} documents.\")\n",
    "        print(\"--- Indexing complete ---\")\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        notebook_logger.error(f\"Failed during indexing: {exc}\", exc_info=True)\n",
    "        print(f\"Error: {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9367841d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. `run_ask(question: str)`\n",
    "- **Purpose**: Runs a Q&A query against the persisted index using **Ollama**.\n",
    "- **Pre-checks**:\n",
    "  - Verifies index exists with `check_index_exists`.\n",
    "  - Confirms Ollama is up (`OLLAMA_UP`).\n",
    "  - Warns if query text matches PHI patterns (`check_for_phi`).\n",
    "- **Visuals/Diagnostics**:\n",
    "  - Query config panel (top-k and temperature).\n",
    "  - Spinner while generating answer.\n",
    "  - Retrieved sources table (`_show_retrieval_sources`).\n",
    "- **Post-processing**:\n",
    "  - Prints the final answer, truncated at `MAX_TOKENS` for display clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_it\n",
    "def run_ask(question: str) -> None:\n",
    "    \"\"\"Ask a question against the persisted index using Ollama (if available).\n",
    "    Visuals:\n",
    "      ‚Ä¢ Query config panel (top-k & temperature)\n",
    "      ‚Ä¢ Spinner while generating answer\n",
    "      ‚Ä¢ Retrieved sources table (top-k provenance)\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "\n",
    "    STORAGE_DIR = \"./storage\"\n",
    "    MAX_TOKENS = 350\n",
    "\n",
    "    if not check_index_exists(STORAGE_DIR):\n",
    "        print(\"üõë Index not found. Please run `run_index()` first.\")\n",
    "        return\n",
    "    if not OLLAMA_UP:\n",
    "        print(\"üõë Ollama is not reachable. Skipping Q&A demonstration.\")\n",
    "        return\n",
    "    if check_for_phi(question):\n",
    "        print(\"‚ö†Ô∏è Warning: Potential PHI detected in query.\")\n",
    "\n",
    "    notebook_logger.info(\"Received query (content omitted for safety log).\")\n",
    "\n",
    "    # Tiny visual panel for current settings (no logic change)\n",
    "    if _RICH and console:\n",
    "        console.print(Panel.fit(\"Engine: QA ‚Ä¢ top-k‚âà4 ‚Ä¢ temperature=0.1\",\n",
    "                                title=\"Query Config\", border_style=\"green\"))\n",
    "    else:\n",
    "        print(\"Query Config: Engine=QA, top-k‚âà4, temperature=0.1\")\n",
    "\n",
    "    try:\n",
    "        with _spinner(\"Generating answer ‚Ä¶\"):\n",
    "            qa_engine = get_or_create_engine(\"qa\")\n",
    "            qa_response = qa_engine.query(question)\n",
    "\n",
    "        # Visual-only provenance\n",
    "        _show_retrieval_sources(qa_response, top=5)\n",
    "\n",
    "        # Print answer (with a simple token cap for display)\n",
    "        answer = str(qa_response)\n",
    "        if len(answer.split()) > MAX_TOKENS:\n",
    "            answer = \" \".join(answer.split()[:MAX_TOKENS]) + \" ‚Ä¶\"\n",
    "        print(f\"\\n‚úÖ Answer:\\n{answer}\")\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        notebook_logger.error(f\"Failed during query: {exc}\", exc_info=True)\n",
    "        print(f\"Error: {exc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2bfdaa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.`run_summarize(topic: str)`\n",
    "- **Purpose**: Summarizes a given topic using the persisted index and Ollama.\n",
    "- **Process**:\n",
    "  1. Checks index existence and Ollama availability.\n",
    "  2. Creates a custom abstractive summary prompt.\n",
    "  3. Runs query via `\"summary\"` engine (`get_or_create_engine(\"summary\")`).\n",
    "  4. Prints retrieved context and the generated summary (truncated to `MAX_TOKENS`).\n",
    "- **Visuals/Diagnostics**: Similar to `run_ask`, with a blue-styled panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_it\n",
    "def run_summarize(topic: str) -> None:\n",
    "    \"\"\"Summarize a topic using the persisted index via Ollama (if available).\n",
    "    Visuals:\n",
    "      ‚Ä¢ Query config panel (top-k & temperature)\n",
    "      ‚Ä¢ Spinner while producing summary\n",
    "      ‚Ä¢ Retrieved sources table (top-k provenance)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìñ Summarizing topic: {topic}\")\n",
    "\n",
    "    STORAGE_DIR = \"./storage\"\n",
    "    MAX_TOKENS = 350\n",
    "\n",
    "    if not check_index_exists(STORAGE_DIR):\n",
    "        print(\"üõë Index not found. Please run `run_index()` first.\")\n",
    "        return\n",
    "    if not OLLAMA_UP:\n",
    "        print(\"üõë Ollama is not reachable. Skipping summarization demonstration.\")\n",
    "        return\n",
    "\n",
    "    summary_prompt = (\n",
    "        f\"Using only the provided context, generate a concise, fully abstractive summary of \"\n",
    "        f\"the key points regarding '{topic}'. Present the essential information in a logically \"\n",
    "        f\"organized, coherent narrative without introductory phrases or filler.\"\n",
    "    )\n",
    "\n",
    "    notebook_logger.info(\"Received summarization request (content omitted for safety log).\")\n",
    "\n",
    "    # Visual panel for current settings (no logic change)\n",
    "    if _RICH and console:\n",
    "        console.print(Panel.fit(\"Engine: Summary ‚Ä¢ top-k‚âà8 ‚Ä¢ temperature=0.3\",\n",
    "                                title=\"Query Config\", border_style=\"blue\"))\n",
    "    else:\n",
    "        print(\"Query Config: Engine=Summary, top-k‚âà8, temperature=0.3\")\n",
    "\n",
    "    try:\n",
    "        with _spinner(\"Producing summary ‚Ä¶\"):\n",
    "            summary_engine = get_or_create_engine(\"summary\")\n",
    "            summary_response = summary_engine.query(summary_prompt)\n",
    "\n",
    "        # Visual-only provenance\n",
    "        _show_retrieval_sources(summary_response, top=5)\n",
    "\n",
    "        summary_text = str(summary_response)\n",
    "        if len(summary_text.split()) > MAX_TOKENS:\n",
    "            summary_text = \" \".join(summary_text.split()[:MAX_TOKENS]) + \" ‚Ä¶\"\n",
    "        print(f\"\\n‚úÖ Summary:\\n{summary_text}\")\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        notebook_logger.error(f\"Failed during summarization: {exc}\", exc_info=True)\n",
    "        print(f\"Error: {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185273b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. `run_interactive_process_session()`\n",
    "- **Purpose**: Starts an **ad-hoc interactive Q&A session** over pasted raw text.\n",
    "- **Process**:\n",
    "  1. User pastes free-form text, ends input with `DONE`.\n",
    "  2. Builds an in-memory vector index (`VectorStoreIndex`) on the fly.\n",
    "  3. User can iteratively enter questions or summary prompts.\n",
    "  4. Each query is answered in real-time until user types `\"exit\"`.\n",
    "- **Features**:\n",
    "  - Warns if PHI-like text is detected.\n",
    "  - Shows query runtime duration.\n",
    "  - Does not persist index ‚Äî temporary only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc7727",
   "metadata": {
    "id": "jobs-and-cli"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Job helpers are defined.\n",
      "Setup complete. You can now run `run_index()`, `run_ask(question)`, or `run_summarize(topic)`.\n"
     ]
    }
   ],
   "source": [
    "def run_interactive_process_session() -> None:\n",
    "    \"\"\"Interactive ad-hoc Q&A over pasted text.\n",
    "\n",
    "    Not auto-invoked to keep Run-All non-blocking. Paste text, then ask questions.\n",
    "    \"\"\"\n",
    "    from llama_index.core import VectorStoreIndex\n",
    "\n",
    "    LLM_MODEL = \"mistral\"\n",
    "    EMBED_MODEL = \"BAAI/bge-small-en\"\n",
    "    TEMPERATURE = 0.1\n",
    "\n",
    "    print(\"--- üöÄ Starting Interactive Processing Session üöÄ ---\")\n",
    "    print(\"Paste text to analyze. Type DONE on a new line to finish.\")\n",
    "\n",
    "    lines = []\n",
    "    while True:\n",
    "        line = input()\n",
    "        if line.strip().upper() == \"DONE\":\n",
    "            break\n",
    "        lines.append(line)\n",
    "    text_to_process = \"\\n\".join(lines)\n",
    "    if not text_to_process.strip():\n",
    "        print(\"No text provided. Exiting session.\")\n",
    "        return\n",
    "\n",
    "    notebook_logger.info(\"Received ad-hoc text for interactive session.\")\n",
    "\n",
    "    try:\n",
    "        # Visual spinner for temporary index setup\n",
    "        with _spinner(\"Preparing in-memory index ‚Ä¶\"):\n",
    "            start_setup = time.time()\n",
    "            Settings.llm = Ollama(model=LLM_MODEL, temperature=TEMPERATURE, request_timeout=120.0)\n",
    "            Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL)\n",
    "            Settings.chunk_size = 512\n",
    "\n",
    "            temp_docs = [Document(text=text_to_process)]\n",
    "            temp_index = VectorStoreIndex.from_documents(temp_docs)\n",
    "            temp_engine = temp_index.as_query_engine()\n",
    "            setup_duration = time.time() - start_setup\n",
    "\n",
    "        notebook_logger.info(\n",
    "            f\"‚úÖ In-memory index created for interactive session in {setup_duration:.2f}s.\"\n",
    "        )\n",
    "        print(\"\\nYou can now ask questions. Type 'exit' to finish.\")\n",
    "\n",
    "        while True:\n",
    "            query = input(\"\\nEnter question/summary (or 'exit'): \")\n",
    "            if query.lower() in {\"exit\", \"quit\"}:\n",
    "                print(\"Exiting session.\")\n",
    "                break\n",
    "            if check_for_phi(query):\n",
    "                print(\"‚ö†Ô∏è Warning: Potential PHI detected in query.\")\n",
    "            start_q = time.time()\n",
    "            response = temp_engine.query(query)\n",
    "            print(f\"\\nüí° Answer:\\n{response}\")\n",
    "            print(f\"(Query processed in {time.time() - start_q:.2f} seconds)\")\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        notebook_logger.error(\n",
    "            f\"Failed during interactive processing session: {exc}\", exc_info=True\n",
    "        )\n",
    "        print(f\"Error: {exc}\")\n",
    "\n",
    "print(\"‚úÖ Job helpers are defined.\")\n",
    "print(\"Setup complete. You can now run `run_index()`, `run_ask(question)`, or `run_summarize(topic)`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60a8c7",
   "metadata": {},
   "source": [
    "---\n",
    "## Running the RAG Workflow ‚Äî End-to-End Demo\n",
    "\n",
    "This section shows how to execute the **full Retrieval-Augmented Generation pipeline** step by step.\n",
    "\n",
    "### 1) Build (or Rebuild) the Index\n",
    "- Runs `run_index()`, which:\n",
    "  - Loads documents from `./docs`\n",
    "  - Builds embeddings and a vector index\n",
    "  - Persists the index into `./storage`\n",
    "- Should always be run after adding or updating documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "079823bf",
   "metadata": {
    "id": "demo-usage"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting document ingestion and indexing ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ RAG Pipeline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">‚îÇ</span>                                                                                  <span style=\"color: #008080; text-decoration-color: #008080\">‚îÇ</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">‚îÇ</span> ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê <span style=\"color: #008080; text-decoration-color: #008080\">‚îÇ</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">‚îÇ</span> ‚îÇ  docs/   ‚îú‚îÄ‚îÄ‚ñ∂‚îÇ   Readers    ‚îú‚îÄ‚îÄ‚ñ∂‚îÇ Chunk/Embed‚îú‚îÄ‚îÄ‚ñ∂‚îÇ Vector Index ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  Query ‚îÇ <span style=\"color: #008080; text-decoration-color: #008080\">‚îÇ</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">‚îÇ</span> ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò <span style=\"color: #008080; text-decoration-color: #008080\">‚îÇ</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">‚îÇ</span>                                                                                  <span style=\"color: #008080; text-decoration-color: #008080\">‚îÇ</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m‚ï≠‚îÄ\u001b[0m\u001b[36m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[36m RAG Pipeline \u001b[0m\u001b[36m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[36m‚îÄ‚ïÆ\u001b[0m\n",
       "\u001b[36m‚îÇ\u001b[0m                                                                                  \u001b[36m‚îÇ\u001b[0m\n",
       "\u001b[36m‚îÇ\u001b[0m ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê \u001b[36m‚îÇ\u001b[0m\n",
       "\u001b[36m‚îÇ\u001b[0m ‚îÇ  docs/   ‚îú‚îÄ‚îÄ‚ñ∂‚îÇ   Readers    ‚îú‚îÄ‚îÄ‚ñ∂‚îÇ Chunk/Embed‚îú‚îÄ‚îÄ‚ñ∂‚îÇ Vector Index ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  Query ‚îÇ \u001b[36m‚îÇ\u001b[0m\n",
       "\u001b[36m‚îÇ\u001b[0m ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò \u001b[36m‚îÇ\u001b[0m\n",
       "\u001b[36m‚îÇ\u001b[0m                                                                                  \u001b[36m‚îÇ\u001b[0m\n",
       "\u001b[36m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 01:30:33,467 - biomed_notebook - INFO - Loading documents from: ./docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 01:30:33,467 - INFO - Loading documents from: ./docs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 01:30:33,467 - biomed_notebook - INFO - LlamaParse key not found. Using UnstructuredReader for PDFs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 01:30:33,467 - INFO - LlamaParse key not found. Using UnstructuredReader for PDFs.\n",
      "2025-08-23 01:30:35,209 - INFO - pikepdf C++ to Python logger bridge initialized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Warning: No languages specified, defaulting to English.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Warning: No languages specified, defaulting to English.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 01:39:45,290 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 01:39:45,302 - biomed_notebook - INFO - Successfully loaded 1 document(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 01:39:45,302 - INFO - Successfully loaded 1 document(s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\"> Document Inventory  </span>\n",
       "<span style=\"font-style: italic\">   (by extension)    </span>\n",
       "                     \n",
       " <span style=\"font-weight: bold\"> Extension </span> <span style=\"font-weight: bold\"> Count </span> \n",
       " ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \n",
       "     (none)       1  \n",
       "       .pdf       1  \n",
       "                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m Document Inventory  \u001b[0m\n",
       "\u001b[3m   (by extension)    \u001b[0m\n",
       "                     \n",
       " \u001b[1m \u001b[0m\u001b[1mExtension\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mCount\u001b[0m\u001b[1m \u001b[0m \n",
       " ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \n",
       "     (none)       1  \n",
       "       .pdf       1  \n",
       "                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Estimated Chunking (size=512, overlap=50)         </span>\n",
       "                                                          \n",
       " <span style=\"font-weight: bold\"> Source                            </span> <span style=\"font-weight: bold\">    Chars </span> <span style=\"font-weight: bold\"> ~Chunks </span> \n",
       " ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \n",
       "  synthetic_oral_cancer_dataset.pdf   40796291     88304  \n",
       "                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m        Estimated Chunking (size=512, overlap=50)         \u001b[0m\n",
       "                                                          \n",
       " \u001b[1m \u001b[0m\u001b[1mSource                           \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m   Chars\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m~Chunks\u001b[0m\u001b[1m \u001b[0m \n",
       " ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \n",
       "  synthetic_oral_cancer_dataset.pdf   40796291     88304  \n",
       "                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 01:39:45,309 - biomed_notebook - INFO - Using embedding model: BAAI/bge-small-en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 01:39:45,309 - INFO - Using embedding model: BAAI/bge-small-en\n",
      "2025-08-23 01:39:45,343 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en\n",
      "2025-08-23 01:39:47,508 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM is explicitly disabled. Using MockLLM.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM is explicitly disabled. Using MockLLM.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 01:39:47,614 - biomed_notebook - INFO - Building index with chunk size 512 ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 01:39:47,614 - INFO - Building index with chunk size 512 ‚Ä¶\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:13:44,997 - biomed_notebook - INFO - Index persisted to './storage'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:13:44,997 - INFO - Index persisted to './storage'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully built index with 1 documents.\n",
      "--- Indexing complete ---\n",
      "\n",
      "--- ‚è±Ô∏è Function 'run_index' executed in 2591.74 seconds. ---\n"
     ]
    }
   ],
   "source": [
    "# 1) Build (or rebuild) the index\n",
    "run_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47b45a63-72df-43b2-8ec3-9fecd5c42701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Question: What is the most common cause of oral cancer?\n",
      "2025-08-23 02:13:45,207 - biomed_notebook - INFO - Received query (content omitted for safety log).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:13:45,207 - INFO - Received query (content omitted for safety log).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Query Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">‚îÇ</span> Engine: QA ‚Ä¢ top-k‚âà4 ‚Ä¢ temperature=0.1 <span style=\"color: #008000; text-decoration-color: #008000\">‚îÇ</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚ï≠‚îÄ\u001b[0m\u001b[32m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[32m Query Config \u001b[0m\u001b[32m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[32m‚îÄ‚ïÆ\u001b[0m\n",
       "\u001b[32m‚îÇ\u001b[0m Engine: QA ‚Ä¢ top-k‚âà4 ‚Ä¢ temperature=0.1 \u001b[32m‚îÇ\u001b[0m\n",
       "\u001b[32m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:13:45,210 - biomed_notebook - INFO - üß† No QA engine found. Creating one ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:13:45,210 - INFO - üß† No QA engine found. Creating one ‚Ä¶\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:13:45,211 - biomed_notebook - INFO - Creating query engine ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:13:45,211 - INFO - Creating query engine ‚Ä¶\n",
      "2025-08-23 02:13:45,216 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en\n",
      "2025-08-23 02:13:47,294 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:15:00,656 - INFO - Loading all indices.\n",
      "2025-08-23 02:15:01,122 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:15:01,125 - biomed_notebook - INFO - Query engine ready (LLM='mistral', top-k=4).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:15:01,125 - INFO - Query engine ready (LLM='mistral', top-k=4).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:15:01,125 - biomed_notebook - INFO - ‚úÖ QA engine is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:15:01,125 - INFO - ‚úÖ QA engine is ready.\n",
      "2025-08-23 02:15:42,692 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                 Retrieved Context (top-k)                  </span>\n",
       "                                                            \n",
       " <span style=\"font-weight: bold\"> Rank </span> <span style=\"font-weight: bold\"> Score </span> <span style=\"font-weight: bold\"> Source                            </span> <span style=\"font-weight: bold\"> Chars </span> \n",
       " ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \n",
       "     1   0.883   synthetic_oral_cancer_dataset.pdf    1137  \n",
       "     2   0.883   synthetic_oral_cancer_dataset.pdf    1140  \n",
       "     3   0.882   synthetic_oral_cancer_dataset.pdf    1211  \n",
       "     4   0.882   synthetic_oral_cancer_dataset.pdf    1213  \n",
       "                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                 Retrieved Context (top-k)                  \u001b[0m\n",
       "                                                            \n",
       " \u001b[1m \u001b[0m\u001b[1mRank\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mScore\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mSource                           \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mChars\u001b[0m\u001b[1m \u001b[0m \n",
       " ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \n",
       "     1   0.883   synthetic_oral_cancer_dataset.pdf    1137  \n",
       "     2   0.883   synthetic_oral_cancer_dataset.pdf    1140  \n",
       "     3   0.882   synthetic_oral_cancer_dataset.pdf    1211  \n",
       "     4   0.882   synthetic_oral_cancer_dataset.pdf    1213  \n",
       "                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Answer:\n",
      " The provided context does not specify a most common cause of oral cancer across all patients. However, it shows that in the cases presented, long-term smokeless tobacco use and heavy tobacco use are associated with oral cancer. It's important to note that this is based on a small dataset and may not represent the general population. For a comprehensive understanding, further research or consultation with a healthcare professional would be recommended.\n",
      "\n",
      "--- ‚è±Ô∏è Function 'run_ask' executed in 117.55 seconds. ---\n",
      "\n",
      "üìñ Summarizing topic: findings on oral cancer and ethnicity\n",
      "2025-08-23 02:15:42,764 - biomed_notebook - INFO - Received summarization request (content omitted for safety log).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:15:42,764 - INFO - Received summarization request (content omitted for safety log).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Query Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">‚îÇ</span> Engine: Summary ‚Ä¢ top-k‚âà8 ‚Ä¢ temperature=0.3 <span style=\"color: #000080; text-decoration-color: #000080\">‚îÇ</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m‚ï≠‚îÄ\u001b[0m\u001b[34m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[34m Query Config \u001b[0m\u001b[34m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[34m‚îÄ‚ïÆ\u001b[0m\n",
       "\u001b[34m‚îÇ\u001b[0m Engine: Summary ‚Ä¢ top-k‚âà8 ‚Ä¢ temperature=0.3 \u001b[34m‚îÇ\u001b[0m\n",
       "\u001b[34m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:15:42,770 - biomed_notebook - INFO - üß† No Summary engine found. Creating one ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:15:42,770 - INFO - üß† No Summary engine found. Creating one ‚Ä¶\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:15:42,771 - biomed_notebook - INFO - Creating query engine ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:15:42,771 - INFO - Creating query engine ‚Ä¶\n",
      "2025-08-23 02:15:42,788 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en\n",
      "2025-08-23 02:15:45,301 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:16:53,103 - INFO - Loading all indices.\n",
      "2025-08-23 02:16:53,567 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:16:53,569 - biomed_notebook - INFO - Query engine ready (LLM='mistral', top-k=8).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:16:53,569 - INFO - Query engine ready (LLM='mistral', top-k=8).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:16:53,570 - biomed_notebook - INFO - ‚úÖ Summary engine is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 02:16:53,570 - INFO - ‚úÖ Summary engine is ready.\n",
      "2025-08-23 02:17:32,412 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                 Retrieved Context (top-k)                  </span>\n",
       "                                                            \n",
       " <span style=\"font-weight: bold\"> Rank </span> <span style=\"font-weight: bold\"> Score </span> <span style=\"font-weight: bold\"> Source                            </span> <span style=\"font-weight: bold\"> Chars </span> \n",
       " ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \n",
       "     1   0.883   synthetic_oral_cancer_dataset.pdf    1232  \n",
       "     2   0.883   synthetic_oral_cancer_dataset.pdf    1131  \n",
       "     3   0.883   synthetic_oral_cancer_dataset.pdf    1147  \n",
       "     4   0.883   synthetic_oral_cancer_dataset.pdf    1238  \n",
       "     5   0.882   synthetic_oral_cancer_dataset.pdf    1236  \n",
       "                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                 Retrieved Context (top-k)                  \u001b[0m\n",
       "                                                            \n",
       " \u001b[1m \u001b[0m\u001b[1mRank\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mScore\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mSource                           \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mChars\u001b[0m\u001b[1m \u001b[0m \n",
       " ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \n",
       "     1   0.883   synthetic_oral_cancer_dataset.pdf    1232  \n",
       "     2   0.883   synthetic_oral_cancer_dataset.pdf    1131  \n",
       "     3   0.883   synthetic_oral_cancer_dataset.pdf    1147  \n",
       "     4   0.883   synthetic_oral_cancer_dataset.pdf    1238  \n",
       "     5   0.882   synthetic_oral_cancer_dataset.pdf    1236  \n",
       "                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Summary:\n",
      " Oral cancer cases were presented for various patients, all showing non-healing ulcers on the lateral border of the tongue for varying durations. The lesions were painful and showed progressive growth. Associated symptoms included mild odynophagia. All patients had a history of long-term smokeless tobacco use.\n",
      "\n",
      "Examinations revealed ulcerated, indurated lesions in various locations: floor of mouth, buccal mucosa, and left tonsillar pillar. The size of the lesions ranged from 2.5x1.5 cm to 3.0 cm. Keratin pearl formation was noted in some cases.\n",
      "\n",
      "Lymphadenopathy on the ipsilateral side or a firm, non-mobile submandibular lymph node was palpable in several instances.\n",
      "\n",
      "The final diagnoses included Well-Differentiated Squamous Cell Carcinoma (SCC), HPV-16 positive, and Poorly Differentiated SCC. The cancer was invasive and arose from the floor of mouth, buccal mucosa, or left tonsillar pillar in different cases.\n",
      "\n",
      "The ethnicity of the patients was not explicitly mentioned in the provided context.\n",
      "\n",
      "--- ‚è±Ô∏è Function 'run_summarize' executed in 109.70 seconds. ---\n"
     ]
    }
   ],
   "source": [
    "# 2) Ask a question (requires Ollama)\n",
    "run_ask(\"What is the most common cause of oral cancer?\")\n",
    "\n",
    "# 3) Summarize a topic (requires Ollama)\n",
    "run_summarize(\"findings on oral cancer and ethnicity\")\n",
    "\n",
    "# 4) Optional interactive session (NOT auto-run to keep Run-All non-blocking)\n",
    "# run_interactive_process_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI-Thesis)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
